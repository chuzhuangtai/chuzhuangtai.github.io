<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2F%E5%A4%9A%E4%B8%AAgit%E8%B4%A6%E5%8F%B7%E4%B9%8B%E9%97%B4%E7%9A%84%E5%88%87%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[介绍所谓多个git账号，可能有两种情况: 我有多个github的账号，不同的账号对应不同的repo，需要push的时候自动区分账号 我有多个git的账号，有的是github的，有的是bitbucket的，有的是单位的gitlab的，不同账号对应不同的repo，需要push的时候自动区分账号 这两种情况的处理方法是一样的，分下面几步走: 处理 先假设我有两个账号，一个是github上的，一个是公司gitlab上面的。先为不同的账号生成不同的ssh-key ssh-keygen -t rsa -f ~/.ssh/id_rsa_work -c xxx@gmail.com 然后根据提示连续回车即可在~/.ssh目录下得到id_rsa_work和id_rsa_work.pub两个文件，id_rsa_work.pub文件里存放的就是我们要使用的key ssh-keygen -t rsa -f ~/.ssh/id_rsa_github -c xxx@gmail.com 然后根据提示连续回车即可在~/.ssh目录下得到id_rsa_github和id_rsa_github.pub两个文件，id_rsa_gthub.pub文件里存放的就是我们要使用的key 把id_rsa_xxx.pub中的key添加到github或gitlab上，这一步在github或gitlab上都有帮助，不再赘述 编辑 ~/.ssh/config，设定不同的git 服务器对应不同的key 编辑完成后可以使用命令 ssh -vT git@github.com 看看是不是采用了正确的id_rsa_github.pub文件 这样每次push的时候系统就会根据不同的仓库地址使用不同的账号提交了 从上面一步可以看到，ssh区分账号，其实靠的是HostName这个字段，因此如果在github上有多个账号，很容易的可以把不同的账号映射到不同的HostName上就可以了。比如我有A和B两个账号， 先按照步骤一生成不同的key文件，再修改~/.ssh/config 内容应该是这样的。 同时你的github的repo ssh url就要做相应的修改了，比如根据上面的配置,原连接地址是: 1git@github.com:testA/gopkg.git 那么根据上面的配置，就要把github.com换成A.github.com, 那么ssh解析的时候就会自动把testA.github.com 转换为 github.com,修改后就是 1git@A.github.com:testA/gopkg.git 直接更改 repo/.git/config 里面的url即可 这样每次push的时候系统就会根据不同的仓库地址使用不同的账号提交了 一些题外话我有一个repo，想要同时push到不同的仓库该如何设置?很简单， 直接更改 repo/.git/config 里面的url即可，把里面对应tag下的url增加一个就可以了。例: 上面这个立即就是有4个远端仓库，不同的tag表示不同的远端仓库，最后的Origin标签写法表示默认push到github和codaset这两个远端仓库去。当然，你可以自己随意定制tag和url 我有一个github的repo，clone没有问题，push的时候总是报错:error: The requested URL returned error: 403 while accessing xxx这个问题也困扰了我一段时间，后来发现修改 repo/.git/config 里面的url，把https地址替换为ssh就好了。 例如 1url=https://MichaelDrogalis@github.com/derekerdmann/lunch_call.git 替换为 1url=ssh://git@github.com/derekerdmann/lunch_call.git 参考http://stackoverflow.com/questions/7438313/pushing-to-git-returning-error-code-403-fatal-http-request-failed http://stackoverflow.com/questions/849308/pull-push-from-multiple-remote-locations/3195446#3195446]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2FZookeeper%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Zookeeper简单使用标签（空格分隔）： 大数据学习 ##zookeeper命令行操作 运行 zkCli.sh –server &lt;ip&gt;进入命令行工具 使用 ls 命令来查看当前 ZooKeeper 中所包含的内容： ls / 创建一个新的 znode ，使用 create /zk myData 。这个命令创建了一个新的 znode 节点“ zk ”以及与它关联的字符串： create /zk &quot;myData&quot; 我们运行 get 命令来确认 znode 是否包含我们所创建的字符串： get /zk 监听这个节点的变化,当另外一个客户端改变/zk时,它会打出下面的 get /zk watch WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/zk 下面我们通过 set 命令来对 zk 所关联的字符串进行设置： set /zk &quot;zsl&quot; 下面我们将刚才创建的 znode 删除： delete /zk 删除节点： rmr /zk ##zookeeper-api应用| 功能 | 描述 ||–|| create | 在本地目录树中创建一个节点||delete |删除一个节点||exists |测试本地是否存在目标节点||get/set data | 从目标节点上读取 / 写数据||get/set ACL | 获取 / 设置目标节点访问控制列表信息||get children | 检索一个子节点上的列表||sync | 等待要被传送的数据| ###demo代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import java.io.IOException;import java.util.List;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;import org.junit.Before;import org.junit.Test;public class SimpleZkClient &#123; private static final String connectString = &quot;192.168.127.61:2181,192.168.127.62:2181,192.168.127.63:2181&quot;; private static final int sessionTimeout = 2000; ZooKeeper zkClient = null; @Before public void init() throws Exception &#123; zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 收到事件通知后的回调函数（应该是我们自己的事件处理逻辑） System.out.println(event.getType() + &quot;---&quot; + event.getPath()); try &#123; zkClient.getChildren(&quot;/&quot;, true); &#125; catch (Exception e) &#123; &#125; &#125; &#125;); &#125; /** * 数据的增删改查 * * @throws InterruptedException * @throws KeeperException */ // 创建数据节点到zk中 public void testCreate() throws KeeperException, InterruptedException &#123; // 参数1：要创建的节点的路径 参数2：节点大数据 参数3：节点的权限 参数4：节点的类型 String nodeCreated = zkClient.create(&quot;/eclipse&quot;, &quot;hellozk&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); //上传的数据可以是任何类型，但都要转成byte[] &#125; //判断znode是否存在 @Test public void testExist() throws Exception&#123; Stat stat = zkClient.exists(&quot;/eclipse&quot;, false); System.out.println(stat==null?&quot;not exist&quot;:&quot;exist&quot;); &#125; // 获取子节点 @Test public void getChildren() throws Exception &#123; List&lt;String&gt; children = zkClient.getChildren(&quot;/&quot;, true); for (String child : children) &#123; System.out.println(child); &#125; Thread.sleep(Long.MAX_VALUE); &#125; //获取znode的数据 @Test public void getData() throws Exception &#123; byte[] data = zkClient.getData(&quot;/eclipse&quot;, false, null); System.out.println(new String(data)); &#125; //删除znode @Test public void deleteZnode() throws Exception &#123; //参数2：指定要删除的版本，-1表示删除所有版本 zkClient.delete(&quot;/eclipse&quot;, -1); &#125; //删除znode @Test public void setData() throws Exception &#123; zkClient.setData(&quot;/app1&quot;, &quot;imissyou angelababy&quot;.getBytes(), -1); byte[] data = zkClient.getData(&quot;/app1&quot;, false, null); System.out.println(new String(data)); &#125; &#125; ##Zookeeper的监听器工作机制 监听器是一个接口，我们的代码中可以实现Wather这个接口，实现其中的process方法，方法中即我们自己的业务逻辑 监听器的注册是在获取数据的操作中实现： getData(path,watch?)监听的事件是：节点数据变化事件 getChildren(path,watch?)监听的事件是：节点下的子节点增减变化事件 ###分布式共享锁的简单实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495import java.util.Collections;import java.util.List;import java.util.Random;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.EventType;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;public class DistributedClientLock &#123; // 会话超时 private static final int SESSION_TIMEOUT = 2000; // zookeeper集群地址 private String hosts = &quot;mini1:2181,mini2:2181,mini3:2181&quot;; private String groupNode = &quot;locks&quot;; private String subNode = &quot;sub&quot;; private boolean haveLock = false; private ZooKeeper zk; // 记录自己创建的子节点路径 private volatile String thisPath; /** * 连接zookeeper */ public void connectZookeeper() throws Exception &#123; zk = new ZooKeeper(hosts, SESSION_TIMEOUT, new Watcher() &#123; public void process(WatchedEvent event) &#123; try &#123; // 判断事件类型，此处只处理子节点变化事件 if (event.getType() == EventType.NodeChildrenChanged &amp;&amp; event.getPath().equals(&quot;/&quot; + groupNode)) &#123; //获取子节点，并对父节点进行监听 List&lt;String&gt; childrenNodes = zk.getChildren(&quot;/&quot; + groupNode, true); String thisNode = thisPath.substring((&quot;/&quot; + groupNode + &quot;/&quot;).length()); // 去比较是否自己是最小id Collections.sort(childrenNodes); if (childrenNodes.indexOf(thisNode) == 0) &#123; //访问共享资源处理业务，并且在处理完成之后删除锁 doSomething(); //重新注册一把新的锁 thisPath = zk.create(&quot;/&quot; + groupNode + &quot;/&quot; + subNode, null, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); // 1、程序一进来就先注册一把锁到zk上 thisPath = zk.create(&quot;/&quot; + groupNode + &quot;/&quot; + subNode, null, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); // wait一小会，便于观察 Thread.sleep(new Random().nextInt(1000)); // 从zk的锁父目录下，获取所有子节点，并且注册对父节点的监听 List&lt;String&gt; childrenNodes = zk.getChildren(&quot;/&quot; + groupNode, true); //如果争抢资源的程序就只有自己，则可以直接去访问共享资源 if (childrenNodes.size() == 1) &#123; doSomething(); thisPath = zk.create(&quot;/&quot; + groupNode + &quot;/&quot; + subNode, null, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); &#125; &#125; /** * 处理业务逻辑，并且在最后释放锁 */ private void doSomething() throws Exception &#123; try &#123; System.out.println(&quot;gain lock: &quot; + thisPath); Thread.sleep(2000); // do something &#125; finally &#123; System.out.println(&quot;finished: &quot; + thisPath); //释放锁 zk.delete(this.thisPath, -1); &#125; &#125; public static void main(String[] args) throws Exception &#123; DistributedClientLock dl = new DistributedClientLock(); dl.connectZookeeper(); Thread.sleep(Long.MAX_VALUE); &#125;&#125;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2Fnexo%2BgithubPage%2F</url>
    <content type="text"><![CDATA[前言：随着互联网浪潮的翻腾，国内外涌现出越来越多优秀的社交网站让用户分享信息更加便捷。然后，如果你是一个不甘寂寞的程序猿（媛），是否也想要搭建一个属于自己的个人网站，如果你曾经或者现在正有这样的想法，请跟随这篇文章发挥你的Geek精神，让你快速拥有自己的博客网站，写文章记录生活，享受这种从0到1的过程。 你见过的最棒的个人博客界面是什么样的？ 什么是Hexo ? Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Heroku上，是搭建博客的首选框架。这里我们选用的是GitHub，你没看错，全球最大的同性恋交友网站（逃……）。Hexo同时也是GitHub上的开源项目，参见：hexojs/hexo 如果想要更加全面的了解Hexo，可以到其官网 Hexo 了解更多的细节，因为Hexo的创建者是台湾人，对中文的支持很友好，可以选择中文进行查看。这里，默认各位猿/媛儿都知道GitHub就不再赘述。 这是我的个人博客效果: 吴润 · RunDouble 下面正式从零开始搭建年轻人的第一个网站。 搭建步骤： 获得个人网站域名 GitHub创建个人仓库 安装Git 安装Node.js 安装Hexo 推送网站 绑定域名 更换主题 初识MarkDown语法 发布文章 寻找图床 个性化设置 其他 附录 获得个人网站域名 域名是网站的入口，也是网站的第一印象，比如饿了么的官网的域名是：https://www.ele.me/ ，很是巧妙。常见的有com,cn,net,org等后缀，也有小众的xyz,me,io等后缀，根据你自己的喜好，选择不同的后缀，比如我选择就是常见的com后缀。很多小众奇特的后缀在大陆是没办法备案的，网站也就无法上线。然而使用GitHub托管我们的网站，完全不需要备案，因为托管我们的网站内容的服务器在美国，而且在国内备案流程也比较繁杂，时间需要一周左右。 申请域名的地方有很多，这里推荐阿里云：阿里云-为了无法计算的价值 申请入口：域名注册购买域名这也是我们整个搭建过程中惟一一个需要花钱的地方。如果你已经有了空闲域名就无需购买，直接使用即可。 GitHub创建个人仓库 登录到GitHub,如果没有GitHub帐号，使用你的邮箱注册GitHub帐号：Build software better, together 点击GitHub中的New repository创建新仓库，仓库名应该为：用户名.http://github.io这个用户名使用你的GitHub帐号名称代替，这是固定写法，比如我的仓库名为： 安装Git 什么是Git ?简单来说Git是开源的分布式版本控制系统，用于敏捷高效地处理项目。我们网站在本地搭建好了，需要使用Git同步到GitHub上。如果想要了解Git的细节，参看廖雪峰老师的Git教程：Git教程 从Git官网下载：Git - Downloading Package 现在的机子基本都是64位的，选择64位的安装包，下载后安装，在命令行里输入git测试是否安装成功，若安装失败，参看其他详细的Git安装教程。安装成功后，将你的Git与GitHub帐号绑定，鼠标右击打开Git Bash 或者在菜单里搜索Git Bash，设置user.name和user.email配置信息： 12git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub注册邮箱&quot; 生成ssh密钥文件： 1ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot; 然后直接三个回车即可，默认不需要设置密码然后找到生成的.ssh的文件夹中的id_rsa.pub密钥，将内容全部复制 打开GitHub_Settings_keys 页面，新建new SSH Key Title为标题，任意填即可，将刚刚复制的id_rsa.pub内容粘贴进去，最后点击Add SSH key。在Git Bash中检测GitHub公钥设置是否成功，输入 ssh git@github.com ： 如上则说明成功。这里之所以设置GitHub密钥原因是，通过非对称加密的公钥与私钥来完成加密，公钥放置在GitHub上，私钥放置在自己的电脑里。GitHub要求每次推送代码都是合法用户，所以每次推送都需要输入账号密码验证推送用户是否是合法用户，为了省去每次输入密码的步骤，采用了ssh，当你推送的时候，git就会匹配你的私钥跟GitHub上面的公钥是否是配对的，若是匹配就认为你是合法用户，则允许推送。这样可以保证每次的推送都是正确合法的。 安装Node.js Hexo基于Node.js，Node.js下载地址：Download | Node.js 下载安装包，注意安装Node.js会包含环境变量及npm的安装，安装后，检测Node.js是否安装成功，在命令行中输入 node -v : 检测npm是否安装成功，在命令行中输入npm -v : 到这了，安装Hexo的环境已经全部搭建完成。 安装Hexo Hexo就是我们的个人博客网站的框架， 这里需要自己在电脑常里创建一个文件夹，可以命名为Blog，Hexo框架与以后你自己发布的网页都在这个文件夹中。创建好后，进入文件夹中，按住shift键，右击鼠标点击命令行 使用npm命令安装Hexo，输入： 1npm install -g hexo-cli 这个安装时间较长耐心等待，安装完成后，初始化我们的博客，输入： 1hexo init blog 注意，这里的命令都是作用在刚刚创建的Blog文件夹中。 为了检测我们的网站雏形，分别按顺序输入以下三条命令： 12345hexo new test_my_sitehexo ghexo s 这些命令在后面作介绍，完成后，打开浏览器输入地址： localhost:4000 可以看出我们写出第一篇博客，只不过我下图是我修改过的配置，和你的显示不一样。 现在来介绍常用的Hexo 命令npm install hexo -g #安装Hexonpm update hexo -g #升级hexo init #初始化博客命令简写hexo n “我的博客” == hexo new “我的博客” #新建文章hexo g == hexo generate #生成hexo s == hexo server #启动服务预览hexo d == hexo deploy #部署hexo server #Hexo会监视文件变动并自动更新，无须重启服务器hexo server -s #静态模式hexo server -p 5000 #更改端口hexo server -i 192.168.1.1 #自定义 IPhexo clean #清除缓存，若是网页正常情况下可以忽略这条命令刚刚的三个命令依次是新建一篇博客文章、生成网页、在本地预览的操作。 推送网站 上面只是在本地预览，接下来要做的就是就是推送网站，也就是发布网站，让我们的网站可以被更多的人访问。在设置之前，需要解释一个概念，在blog根目录里的_config.yml文件称为站点配置文件，如下图 进入根目录里的themes文件夹，里面也有个_config.yml文件，这个称为主题配置文件，如下图 下一步将我们的Hexo与GitHub关联起来，打开站点的配置文件_config.yml，翻到最后修改为： deploy:type: gitrepo: 这里填入你之前在GitHub上创建仓库的完整路径，记得加上 .gitbranch: master参考如下： 保存站点配置文件。 其实就是给hexo d 这个命令做相应的配置，让hexo知道你要把blog部署在哪个位置，很显然，我们部署在我们GitHub的仓库里。最后安装Git部署插件，输入命令： 1npm install hexo-deployer-git --save 这时，我们分别输入三条命令： 123hexo clean hexo g hexo d 其实第三条的 hexo d 就是部署网站命令，d是deploy的缩写。完成后，打开浏览器，在地址栏输入你的放置个人网站的仓库路径，即 http://xxxx.github.io (知乎排版可能会出现”http://“字样，参考下图) 比如我的xxxx就是我的GitHub用户名： 你就会发现你的博客已经上线了，可以在网络上被访问了。 绑定域名 虽然在Internet上可以访问我们的网站，但是网址是GitHub提供的:http://xxxx.github.io (知乎排版可能会出现”http://“字样) 而我们想使用我们自己的个性化域名，这就需要绑定我们自己的域名。这里演示的是在阿里云万网的域名绑定，在国内主流的域名代理厂商也就阿里云和腾讯云。登录到阿里云，进入管理控制台的域名列表，找到你的个性化域名，进入解析 然后添加解析 包括添加三条解析记录，192.30.252.153是GitHub的地址，你也可以ping你的 http://xxxx.github.io 的ip地址，填入进去。第三个记录类型是CNAME，CNAME的记录值是：你的用户名.http://github.io 这里千万别弄错了。第二步，登录GitHub，进入之前创建的仓库，点击settings，设置Custom domain，输入你的域名 点击save保存。第三步，进入本地博客文件夹 ，进入blog/source目录下，创建一个记事本文件，输入你的域名，对，只要写进你自己的域名即可。如果带有www，那么以后访问的时候必须带有www完整的域名才可以访问，但如果不带有www，以后访问的时候带不带www都可以访问。所以建议，不要带有www。这里我还是写了www(不建议带有www): 保存，命名为CNAME ，注意保存成所有文件而不是txt文件。 完成这三步，进入blog目录中，按住shift键右击打开命令行，依次输入： 123hexo cleanhexo ghexo d 这时候打开浏览器在地址栏输入你的个性化域名将会直接进入你自己搭建的网站。 更换主题 如果你不喜欢Hexo默认的主题，可以更换不同的主题，主题传送门：Themes 我自己使用的是Next主题，可以在blog目录中的themes文件夹中查看你自己主题是什么。现在把默认主题更改成Next主题，在blog目录中（就是命令行的位置处于blog目录）打开命令行输入： 1git clone https://github.com/iissnan/hexo-theme-next themes/next 这是将Next主题下载到blog目录的themes主题下的next文件夹中。打开站点的_config.yml配置文件，修改主题为next 打开主题的_config.yml配置文件，不是站点主题文件，找到Scheme Settings next主题有三个样式，我用的是Pisces，你们可以自己试试看，选择你自己喜欢的样式（只需要把行首的#去除，#是注释），选择好后，再次部署网站，hexo g、hexo d，查看效果。选择其他主题，按照上述过程即可实现。 初识Markdown语法 Markdown是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。Markdown语法简洁明了、容易掌握，而且功能比纯文本更强，因此写博客使用它，可以让用户更加专注的写文章，而不需要费尽心力的考虑样式，相对于html已经算是轻量级语言，像有道云笔记也支持Markdown写作。并且Markdown完全兼容html，也就是可以在文章里直接插入html代码。比如给博文添加音乐，就可以直接把音乐的外链html代码插入文章中。具体语法参看：Markdown 语法说明(简体中文版) 可以说十分钟就可以入门。当然，工欲善其事必先利其器，选择一个好的Markdown编辑器也是非常重要的，这里推荐MarkPad 和The Markdown Editor for Windows ，这是带有预览效果的编辑器，也可以使用本地的文本编辑器，更多的Markdown的语法与编辑器自己可以搜索了解。 发布文章 我们开始正式发布上线博客文章，在命令行中输入： 1hexo n &quot;博客名字&quot; 我们会发现在blog根目录下的source文件夹中的_post文件夹中多了一个 博客名字.md 文件，使用Markdown编辑器打开，就可以开始你的个人博客之旅了，Markdown常用的样式也就十来种，完全能够满足一般博文的样式要求，这是我的一篇博文内容示例： 通过带有预览样式的Markdown编辑器实时预览书写的博文样式，也可以通过命令 hexo s –debug 在本地浏览器的localhost:4000 预览博文效果。写好博文并且样式无误后，通过hexo g、hexo d 生成、部署网页。随后可以在浏览器中输入域名浏览。 寻找图床 图床，当博文中有图片时，若是少量图片，可以直接把图片存放在source文件夹中，但这显然不合理的，因为图片会占据大量的存储的空间，加载的时候相对缓慢 ，这时考虑把博文里的图片上传到某一网站，然后获得外部链接，使用Markdown语法， 完成图片的插入，这种网站就被成为图床。常见的简易的图床网站有：贴图库图片外链 国内算比较好的图床我认为有两个：新浪微博和 七牛云 ，七牛云的使用方法可以参看其他文章。图床最重要的就是稳定速度快，所以在挑选图床的时候一定要仔细，下图是博文插入图片，使用图床外链的示例： 个性化设置 所谓的个性化设置就是根据个人需要添加不同的插件及功能。 基本的有： 在站点配置文件_config.yml修改基本的站点信息 依次是网站标题、副标题、网站描述、作者、网站头像外部链接、网站语言、时区等。 在主题配置文件_config.yml修改基本的主题信息，如： 博文打赏的微信、支付宝二维码图片，这里我是直接把这两张放在根目录的source文件夹中，并没有使用图床外链。 社交外链的设置，即在侧栏展示你的个人社交网站信息。 博文分享的插件jiathis，值设置为true。在配置文件中有很多的个性化设置，可以自尝试更多的修改。 进阶个性化： 添加网易云音乐 打开网页版的网易云音乐，选择喜欢的音乐，点击生成外链播放器 复制外链的代码 比如在侧栏插入这首歌的音乐播放器，修改 blog\themes\next\layout_macro的sidebar.swig文件，添加刚刚复制的外链代码 重新生成、部署网页，效果如下 设置背景 把你挑选的背景图片命名为：background.jpg，放在blog\themes\next\source\images里，在blog\themes\next\source\css_custom文件的custom.styl首部添加： 1234body &#123; background:url(/images/background.jpg); background-attachment: fixed;&#125; background-attachment: fixed;是固定背景图片。 这是设置一张静态图片作为背景，其实Next主题自带有动态的背景效果，修改主题配置文件中的canvas_nest: false为canvas_nest: true即可。 增加侧栏菜单条目 默认的侧栏菜单条目有：首页、归档、标签、关于、搜索等。如果你想要增加其他的菜单条目，修改主题配置文件_config.yml里的Menu Settings中的menu和menu_icons两个地方 其中menu里是配置菜单项对应的页面位置（如：/love），menu_icons对应菜单项的图标，这里的图标是来自于Font Awesome ，所以你需要在Font Awesome网站上找到你需要的icon，然后把该icon的名字写在menu_icons对应菜单名后面，注意冒号有一个英文输入状态的空格。设置好后，在命令行里输入： 1hexo new page &quot;你所要增加的菜单项名称（要和你在menu中的填写要匹配）&quot; 新建的页面在博客根目录下的source文件里，这时你就可以对新建的页面自定义设计。 还有更多的进阶个性化设置，如SEO、评论系统、个人头像、博客分享、订阅功能、High功能、404网页设置等，可以参看： 主题配置 - NexT 使用文档 第三方服务集成 - NexT 使用文档 内置标签 - NexT 使用文 档进阶设定 - NexT 使用文档 有很多人私信问我High功能特效如何设置，这里推荐一篇同是Next主题网站博主的文章：为Hexo Next主题添加哈林摇特效（五） 其他 终于写到这里了，也算是基本圆满完成了我的写作初衷，总结自己的一些经验，分享一些有趣的东西，不过脖子也是僵硬的受不了。我知道很多人想要建立自己的个人网站却一直没有付诸行动，希望这篇文章能给你一点点灵感与想法，just do it。本文是windows平台的搭建过程，其他平台可以参看相关资料，也有很多其他优秀的博客框架值得学习。如果有任何建议或想法、或疑问欢迎在评论区交流， 做好的网站不妨在评论区贴出网址，让大家一起学习。 进阶：制作自定义页面 Hexo与七牛云的最佳实践 附录 以下是评论区贴出网址的博客展示： …Miss.j BlogDiary… RILWEIC Xin’s Notes 飞鸟与鱼 无名博客 iTesting软件测试知识分享 赵小源的个人博客 华嘉熠’s Blog Cocoon 其他优秀个人博客展示： 翁天信 · Dandy Weng chaoxuprim KENJI ENDO Moorez CodeSky 代码之空 大江东去 UI Design Portfolio of JJ Ying JeyZhang jacklightChen | 得到的都是侥幸 edwardtoday Rafal Tomal - Web Designer and Genesis Developer Projects and Work 绘画爱好者 万世奇的博客 Guillermo Rauch]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2FMapReduce%E9%81%87%E5%88%B0%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[连接reduceManager错误错误信息 12345678910111213[root@fk01 mapreduce]# hadoop jar hadoop-mapreduce-examples-2.2.0.jar wordcount /input /ouput314/08/21 10:41:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable14/08/21 10:41:18 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:803214/08/21 10:41:19 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)14/08/21 10:41:20 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)14/08/21 10:41:21 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)14/08/21 10:41:22 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)14/08/21 10:41:23 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)14/08/21 10:41:24 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)14/08/21 10:41:25 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)14/08/21 10:41:26 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)14/08/21 10:41:27 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)14/08/21 10:41:28 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS) 解决办法：在yare-site.xml里添加如下信息之后问题得到解决 123456789101112&lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2FMAPREDUCE%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[MAPREDUCE学习##MAPREDUCE原理篇（1） ###为什么要MAPREDUCE 海量数据在单机上处理因为硬件资源限制，无法胜任 而一旦将单机版程序扩展到集群来分布式运行，将极大增加程序的复杂度和开发难度 引入mapreduce框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将分布式计算中的复杂性交由框架来处理 设想一个海量数据场景下的wordcount需求： 单机版：内存受限，磁盘受限，运算能力受限 分布式： 文件分布式存储（HDFS） 运算逻辑需要至少分成2个阶段（一个阶段独立并发，一个阶段汇聚） 运算程序如何分发 程序如何分配运算任务（切片） 两阶段的程序如何启动？如何协调？ 整个程序运行过程中的监控？容错？重试？ 可见在程序由单机版扩成分布式时，会引入大量的复杂工作。为了提高开发效率，可以将分布式程序中的公共功能封装成框架，让开发人员可以将精力集中于业务逻辑。 而mapreduce就是这样一个分布式程序的通用框架，其应对以上问题的整体结构如下： MRAppMaster(mapreduce application master) MapTask ReduceTask ###MAPREDUCE框架结构及核心运行机制 ####结构 一个完整的mapreduce程序在分布式运行时有三类实例进程： MRAppMaster：负责整个程序的过程调度及状态协调 mapTask：负责map阶段的整个数据处理流程 ReduceTask：负责reduce阶段的整个数据处理流程 ####MR程序运行流程 流程示意图 流程解析 1. 一个mr程序启动的时候，最先启动的是MRAppMaster，MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程 2. maptask进程启动之后，根据给定的数据切片范围进行数据处理，主体流程为： 1. 利用客户指定的inputformat来获取RecordReader读取数据，形成输入KV对 2. 将输入KV对传递给客户定义的map()方法，做逻辑运算，并将map()方法输出的KV对收集到缓存 3. 将缓存中的KV对按照K分区排序后不断溢写到磁盘文件 3. MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据范围（数据分区） 4. Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算，并收集运算输出的结果KV，然后调用客户指定的outputformat将结果数据输出到外部存储 ###MapTask并行度决定机制 maptask的并行度决定map阶段的任务处理并发度，进而影响到整个job的处理速度 那么，mapTask并行实例是否越多越好呢？其并行度又是如何决定呢？ mapTask并行度的决定机制 一个job的map阶段并行度由客户端在提交job时决定 而客户端对map阶段并行度的规划的基本逻辑为： 将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多个split），然后每一个split分配一个mapTask并行实例处理 这段逻辑及形成的切片规划描述文件，由FileInputFormat实现类的getSplits()方法完成，其过程如下图： ###FileInputFormat切片机制 切片定义在InputFormat类中的getSplit()方法 FileInputFormat中默认的切片机制： 简单地按照文件的内容长度进行切片 切片大小，默认等于block大小 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 比如待处理数据有两个文件： file1.txt 320M file2.txt 10M 经过FileInputFormat的切片机制运算后，形成的切片信息如下： file1.txt.split1– 0~128 file1.txt.split2– 128~256 file1.txt.split3– 256~320 file2.txt.split1– 0~10M FileInputFormat中切片的大小的参数配置 通过分析源码，在FileInputFormat中，计算切片大小的逻辑：Math.max(minSize, Math.min(maxSize, blockSize)); 切片主要由这几个值来运算决定 minsize：默认值：1 配置参数： mapreduce.input.fileinputformat.split.minsize maxsize：默认值：Long.MAXValue 配置参数：mapreduce.input.fileinputformat.split.maxsize blocksize 因此，默认情况下，切片大小=blocksize maxsize（切片最大值）： 参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值 minsize （切片最小值）： 参数调的比blockSize大，则可以让切片变得比blocksize还大 选择并发数的影响因素： 运算节点的硬件配置 运算任务的类型：CPU密集型还是IO密集型 运算任务的数 ###map并行度的经验之谈 如果硬件配置为2*12core + 64G，恰当的map并行度是大约每个节点20-100个map，最好每个map的执行时间至少一分钟。 如果job的每个map或者 reduce task的运行时间都只有30-40秒钟，那么就减少该job的map或者reduce数，每一个task(map|reduce)的setup和加入到调度器中进行调度，这个中间的过程可能都要花费几秒钟，所以如果每个task都非常快就跑完了，就会在task的开始和结束的时候浪费太多的时间。 配置task的JVM重用(JVM重用技术不是指同一Job的两个或两个以上的task可以同时运行于同一JVM上，而是排队按顺序执行。)可以改善该问题：（mapred.job.reuse.jvm.num.tasks，默认是1，表示一个JVM上最多可以顺序执行的task数目（属于同一个Job）是1。也就是说一个task启一个JVM） 如果input的文件非常的大，比如1TB，可以考虑将hdfs上的每个block size设大，比如设成256MB或者512MB ###ReduceTask并行度的决定 reducetask的并行度同样影响整个job的执行并发度和执行效率，但与maptask的并发数由切片数决定不同，Reducetask数量的决定是可以直接手动设置： 12//默认值是1，手动设置为4job.setNumReduceTasks(4); 如果数据分布不均匀，就有可能在reduce阶段产生数据倾斜 注意： reducetask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个reducetask 尽量不要运行太多的reduce task。对大多数job来说，最好rduce的个数最多和集群中的reduce持平，或者比集群的 reduce slots小。这个对于小集群而言，尤其重要。 ###MAPREDUCE程序初体验 Hadoop的发布包中内置了一个hadoop-mapreduce-example-2.4.1.jar，这个jar包中有各种MR示例程序，可以通过以下步骤运行： 启动hdfs，yarn 然后在集群中的任意一台服务器上启动执行程序（比如运行wordcount）： hadoop jar /home/hadoop/apps/hadoop-2.9.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.0.jar wordcount /wordcount/in /wordcount/out hadoop jar /home/hadoop/apps/hadoop-2.6.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.4.jar wordcount ：运行程序 /wordcount/data：需要统计的文件（HDFS的路径） /wordcount/out：统计后的结果输出文件（HDFS的路径） hadoop fs -ls /wordcount/out ##MAPREDUCE实践篇（1） ###MAPREDUCE 示例编写及编程规范 ####编程规范 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端) Mapper的输入数据是KV对的形式KV的类型可自定义 Mapper的输出数据是KV对的形式KV的类型可自定义 Mapper中的业务逻辑写在map()方法中 map()方法maptask进程对每一个\&lt;K,V&gt;调用一次 Reducer的输入数据类型对应Mapper的输出数据类型，也是KV Reducer的业务逻辑写在reduce()方法中 Reducetask进程对每一组相同k的\&lt;k,v>组调用一次reduce()方法 用户自定义的Mapper和Reducer都要继承各自的父类 整个程序需要一个Drvier来进行提交，提交的是一个描述了各种必要信息的job对象 ####wordcount示例编写 需求：在一堆给定的文本文件中统计输出每一个单词出现的总次数 定义一个mapper类 1234567891011121314151617181920212223242526272829303132333435363738394041import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;/** * KEYIN: 默认情况下，是mr框架所读到的一行文本的起始偏移量，Long, * 但是在hadoop中有自己的更精简的序列化接口，所以不直接用Long，而用LongWritable * * VALUEIN:默认情况下，是mr框架所读到的一行文本的内容，String，同上，用Text * * KEYOUT：是用户自定义逻辑处理完成之后输出数据中的key，在此处是单词，String，同上，用Text * VALUEOUT：是用户自定义逻辑处理完成之后输出数据中的value，在此处是单词次数，Integer，同上，用IntWritable * * @author * */public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; /** * map阶段的业务逻辑就写在自定义的map()方法中 * maptask会对每一行输入数据调用一次我们自定义的map()方法 */ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //将maptask传给我们的文本内容先转换成String String line = value.toString(); //根据空格将这一行切分成单词 String[] words = line.split(" "); //将单词输出为&lt;单词，1&gt; for(String word:words)&#123; //将单词作为key，将次数1作为value，以便于后续的数据分发，可以根据单词分发，以便于相同单词会到相同的reduce task context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; 定义一个reducer类 12345678910111213141516171819202122232425262728293031323334353637import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;/** * KEYIN, VALUEIN 对应 mapper输出的KEYOUT,VALUEOUT类型对应 * * KEYOUT, VALUEOUT 是自定义reduce逻辑处理结果的输出数据类型 * KEYOUT是单词 * VLAUEOUT是总次数 * @author * */public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; /** * &lt;angelababy,1&gt;&lt;angelababy,1&gt;&lt;angelababy,1&gt;&lt;angelababy,1&gt;&lt;angelababy,1&gt; * &lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt; * &lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt; * 入参key，是一组相同单词kv对的key */ @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count=0; /*Iterator&lt;IntWritable&gt; iterator = values.iterator(); while(iterator.hasNext())&#123; count += iterator.next().get(); &#125;*/ for(IntWritable value:values)&#123; count += value.get(); &#125; context.write(key, new IntWritable(count)); &#125;&#125; ​ 定义一个主类，用来描述job并提交job 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;/** * 相当于一个yarn集群的客户端 * 需要在此封装我们的mr程序的相关运行参数，指定jar包 * 最后提交给yarn * @author * */public class WordcountDriver &#123; public static void main(String[] args) throws Exception &#123; if (args == null || args.length == 0) &#123; args = new String[2]; args[0] = "hdfs://master:9000/wordcount/input/wordcount.txt"; args[1] = "hdfs://master:9000/wordcount/output8"; &#125; Configuration conf = new Configuration(); //设置的没有用! ??????// conf.set("HADOOP_USER_NAME", "hadoop");// conf.set("dfs.permissions.enabled", "false"); /*conf.set("mapreduce.framework.name", "yarn"); conf.set("yarn.resoucemanager.hostname", "mini1");*/ Job job = Job.getInstance(conf); /*job.setJar("/home/hadoop/wc.jar");*/ //指定本程序的jar包所在的本地路径 job.setJarByClass(WordcountDriver.class); //指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); //指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); //指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); //指定job的输出结果所在目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); //将job中配置的相关参数，以及job所用的java类所在的jar包，提交给yarn去运行 /*job.submit();*/ boolean res = job.waitForCompletion(true); System.exit(res?0:1); &#125;&#125; ​ ###MAPREDUCE程序运行模式 ####本地运行模式 mapreduce程序是被提交给LocalJobRunner在本地以单进程的形式运行 而处理的数据及输出结果可以在本地文件系统，也可以在hdfs上 怎样实现本地运行？写一个程序，不要带集群的配置文件本质是你的mr程序的conf中是否有mapreduce.framework.name=local以及yarn.resourcemanager.hostname参数. 本地模式非常便于进行业务逻辑的debug，只要在eclipse中打断点即可 如果在windows下想运行本地模式来测试程序逻辑，需要在windows中配置环境变量： ％HADOOP_HOME％ = d:/hadoop-2.6.1 %PATH% = ％HADOOP_HOME％\bin 并且要将d:/hadoop-2.6.1的lib和bin目录替换成windows平台编译的版本 ####集群运行模式 将mapreduce程序提交给yarn集群resourcemanager，分发到很多的节点上并发执行 处理的数据和输出结果应该位于hdfs文件系统 提交集群的实现步骤： 将程序打成JAR包，然后在集群的任意一个节点上用hadoop命令启动 hadoop jar wordcount.jar cn.demo.bigdata.mrsimple.WordCountDriver inputpath outputpath 直接在linux的eclipse中运行main方法 项目中要带参数：mapreduce.framework.name=yarn以及yarn的两个基本配置. 如果要在windows的eclipse中提交job给集群，则要修改YarnRunner类 mapreduce程序在集群中运行时的大体流程 ###MAPREDUCE中的Combiner 因为combiner在mapreduce过程中可能调用也肯能不调用，可能调一次也可能调多次. 所以：combiner使用的原则是：有或没有都不能影响业务逻辑 combiner是MR程序中Mapper和Reducer之外的一种组件 combiner组件的父类就是Reducer combiner和reducer的区别在于运行的位置： Combiner是在每一个maptask所在的节点运行 Reducer是接收全局所有Mapper的输出结果； combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量具体实现步骤：1、自定义一个combiner继承Reducer，重写reduce方法2、在job中设置： job.setCombinerClassCustomCombiner.class) combiner能够应用的前提是不能影响最终的业务逻辑 而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来 ##MAPREDUCE原理篇（2） ###mapreduce的shuffle机制 ####概述： mapreduce中，map阶段处理的数据如何传递给reduce阶段，是mapreduce框架中最关键的一个流程，这个流程就叫shuffle； shuffle: 洗牌、发牌——（核心机制：数据分区，排序，缓存）； 具体来说：就是将maptask输出的处理结果数据，分发给reducetask，并在分发的过程中，对数据按key进行了分区和排序； ####主要流程： Shuffle缓存流程： shuffle是MR处理流程中的一个过程，它的每一个处理步骤是分散在各个map task和reduce task节点上完成的，整体来看，分为3个操作： 分区partition Sort根据key排序 Combiner进行局部value的合并 ####详细流程 maptask收集我们的map()方法输出的kv对，放到内存缓冲区中 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件 多个溢出文件会被合并成大的溢出文件 在溢出过程中，及合并的过程中，都要调用partitioner进行分组和针对key进行排序 reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据 reducetask会取到同一个分区的来自不同maptask的结果文件，reducetask会将这些文件再进行合并（归并排序） 合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法） Shuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快缓冲区的大小可以通过参数调整, 参数：io.sort.mb 默认100M ####详细流程示意图 ###MAPREDUCE中的序列化 ####概述 Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系。。。。），不便于在网络中高效传输； 所以，hadoop自己开发了一套序列化机制（Writable），精简，高效 ####Jdk序列化和MR序列化之间的比较 简单代码验证两种序列化机制的差别： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class TestSeri &#123; public static void main(String[] args) throws Exception &#123; //定义两个ByteArrayOutputStream，用来接收不同序列化机制的序列化结果 ByteArrayOutputStream ba = new ByteArrayOutputStream(); ByteArrayOutputStream ba2 = new ByteArrayOutputStream(); //定义两个DataOutputStream，用于将普通对象进行jdk标准序列化 DataOutputStream dout = new DataOutputStream(ba); DataOutputStream dout2 = new DataOutputStream(ba2); ObjectOutputStream obout = new ObjectOutputStream(dout2); //定义两个bean，作为序列化的源对象 ItemBeanSer itemBeanSer = new ItemBeanSer(1000L, 89.9f); ItemBean itemBean = new ItemBean(1000L, 89.9f); //用于比较String类型和Text类型的序列化差别 Text atext = new Text("a"); // atext.write(dout); itemBean.write(dout); byte[] byteArray = ba.toByteArray(); //比较序列化结果 System.out.println(byteArray.length); for (byte b : byteArray) &#123; System.out.print(b); System.out.print(":"); &#125; System.out.println("-----------------------"); String astr = "a"; // dout2.writeUTF(astr); obout.writeObject(itemBeanSer); byte[] byteArray2 = ba2.toByteArray(); System.out.println(byteArray2.length); for (byte b : byteArray2) &#123; System.out.print(b); System.out.print(":"); &#125; &#125;&#125; ####自定义对象实现MR中的序列化接口 如果需要将自定义的bean放在key中传输，则还需要实现comparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序,此时，自定义的bean实现的接口应该是： public class FlowBean implements WritableComparable&lt;FlowBean&gt; 需要自己实现的方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;public class FlowBean implements Writable&#123; private long upFlow; private long dFlow; private long sumFlow; //反序列化时，需要反射调用空参构造函数，所以要显示定义一个 public FlowBean()&#123;&#125; public FlowBean(long upFlow, long dFlow) &#123; this.upFlow = upFlow; this.dFlow = dFlow; this.sumFlow = upFlow + dFlow; &#125; get/set..... /** * 反序列化的方法，反序列化时，从流中读取到的各个字段的顺序应该与序列化时写出去的顺序保持一致 */ @Override public void readFields(DataInput in) throws IOException &#123; upflow = in.readLong(); dflow = in.readLong(); sumflow = in.readLong(); &#125; /** * 序列化的方法 */ @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upflow); out.writeLong(dflow); //可以考虑不序列化总流量，因为总流量是可以通过上行流量和下行流量计算出来的 out.writeLong(sumflow); &#125; @Override public int compareTo(FlowBean o) &#123; //实现按照sumflow的大小倒序排序 return sumflow&gt;o.getSumflow()?-1:1; &#125; ###MapReduce与YARN ####YARN概述 Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而mapreduce等运算程序则相当于运行于操作系统之上的应用程序 ####YARN的重要概念 yarn并不清楚用户提交的程序的运行机制 yarn只提供运算资源的调度（用户程序向yarn申请资源，yarn就负责分配资源） yarn中的主管角色叫ResourceManager yarn中具体提供运算资源的角色叫NodeManager 这样一来，yarn其实就与运行的用户程序完全解耦，就意味着yarn上可以运行各种类型的分布式运算程序（mapreduce只是其中的一种），比如mapreduce. storm程序，spark程序，tez …… 所以，spark. storm等运算框架都可以整合在yarn上运行，只要他们各自的框架中有符合yarn规范的资源请求机制即可 Yarn就成为一个通用的资源调度平台，从此，企业中以前存在的各种运算集群都可以整合在一个物理集群上，提高资源利用率，方便数据共享 Yarn中运行运算程序的示例mapreduce程序的调度过程，如下图: ##MAPREDUCE实践篇（2） ###Mapreduce中的排序初步 ####需求 对日志数据中的上下行流量信息汇总，并输出按照总流量倒序排序的结果数据如下： 1363157985066 13726230503(手机号) 00-FD-07-A4-72-B8:CMCC 120.196.100.82 24 27 2481(上行流量) 24681(下行流量) 200 1363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 200 1363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 200 1363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 200 ####分析 基本思路：实现自定义的bean来封装流量信息，并将bean作为map输出的key来传输 MR程序在处理数据的过程中会对数据排序(map输出的kv对传输到reduce之前，会排序)，排序的依据是map输出的key 所以，我们如果要实现自己需要的排序规则，则可以考虑将排序因素放到key中，让key实现接口：WritableComparable 然后重写key的compareTo方法 ####实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowCount &#123; static class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //将一行内容转成string String line = value.toString(); //切分字段 String[] fields = line.split("\t"); //取出手机号 String phoneNbr = fields[1]; //取出上行流量下行流量 long upFlow = Long.parseLong(fields[fields.length-3]); long dFlow = Long.parseLong(fields[fields.length-2]); context.write(new Text(phoneNbr), new FlowBean(upFlow, dFlow)); &#125; &#125; static class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt;&#123; //&lt;183323,bean1&gt;&lt;183323,bean2&gt;&lt;183323,bean3&gt;&lt;183323,bean4&gt;....... @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123; long sum_upFlow = 0; long sum_dFlow = 0; //遍历所有bean，将其中的上行流量，下行流量分别累加 for(FlowBean bean: values)&#123; sum_upFlow += bean.getUpFlow(); sum_dFlow += bean.getdFlow(); &#125; FlowBean resultBean = new FlowBean(sum_upFlow, sum_dFlow); context.write(key, resultBean); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); /*conf.set("mapreduce.framework.name", "yarn"); conf.set("yarn.resoucemanager.hostname", "mini1");*/ Job job = Job.getInstance(conf); /*job.setJar("/home/hadoop/wc.jar");*/ //指定本程序的jar包所在的本地路径 job.setJarByClass(FlowCount.class); //指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); //指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); //指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); //指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); //指定job的输出结果所在目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); //将job中配置的相关参数，以及job所用的java类所在的jar包，提交给yarn去运行 /*job.submit();*/ boolean res = job.waitForCompletion(true); System.exit(res?0:1); &#125;&#125; ###Mapreduce中的分区Partitioner 需求 根据归属地输出流量统计数据结果到不同文件，以便于在查询统计结果时可以定位到省级范围进行 ####分析 Mapreduce中会将map输出的kv对，按照相同key分组，然后分发给不同的reducetask 默认的分发规则为：根据key的hashcode%reducetask数来分发 所以：如果要按照我们自己的需求进行分组，则需要改写数据分发（分组）组件Partitioner 自定义一个CustomPartitioner继承抽象类：Partitioner 然后在job对象中，设置自定义partitioner： job.setPartitionerClass(CustomPartitioner.class) ####实现 12345678910111213141516171819202122232425262728import java.util.HashMap;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;/** * K2 V2 对应的是map输出kv的类型 * @author * */public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt;&#123; public static HashMap&lt;String, Integer&gt; proviceDict = new HashMap&lt;String, Integer&gt;(); static&#123; proviceDict.put("136", 0); proviceDict.put("137", 1); proviceDict.put("138", 2); proviceDict.put("139", 3); &#125; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; String prefix = key.toString().substring(0, 3); Integer provinceId = proviceDict.get(prefix); return provinceId==null?4:provinceId; &#125;&#125; ###mapreduce数据压缩 ####概述 这是mapreduce的一种优化策略：通过压缩编码对mapper或者reducer的输出进行压缩，以减少磁盘IO，提高MR程序运行速度（但相应增加了cpu运算负担） Mapreduce支持将map输出的结果或者reduce输出的结果进行压缩，以减少网络IO或最终输出数据的体积 压缩特性运用得当能提高性能，但运用不当也可能降低性能 基本原则： 运算密集型的job，少用压缩 IO密集型的job，多用压缩 ####MR支持的压缩编码 Reducer输出压缩 在配置参数或在代码中都可以设置reduce的输出压缩 在配置参数中设置 mapreduce.output.fileoutputformat.compress=false mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec mapreduce.output.fileoutputformat.compress.type=RECORD 在代码中设置 123Job job = Job.getInstance(conf);FileOutputFormat.setCompressOutput(job, true);FileOutputFormat.setOutputCompressorClass(job, (Class&lt;? extends CompressionCodec&gt;) Class.forName("")); ####Mapper输出压缩 在配置参数或在代码中都可以设置reduce的输出压缩 在配置参数中设置 mapreduce.map.output.compress=false mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec 在代码中设置： 12conf.setBoolean(Job.MAP_OUTPUT_COMPRESS, true);conf.setClass(Job.MAP_OUTPUT_COMPRESS_CODEC, GzipCodec.class, CompressionCodec.class); ####压缩文件的读取 Hadoop自带的InputFormat类内置支持压缩文件的读取，比如TextInputformat类，在其initialize方法中： 1234567891011121314151617181920212223242526272829303132333435363738394041public void initialize(InputSplit genericSplit, TaskAttemptContext context) throws IOException &#123; FileSplit split = (FileSplit) genericSplit; Configuration job = context.getConfiguration(); this.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE); start = split.getStart(); end = start + split.getLength(); final Path file = split.getPath(); // open the file and seek to the start of the split final FileSystem fs = file.getFileSystem(job); fileIn = fs.open(file); //根据文件后缀名创建相应压缩编码的codec CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file); if (null!=codec) &#123; isCompressedInput = true; decompressor = CodecPool.getDecompressor(codec); //判断是否属于可切片压缩编码类型 if (codec instanceof SplittableCompressionCodec) &#123; final SplitCompressionInputStream cIn = ((SplittableCompressionCodec)codec).createInputStream( fileIn, decompressor, start, end, SplittableCompressionCodec.READ_MODE.BYBLOCK); //如果是可切片压缩编码，则创建一个CompressedSplitLineReader读取压缩数据 in = new CompressedSplitLineReader(cIn, job, this.recordDelimiterBytes); start = cIn.getAdjustedStart(); end = cIn.getAdjustedEnd(); filePosition = cIn; &#125; else &#123; //如果是不可切片压缩编码，则创建一个SplitLineReader读取压缩数据，并将文件输入流转换成解压数据流传递给普通SplitLineReader读取 in = new SplitLineReader(codec.createInputStream(fileIn, decompressor), job, this.recordDelimiterBytes); filePosition = fileIn; &#125; &#125; else &#123; fileIn.seek(start); //如果不是压缩文件，则创建普通SplitLineReader读取数据 in = new SplitLineReader(fileIn, job, this.recordDelimiterBytes); filePosition = fileIn; &#125; ​ ###更多MapReduce编程案例 ####reduce端join算法实现 1、需求： 订单数据表t_order： id date pid amount 1001 20150710 P0001 2 1002 20150710 P0001 3 1002 20150710 P0002 3 商品信息表t_product id name category_id price P0001 小米5 C01 2 P0002 锤子T1 C01 3 假如数据量巨大，两表的数据是以文件的形式存储在HDFS中，需要用mapreduce程序来实现一下SQL查询运算： select a.id,a.date,b.name,b.category_id,b.price from t_order a join t_product b on a.pid = b.id 2、实现机制： 通过将关联的条件作为map输出的key，将两表满足join条件的数据并携带数据所来源的文件信息，发往同一个reduce task，在reduce中进行数据的串联 1234567891011121314151617181920212223242526272829303132333435363738394041public class OrderJoin &#123; static class OrderJoinMapper extends Mapper&lt;LongWritable, Text, Text, OrderJoinBean&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 拿到一行数据，并且要分辨出这行数据所属的文件 String line = value.toString(); String[] fields = line.split("\t"); // 拿到itemid String itemid = fields[0]; // 获取到这一行所在的文件名（通过inpusplit） String name = "你拿到的文件名"; // 根据文件名，切分出各字段（如果是a，切分出两个字段，如果是b，切分出3个字段） OrderJoinBean bean = new OrderJoinBean(); bean.set(null, null, null, null, null); context.write(new Text(itemid), bean); &#125; &#125; static class OrderJoinReducer extends Reducer&lt;Text, OrderJoinBean, OrderJoinBean, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;OrderJoinBean&gt; beans, Context context) throws IOException, InterruptedException &#123; //拿到的key是某一个itemid,比如1000 //拿到的beans是来自于两类文件的bean // &#123;1000,amount&#125; &#123;1000,amount&#125; &#123;1000,amount&#125; --- &#123;1000,price,name&#125; //将来自于b文件的bean里面的字段，跟来自于a的所有bean进行字段拼接并输出 &#125; &#125;&#125; 缺点：这种方式中，join的操作是在reduce阶段完成，reduce端的处理压力太大，map节点的运算负载则很低，资源利用率不高，且在reduce阶段极易产生数据倾斜 解决方案： map端join实现方式 ####map端join算法实现 原理阐述 适用于关联表中有小表的情形； 可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行join并输出最终结果，可以大大提高join操作的并发度，加快处理速度 实现示例 先在mapper类中预先定义好小表，进行join 引入实际场景中的解决方案：一次加载数据库或者用distributedcache 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class TestDistributedCache &#123; static class TestDistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; FileReader in = null; BufferedReader reader = null; HashMap&lt;String,String&gt; b_tab = new HashMap&lt;String, String&gt;(); String localpath =null; String uirpath = null; //是在map任务初始化的时候调用一次 @Override protected void setup(Context context) throws IOException, InterruptedException &#123; //通过这几句代码可以获取到cache file的本地绝对路径，测试验证用 Path[] files = context.getLocalCacheFiles(); localpath = files[0].toString(); URI[] cacheFiles = context.getCacheFiles(); //缓存文件的用法——直接用本地IO来读取 //这里读的数据是map task所在机器本地工作目录中的一个小文件 in = new FileReader("b.txt"); reader =new BufferedReader(in); String line =null; while(null!=(line=reader.readLine()))&#123; String[] fields = line.split(","); b_tab.put(fields[0],fields[1]); &#125; IOUtils.closeStream(reader); IOUtils.closeStream(in); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //这里读的是这个map task所负责的那一个切片数据（在hdfs上） String[] fields = value.toString().split("\t"); String a_itemid = fields[0]; String a_amount = fields[1]; String b_name = b_tab.get(a_itemid); // 输出结果 1001 98.9 banan context.write(new Text(a_itemid), new Text(a_amount + "\t" + ":" + localpath + "\t" +b_name )); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(TestDistributedCache.class); job.setMapperClass(TestDistributedCacheMapper.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); //这里是我们正常的需要处理的数据所在路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); //不需要reducer job.setNumReduceTasks(0); //分发一个文件到task进程的工作目录 job.addCacheFile(new URI("hdfs://hadoop-server01:9000/cachefile/b.txt")); //分发一个归档文件到task进程的工作目录// job.addArchiveToClassPath(archive); //分发jar包到task节点的classpath下// job.addFileToClassPath(jarfile); job.waitForCompletion(true); &#125;&#125; ​ ####web日志预处理 需求： 对web访问日志中的各字段识别切分 去除日志中不合法的记录 根据KPI统计需求，生成各类访问请求过滤数据 实现代码： 定义一个bean，用来记录日志数据中的各数据字段 1234567891011121314151617181920212223242526272829public class WebLogBean &#123; private String remote_addr;// 记录客户端的ip地址 private String remote_user;// 记录客户端用户名称,忽略属性"-" private String time_local;// 记录访问时间与时区 private String request;// 记录请求的url与http协议 private String status;// 记录请求状态；成功是200 private String body_bytes_sent;// 记录发送给客户端文件主体内容大小 private String http_referer;// 用来记录从那个页面链接访问过来的 private String http_user_agent;// 记录客户浏览器的相关信息 private boolean valid = true;// 判断数据是否合法 get/set..... @Override public String toString() &#123; StringBuilder sb = new StringBuilder(); sb.append(this.valid); sb.append("\001").append(this.remote_addr); sb.append("\001").append(this.remote_user); sb.append("\001").append(this.time_local); sb.append("\001").append(this.request); sb.append("\001").append(this.status); sb.append("\001").append(this.body_bytes_sent); sb.append("\001").append(this.http_referer); sb.append("\001").append(this.http_user_agent); return sb.toString(); &#125;&#125; 定义一个parser用来解析过滤web访问日志原始记录 1234567891011121314151617181920212223242526272829303132public class WebLogParser &#123; public static WebLogBean parser(String line) &#123; WebLogBean webLogBean = new WebLogBean(); String[] arr = line.split(" "); if (arr.length &gt; 11) &#123; webLogBean.setRemote_addr(arr[0]); webLogBean.setRemote_user(arr[1]); webLogBean.setTime_local(arr[3].substring(1)); webLogBean.setRequest(arr[6]); webLogBean.setStatus(arr[8]); webLogBean.setBody_bytes_sent(arr[9]); webLogBean.setHttp_referer(arr[10]); if (arr.length &gt; 12) &#123; webLogBean.setHttp_user_agent(arr[11] + " " + arr[12]); &#125; else &#123; webLogBean.setHttp_user_agent(arr[11]); &#125; if (Integer.parseInt(webLogBean.getStatus()) &gt;= 400) &#123;// 大于400，HTTP错误 webLogBean.setValid(false); &#125; &#125; else &#123; webLogBean.setValid(false); &#125; return webLogBean; &#125; public static String parserTime(String time) &#123; time.replace("/", "-"); return time; &#125;&#125; mapreduce程序 12345678910111213141516171819202122232425262728293031323334353637public class WeblogPreProcess &#123; static class WeblogPreProcessMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; Text k = new Text(); NullWritable v = NullWritable.get(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); WebLogBean webLogBean = WebLogParser.parser(line); if (!webLogBean.isValid()) return; k.set(webLogBean.toString()); context.write(k, v); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(WeblogPreProcess.class); job.setMapperClass(WeblogPreProcessMapper.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.waitForCompletion(true); &#125;&#125;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2Fmapreduce%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[MAPREDUCE进阶学习##自定义inputFormat ###需求 无论hdfs还是mapreduce，对于小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案 ###分析 小文件的优化无非以下几种方式： 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS 在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并 在mapreduce处理时，可采用combineInputFormat提高效率 ###实现 程序的核心机制： 自定义一个InputFormat 改写RecordReader，实现一次读取一个完整文件封装为KV 在输出时使用SequenceFileOutPutFormat输出合并文件 代码如下： 自定义InputFromat 1234567891011121314151617public class WholeFileInputFormat extends FileInputFormat&lt;NullWritable, BytesWritable&gt; &#123; //设置每个小文件不可分片,保证一个小文件生成一个key-value键值对 @Override protected boolean isSplitable(JobContext context, Path file) &#123; return false; &#125; @Override public RecordReader&lt;NullWritable, BytesWritable&gt; createRecordReader( InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; WholeFileRecordReader reader = new WholeFileRecordReader(); reader.initialize(split, context); return reader; &#125;&#125; ​ 自定义RecordReader 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class WholeFileRecordReader extends RecordReader&lt;NullWritable, BytesWritable&gt; &#123; private FileSplit fileSplit; private Configuration conf; private BytesWritable value = new BytesWritable(); private boolean processed = false; @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; this.fileSplit = (FileSplit) split; this.conf = context.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if (!processed) &#123; byte[] contents = new byte[(int) fileSplit.getLength()]; Path file = fileSplit.getPath(); FileSystem fs = file.getFileSystem(conf); FSDataInputStream in = null; try &#123; in = fs.open(file); IOUtils.readFully(in, contents, 0, contents.length); value.set(contents, 0, contents.length); &#125; finally &#123; IOUtils.closeStream(in); &#125; processed = true; return true; &#125; return false; &#125; @Override public NullWritable getCurrentKey() throws IOException, InterruptedException &#123; return NullWritable.get(); &#125; @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; @Override public float getProgress() throws IOException &#123; return processed ? 1.0f : 0.0f; &#125; @Override public void close() throws IOException &#123; // do nothing &#125;&#125; 定义mapreduce处理流程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class SmallFilesToSequenceFileConverter extends Configured implements Tool &#123; static class SequenceFileMapper extends Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt; &#123; private Text filenameKey; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; InputSplit split = context.getInputSplit(); Path path = ((FileSplit) split).getPath(); filenameKey = new Text(path.toString()); &#125; @Override protected void map(NullWritable key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; context.write(filenameKey, value); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; Configuration conf = new Configuration(); System.setProperty("HADOOP_USER_NAME", "hdfs"); String[] otherArgs = new GenericOptionsParser(conf, args) .getRemainingArgs(); if (otherArgs.length != 2) &#123; System.err.println("Usage: combinefiles &lt;in&gt; &lt;out&gt;"); System.exit(2); &#125; Job job = Job.getInstance(conf,"combine small files to sequencefile");// job.setInputFormatClass(WholeFileInputFormat.class); job.setOutputFormatClass(SequenceFileOutputFormat.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); job.setMapperClass(SequenceFileMapper.class); return job.waitForCompletion(true) ? 0 : 1; &#125; public static void main(String[] args) throws Exception &#123; int exitCode = ToolRunner.run(new SmallFilesToSequenceFileConverter(), args); System.exit(exitCode); &#125;&#125; ​ ##自定义outputFormat ###需求 现有一些原始日志需要做增强解析处理，流程： 从原始日志文件中读取数据 根据日志中的一个URL字段到外部知识库中获取信息增强到原始日志 如果成功增强，则输出到增强结果目录；如果增强失败，则抽取原始数据中URL字段输出到待爬清单目录 ###分析 程序的关键点是要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现 ###实现 实现要点： 在mapreduce中访问外部资源 自定义outputformat，改写其中的recordwriter，改写具体输出数据的方法write() 代码实现如下： 数据库获取数据的工具 12345678910111213141516171819202122232425262728293031323334353637383940414243public class DBLoader &#123; public static void dbLoader(HashMap&lt;String, String&gt; ruleMap) &#123; Connection conn = null; Statement st = null; ResultSet res = null; try &#123; Class.forName("com.mysql.jdbc.Driver"); conn = DriverManager.getConnection("jdbc:mysql://hdp-node01:3306/urlknowledge", "root", "root"); st = conn.createStatement(); res = st.executeQuery("select url,content from urlcontent"); while (res.next()) &#123; ruleMap.put(res.getString(1), res.getString(2)); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try&#123; if(res!=null)&#123; res.close(); &#125; if(st!=null)&#123; st.close(); &#125; if(conn!=null)&#123; conn.close(); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; DBLoader db = new DBLoader(); HashMap&lt;String, String&gt; map = new HashMap&lt;String,String&gt;(); db.dbLoader(map); System.out.println(map.size()); &#125;&#125; 自定义一个outputformat 12345public class LogEnhancerOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException &#123; FileSystem fs = FileSystem.get(context.getConfiguration()); Path enhancePath = new Path(&quot;hdfs://hdp-node01:9000/flow/enhancelog/enhanced.log&quot;); Path toCrawlPath = new Path(&quot;hdfs://hdp-node01:9000/flow/tocrawl/tocrawl.log&quot;); FSDataOutputStream enhanceOut = fs.create(enhancePath); FSDataOutputStream toCrawlOut = fs.create(toCrawlPath); return new MyRecordWriter(enhanceOut,toCrawlOut); } static class MyRecordWriter extends RecordWriter&lt;Text, NullWritable&gt;{ FSDataOutputStream enhanceOut = null; FSDataOutputStream toCrawlOut = null; public MyRecordWriter(FSDataOutputStream enhanceOut, FSDataOutputStream toCrawlOut) { this.enhanceOut = enhanceOut; this.toCrawlOut = toCrawlOut; } @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException { //有了数据，你来负责写到目的地 —— hdfs //判断，进来内容如果是带tocrawl的，就往待爬清单输出流中写 toCrawlOut if(key.toString().contains(&quot;tocrawl&quot;)){ toCrawlOut.write(key.toString().getBytes()); }else{ enhanceOut.write(key.toString().getBytes()); } } @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException { if(toCrawlOut!=null){ toCrawlOut.close(); } if(enhanceOut!=null){ enhanceOut.close(); } } } } ​1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980- 开发mapreduce处理流程 - ```java /** * 这个程序是对每个小时不断产生的用户上网记录日志进行增强(将日志中的url所指向的网页内容分析结果信息追加到每一行原始日志后面) * * @author * */ public class LogEnhancer &#123; static class LogEnhancerMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; HashMap&lt;String, String&gt; knowledgeMap = new HashMap&lt;String, String&gt;(); /** * maptask在初始化时会先调用setup方法一次 利用这个机制，将外部的知识库加载到maptask执行的机器内存中 */ @Override protected void setup(org.apache.hadoop.mapreduce.Mapper.Context context) throws IOException, InterruptedException &#123; DBLoader.dbLoader(knowledgeMap); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] fields = StringUtils.split(line, &quot;\t&quot;); try &#123; String url = fields[26]; // 对这一行日志中的url去知识库中查找内容分析信息 String content = knowledgeMap.get(url); // 根据内容信息匹配的结果，来构造两种输出结果 String result = &quot;&quot;; if (null == content) &#123; // 输往待爬清单的内容 result = url + &quot;\t&quot; + &quot;tocrawl\n&quot;; &#125; else &#123; // 输往增强日志的内容 result = line + &quot;\t&quot; + content + &quot;\n&quot;; &#125; context.write(new Text(result), NullWritable.get()); &#125; catch (Exception e) &#123; &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(LogEnhancer.class); job.setMapperClass(LogEnhancerMapper.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 要将自定义的输出格式组件设置到job中 job.setOutputFormatClass(LogEnhancerOutputFormat.class); FileInputFormat.setInputPaths(job, new Path(args[0])); // 虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat // 而fileoutputformat要输出一个_SUCCESS文件，所以，在这还得指定一个输出目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); job.waitForCompletion(true); System.exit(0); &#125; &#125; ​ ##自定义GroupingComparator ###需求 有如下订单数据 订单id 商品id 成交金额 Order_0000001 Pdt_01 222.8 Order_0000001 Pdt_05 25.8 Order_0000002 Pdt_03 522.8 Order_0000002 Pdt_04 122.4 Order_0000003 Pdt_01 222.8 现在需要求出每一个订单中成交金额最大的一笔交易 ###分析 利用“订单id和成交金额”作为key，可以将map阶段读取到的所有订单数据按照id分区，按照金额排序，发送到reduce 在reduce端利用groupingcomparator将订单id相同的kv聚合成组，然后取第一个即是最大值 ###实现 自定义groupingcomparator 123456789101112131415161718192021/** * 用于控制shuffle过程中reduce端对kv对的聚合逻辑 * @author duanhaitao@itcast.cn * */public class ItemidGroupingComparator extends WritableComparator &#123; protected ItemidGroupingComparator() &#123; super(OrderBean.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; OrderBean abean = (OrderBean) a; OrderBean bbean = (OrderBean) b; //将item_id相同的bean都视为相同，从而聚合为一组 return abean.getItemid().compareTo(bbean.getItemid()); &#125;&#125; 定义订单信息bean 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * 订单信息bean，实现hadoop的序列化机制 * @author duanhaitao@itcast.cn * */public class OrderBean implements WritableComparable&lt;OrderBean&gt;&#123; private Text itemid; private DoubleWritable amount; public OrderBean() &#123; &#125; public OrderBean(Text itemid, DoubleWritable amount) &#123; set(itemid, amount); &#125; public void set(Text itemid, DoubleWritable amount) &#123; this.itemid = itemid; this.amount = amount; &#125; public Text getItemid() &#123; return itemid; &#125; public DoubleWritable getAmount() &#123; return amount; &#125; @Override public int compareTo(OrderBean o) &#123; int cmp = this.itemid.compareTo(o.getItemid()); if (cmp == 0) &#123; cmp = -this.amount.compareTo(o.getAmount()); &#125; return cmp; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(itemid.toString()); out.writeDouble(amount.get()); &#125; @Override public void readFields(DataInput in) throws IOException &#123; String readUTF = in.readUTF(); double readDouble = in.readDouble(); this.itemid = new Text(readUTF); this.amount= new DoubleWritable(readDouble); &#125; @Override public String toString() &#123; return itemid.toString() + "\t" + amount.get(); &#125;&#125; 编写mapreduce处理流程 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * 利用secondarysort机制输出每种item订单金额最大的记录 * @author duanhaitao@itcast.cn * */public class SecondarySort &#123; static class SecondarySortMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt;&#123; OrderBean bean = new OrderBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] fields = StringUtils.split(line, "\t"); bean.set(new Text(fields[0]), new DoubleWritable(Double.parseDouble(fields[1]))); context.write(bean, NullWritable.get()); &#125; &#125; static class SecondarySortReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt;&#123; //在设置了groupingcomparator以后，这里收到的kv数据 就是： &lt;1001 87.6&gt;,null &lt;1001 76.5&gt;,null .... //此时，reduce方法中的参数key就是上述kv组中的第一个kv的key：&lt;1001 87.6&gt; //要输出同一个item的所有订单中最大金额的那一个，就只要输出这个key @Override protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(SecondarySort.class); job.setMapperClass(SecondarySortMapper.class); job.setReducerClass(SecondarySortReducer.class); job.setOutputKeyClass(OrderBean.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); //指定shuffle所使用的GroupingComparator类 job.setGroupingComparatorClass(ItemidGroupingComparator.class); //指定shuffle所使用的partitioner类 job.setPartitionerClass(ItemIdPartitioner.class); job.setNumReduceTasks(3); job.waitForCompletion(true); &#125;&#125; ​ ##Mapreduce中的DistributedCache应用 ###Map端join案例 ####需求 实现两个“表”的join操作，其中一个表数据量小，一个表很大，这种场景在实际中非常常见，比如“订单日志” join “产品信息” ####分析 原理阐述 适用于关联表中有小表的情形； 可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行join并输出最终结果 可以大大提高join操作的并发度，加快处理速度 示例：先在mapper类中预先定义好小表，进行join 并用distributedcache机制将小表的数据分发到每一个maptask执行节点，从而每一个maptask节点可以从本地加载到小表的数据，进而在本地即可实现join ####实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class TestDistributedCache &#123; static class TestDistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; FileReader in = null; BufferedReader reader = null; HashMap&lt;String,String&gt; b_tab = new HashMap&lt;String, String&gt;(); String localpath =null; String uirpath = null; //是在map任务初始化的时候调用一次 @Override protected void setup(Context context) throws IOException, InterruptedException &#123; //通过这几句代码可以获取到cache file的本地绝对路径，测试验证用 Path[] files = context.getLocalCacheFiles(); localpath = files[0].toString(); URI[] cacheFiles = context.getCacheFiles(); //缓存文件的用法——直接用本地IO来读取 //这里读的数据是map task所在机器本地工作目录中的一个小文件 in = new FileReader("b.txt"); reader =new BufferedReader(in); String line =null; while(null!=(line=reader.readLine()))&#123; String[] fields = line.split(","); b_tab.put(fields[0],fields[1]); &#125; IOUtils.closeStream(reader); IOUtils.closeStream(in); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //这里读的是这个map task所负责的那一个切片数据（在hdfs上） String[] fields = value.toString().split("\t"); String a_itemid = fields[0]; String a_amount = fields[1]; String b_name = b_tab.get(a_itemid); // 输出结果 1001 98.9 banan context.write(new Text(a_itemid), new Text(a_amount + "\t" + ":" + localpath + "\t" +b_name )); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(TestDistributedCache.class); job.setMapperClass(TestDistributedCacheMapper.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); //这里是我们正常的需要处理的数据所在路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); //不需要reducer job.setNumReduceTasks(0); //分发一个文件到task进程的工作目录 job.addCacheFile(new URI("hdfs://hadoop-server01:9000/cachefile/b.txt")); //分发一个归档文件到task进程的工作目录// job.addArchiveToClassPath(archive); //分发jar包到task节点的classpath下// job.addFileToClassPath(jarfile); job.waitForCompletion(true); &#125;&#125; ##Mapreduce的其他补充 ###计数器应用 在实际生产代码中，常常需要将数据处理过程中遇到的不合规数据行进行全局计数，类似这种需求可以借助mapreduce框架中提供的全局计数器来实现 示例代码如下： 12345678910111213141516171819public class MultiOutputs &#123; //通过枚举形式定义自定义计数器 enum MyCounter&#123;MALFORORMED,NORMAL&#125; static class CommaMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] words = value.toString().split(","); for (String word : words) &#123; context.write(new Text(word), new LongWritable(1)); &#125; //对枚举定义的自定义计数器加1 context.getCounter(MyCounter.MALFORORMED).increment(1); //通过动态设置自定义计数器加1 context.getCounter("counterGroupa", "countera").increment(1); &#125; &#125; ###多job串联 一个稍复杂点的处理逻辑往往需要多个mapreduce程序串联处理，多job的串联可以借助mapreduce框架的JobControl实现 1234567891011121314151617181920212223242526ControlledJob cJob1 = new ControlledJob(job1.getConfiguration()); ControlledJob cJob2 = new ControlledJob(job2.getConfiguration()); ControlledJob cJob3 = new ControlledJob(job3.getConfiguration()); // 设置作业依赖关系 cJob2.addDependingJob(cJob1); cJob3.addDependingJob(cJob2); JobControl jobControl = new JobControl("RecommendationJob"); jobControl.addJob(cJob1); jobControl.addJob(cJob2); jobControl.addJob(cJob3); cJob1.setJob(job1); cJob2.setJob(job2); cJob3.setJob(job3); // 新建一个线程来运行已加入JobControl中的作业，开始进程并等待结束 Thread jobControlThread = new Thread(jobControl); jobControlThread.start(); while (!jobControl.allFinished()) &#123; Thread.sleep(500); &#125; jobControl.stop(); return 0; ##mapreduce参数优化 ###资源相关参数 mapreduce.map.memory.mb: 一个Map Task可使用的资源上限（单位:MB），默认为1024。如果Map Task实际使用的资源量超过该值，则会被强制杀死。 mapreduce.reduce.memory.mb: 一个Reduce Task可使用的资源上限（单位:MB），默认为1024。如果Reduce Task实际使用的资源量超过该值，则会被强制杀死。 mapreduce.map.java.opts: Map Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g.“-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc” （@taskid@会被Hadoop框架自动换为相应的taskid）, 默认值: “” mapreduce.reduce.java.opts: Reduce Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g.“-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc”, 默认值: “” mapreduce.map.cpu.vcores: 每个Map task可使用的最多cpu core数目, 默认值: 1 mapreduce.map.cpu.vcores: 每个Reduce task可使用的最多cpu core数目, 默认值: 1 yarn.scheduler.minimum-allocation-mb 1024 yarn.scheduler.maximum-allocation-mb 8192 yarn.scheduler.minimum-allocation-vcores1 yarn.scheduler.maximum-allocation-vcores32 ###容错相关参数 mapreduce.map.maxattempts: 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。 mapreduce.reduce.maxattempts: 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。 mapreduce.map.failures.maxpercent: 当失败的Map Task失败比例超过该值为，整个作业则失败，默认值为0. 如果你的应用程序允许丢弃部分输入数据，则该该值设为一个大于0的值，比如5，表示如果有低于5%的Map Task失败（如果一个Map Task重试次数超过mapreduce.map.maxattempts，则认为这个Map Task失败，其对应的输入数据将不会产生任何结果），整个作业扔认为成功。 mapreduce.reduce.failures.maxpercent: 当失败的Reduce Task失败比例超过该值为，整个作业则失败，默认值为0. mapreduce.task.timeout: Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该task处于block状态，可能是卡住了，也许永远会卡主，为了防止因为用户程序永远block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是300000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。 ###本地运行mapreduce 作业 设置以下几个参数: mapreduce.framework.name=local mapreduce.jobtracker.address=local fs.defaultFS=local ###效率和稳定性相关参数 mapreduce.map.speculative: 是否为Map Task打开推测执行机制，默认为false mapreduce.reduce.speculative: 是否为Reduce Task打开推测执行机制，默认为false mapreduce.job.user.classpath.first &amp; mapreduce.task.classpath.user.precedence：当同一个class同时出现在用户jar包和hadoop jar中时，优先使用哪个jar包中的class，默认为false，表示优先使用hadoop jar中的class。 mapreduce.input.fileinputformat.split.minsize: 每个Map Task处理的数据量（仅针对基于文件的Inputformat有效，比如TextInputFormat，SequenceFileInputFormat），默认为一个block大小，即 134217728。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2FLet's%20Encrypt%20%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%EF%BC%8C%E5%85%8D%E8%B4%B9%E7%9A%84%20SSL%20%E8%AF%81%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[Let’s Encrypt 简介Let’s Encrypt 是国外一个公共的免费SSL项目，由 Linux 基金会托管，它的来头不小，由 Mozilla、思科、Akamai、IdenTrust 和 EFF 等组织发起，目的就是向网站自动签发和管理免费证书，以便加速互联网由 HTTP 过渡到 HTTPS，目前 Facebook 等大公司开始加入赞助行列。 Let’s Encrypt 已经得了 IdenTrust 的交叉签名，这意味着其证书现在已经可以被 Mozilla、Google、Microsoft 和 Apple 等主流的浏览器所信任，你只需要在 Web 服务器证书链中配置交叉签名，浏览器客户端会自动处理好其它的一切，Let’s Encrypt 安装简单，使用非常方便。 Certbot 简介Certbot 为 Let’s Encrypt 项目发布了一个官方的客户端 Certbot ，利用它可以完全自动化的获取、部署和更新安全证书，并且 Certbot 是支持所有 Unix 内核的操作系统。 安装 Certbot 客户端12$ yum install certbot # centos$ # apt install certbot # ubuntu Certbot 的两种使用方式webroot 方式： certbot 会利用既有的 web server，在其 web root 目录下创建隐藏文件，Let’s Encrypt 服务端会通过域名来访问这些隐藏文件，以确认你的确拥有对应域名的控制权。standalone 方式： Certbot 会自己运行一个 web server 来进行验证。如果我们自己的服务器上已经有 web server 正在运行 （比如 Nginx 或 Apache ），用 standalone 方式的话需要先关掉它，以免冲突。获取证书 webroot 模式使用这种模式会在 web root 中创建 .well-known 文件夹，这个文件夹里面包含了一些验证文件，Certbot 会通过访问 example.com/.well-known/acme-challenge 来验证你的域名是否绑定的这个服务器，所以需要编辑 nginx 配置文件确保可以访问刚刚创建的 .well-known 文件夹及里边存放的验证文件，以便在生成证书时进行验证： 使用一下命令查看 nginx 配置文件地址： 123$ sudo nginx -tnginx: the configuration file /usr/local/nginx/nginx.conf syntax is ok nginx: configuration file /usr/local/nginx/nginx.conf test is successful 编辑 /usr/local/nginx/nginx.conf 配置 12345678server &#123; ... location /.well-known/acme-challenge/ &#123; default_type &quot;text/plain&quot;; root /var/www/example; &#125; ...&#125; 重启 nginx 服务 $ nginx -s reload获取证书，–email 为申请者邮箱，–webroot 为 webroot 方式，-w 为站点目录，-d 为要加 https 的域名，以下命令会为 example.com 和 www.example.com 这两个域名生成一个证书： $ certbot certonly --email admin@example.com --webroot -w /var/www/example -d example.com -d www.example.comstandalone 模式获取证书 但是有些时候我们的一些服务并没有根目录，例如一些微服务，这时候使用 webroot 模式就走不通了。这时可以使用模式 standalone 模式，这种模式不需要指定网站根目录，他会自动启用服务器的443端口，来验证域名的归属。我们有其他服务（例如nginx）占用了443端口，就必须先停止这些服务，在证书生成完毕后，再启用。 $ certbot certonly --email admin@example.com --standalone -d example.com -d www.example.comnginx 开启 https 证书生成完成后可以到 /etc/letsencrypt/live/ 目录下查看对应域名的证书文件。编辑 nginx 配置文件监听 443 端口，启用 SSL，并配置 SSL 的公钥、私钥证书路径： 1234567891011server &#123; listen 443; server_name example.com; root /var/www/example; index index.html index.htm; ssl on; ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem; charset utf-8; ...&#125; 添加 HTTP 跳转到 HTTPS： 12345server &#123; listen 80; server_name example.com; return 301 https://$server_name$request_uri;&#125; 重启 nginx 服务，访问 https://example.com 查看是否配置成功 自动续期 Let’s Encrypt 提供的证书只有90天的有效期，所以我们要在在证书到期之前重新获取这些证书，Certbot 提供了一个方便的命令 certbot renew，我们可以先使用 –dry-run 测试是否可用： $ certbot renew --dry-runlinux 系统上有 cron 可以来搞定这件事情，使用一下命令新建任务： $ crontab -e写入一下任务内容。这段内容的意思就是 每隔 两个月的 凌晨 2:15 执行 更新操作 15 2 * */2 * certbot renew --quiet --renew-hook &quot;service nginx restart&quot;参数 表述 –quiet 执行时屏蔽错误以外的所有输出，也可以使用 -q –pre-hook 执行更新操作之前要做的事情 –pre-hook 执行更新操作之前要做的事情 –post-hook 执行更新操作完成后要做的事情 取消证书 可以使用一下命令取消刚刚生成的密匙，也就是以上的反操作： 12$ certbot revoke --cert-path /etc/letsencrypt/live/example.com/cert.pem$ certbot delete --cert-name example.com 至此，整个网站升级到 HTTPS 就完成了。 七牛静态资源也能使用 HTTPS 如果博客里面没有用到外部的一些静态图片等资源，那这时候访问网站，应该会看到地址栏有一把小绿锁了。但是我的博客一直使用的是七牛的免费图床，一直都是 HTTP 外链，所以地址栏的小绿锁也没有显示。 结束语 如果加上 HTTPS 之后😊锁不够绿的话，检查下站点加载的资源（比如 js、css、照片等）是不是有 HTTP 的，有的话就会导致小锁变为灰色。 在我们使用 HTTP 的时候，打开网页总会遇到第三方偷偷加的一些脚本广告，很是烦人，升级 HTTPS 后他们就无从下手了，欧耶。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2FJava%E5%8F%8D%E5%B0%84%2F</url>
    <content type="text"><![CDATA[Java反射##反射的代码示例 通过反射的方式可以获取class对象中的属性、方法、构造函数等，一下是实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154package cn.java.reflect;import java.lang.reflect.Constructor;import java.lang.reflect.Field;import java.lang.reflect.Method;import java.util.ArrayList;import java.util.List;import org.junit.Before;import org.junit.Test;public class MyReflect &#123; public String className = null; @SuppressWarnings("rawtypes") public Class personClass = null; /** * 反射Person类 * @throws Exception */ @Before public void init() throws Exception &#123; className = "cn.java.reflect.Person"; personClass = Class.forName(className); &#125; /** *获取某个class文件对象 */ @Test public void getClassName() throws Exception &#123; System.out.println(personClass); &#125; /** *获取某个class文件对象的另一种方式 */ @Test public void getClassName2() throws Exception &#123; System.out.println(Person.class); &#125; /** *创建一个class文件表示的真实对象，底层会调用空参数的构造方法 */ @Test public void getNewInstance() throws Exception &#123; System.out.println(personClass.newInstance()); &#125; /** *获取非私有的构造函数 */ @SuppressWarnings(&#123; "rawtypes", "unchecked" &#125;) @Test public void getPublicConstructor() throws Exception &#123; Constructor constructor = personClass.getConstructor(Long.class,String.class); Person person = (Person)constructor.newInstance(100L,"zhangsan"); System.out.println(person.getId()); System.out.println(person.getName()); &#125; /** *获得私有的构造函数 */ @SuppressWarnings(&#123; "rawtypes", "unchecked" &#125;) @Test public void getPrivateConstructor() throws Exception &#123; Constructor con = personClass.getDeclaredConstructor(String.class); con.setAccessible(true);//强制取消Java的权限检测 Person person2 = (Person)con.newInstance("zhangsan"); System.out.println(person2.getName()); &#125; /** *获取非私有的成员变量 */ @SuppressWarnings(&#123; "rawtypes", "unchecked" &#125;) @Test public void getNotPrivateField() throws Exception &#123; Constructor constructor = personClass.getConstructor(Long.class,String.class); Object obj = constructor.newInstance(100L,"zhangsan"); Field field = personClass.getField("name"); field.set(obj, "lisi"); System.out.println(field.get(obj)); &#125; /** *获取私有的成员变量 */ @SuppressWarnings(&#123; "rawtypes", "unchecked" &#125;) @Test public void getPrivateField() throws Exception &#123; Constructor constructor = personClass.getConstructor(Long.class); Object obj = constructor.newInstance(100L); Field field2 = personClass.getDeclaredField("id"); field2.setAccessible(true);//强制取消Java的权限检测 field2.set(obj,10000L); System.out.println(field2.get(obj)); &#125; /** *获取非私有的成员函数 */ @SuppressWarnings(&#123; "unchecked" &#125;) @Test public void getNotPrivateMethod() throws Exception &#123; System.out.println(personClass.getMethod("toString")); Object obj = personClass.newInstance();//获取空参的构造函数 Object object = personClass.getMethod("toString").invoke(obj); System.out.println(object); &#125; /** *获取私有的成员函数 */ @SuppressWarnings("unchecked") @Test public void getPrivateMethod() throws Exception &#123; Object obj = personClass.newInstance();//获取空参的构造函数 Method method = personClass.getDeclaredMethod("getSomeThing"); method.setAccessible(true); Object value = method.invoke(obj); System.out.println(value); &#125; /** * */ @Test public void otherMethod() throws Exception &#123; //当前加载这个class文件的那个类加载器对象 System.out.println(personClass.getClassLoader()); //获取某个类实现的所有接口 Class[] interfaces = personClass.getInterfaces(); for (Class class1 : interfaces) &#123; System.out.println(class1); &#125; //反射当前这个类的直接父类 System.out.println(personClass.getGenericSuperclass()); /** * getResourceAsStream这个方法可以获取到一个输入流，这个输入流会关联到name所表示的那个文件上。 */ //path 不以’/'开头时默认是从此类所在的包下取资源，以’/'开头则是从ClassPath根下获取。其只是通过path构造一个绝对路径，最终还是由ClassLoader获取资源。 System.out.println(personClass.getResourceAsStream("/log4j.properties")); //默认则是从ClassPath根下获取，path不能以’/'开头，最终是由ClassLoader获取资源。 System.out.println(personClass.getResourceAsStream("/log4j.properties")); //判断当前的Class对象表示是否是数组 System.out.println(personClass.isArray()); System.out.println(new String[3].getClass().isArray()); //判断当前的Class对象表示是否是枚举类 System.out.println(personClass.isEnum()); System.out.println(Class.forName("cn.java.reflect.City").isEnum()); //判断当前的Class对象表示是否是接口 System.out.println(personClass.isInterface()); System.out.println(Class.forName("cn.java.reflect.TestInterface").isInterface()); &#125;&#125;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2Fjava%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%2F</url>
    <content type="text"><![CDATA[java多线程-线程安全标签（空格分隔）： 并发编程学习 ##什么是线程安全和线程不安全 线程安全就是多线程访问时，采用了加锁机制，当一个线程访问该类的某个数据时，进行保护，其他线程不能进行访问直到该线程读取完，其他线程才可使用。不会出现数据不一致或者数据污染。 线程不安全就是不提供数据访问保护，有可能出现多个线程先后更改数据造成所得到的数据是脏数据。 ##java内置的线程锁机制 ###关键字synchronized synchronized是java中的一个关键字，也就是说是Java语言内置的特性。 实现方式 加同步格式： synchronized( 需要一个任意的对象（锁） ){ 代码块中放操作共享数据的代码。 } synchronized的缺陷 如果一个代码块被synchronized修饰了，当一个线程获取了对应的锁，并执行该代码块时，其他线程便只能一直等待，等待获取锁的线程释放锁，而这里获取锁的线程释放锁只会有两种情况： 获取锁的线程执行完了该代码块，然后线程释放对锁的占有； 线程执行发生异常，此时JVM会让线程自动释放锁。 例子1： 如果这个获取锁的线程由于要等待IO或者其他原因（比如调用sleep方法）被阻塞了，但是又没有释放锁，其他线程便只能干巴巴地等待，试想一下，这多么影响程序执行效率。 因此就需要有一种机制可以不让等待的线程一直无期限地等待下去（比如只等待一定的时间或者能够响应中断），通过Lock就可以办到。 例子2： 当有多个线程读写文件时，读操作和写操作会发生冲突现象，写操作和写操作会发生冲突现象，但是读操作和读操作不会发生冲突现象。 但是采用synchronized关键字来实现同步的话，就会导致一个问题：如果多个线程都只是进行读操作，当一个线程在进行读操作时，其他线程只能等待无法进行读操作。 因此就需要一种机制来使得多个线程都只是进行读操作时，线程之间不会发生冲突，通过Lock就可以办到。 另外，通过Lock可以知道线程有没有成功获取到锁。这个是synchronized无法办到的。 总的来说，也就是说Lock提供了比synchronized更多的功能。 ###Lock 首先要说明的就是Lock，通过查看Lock的源码可知，Lock是一个接口 1234567public interface Lock &#123; void lock(); void lockInterruptibly() throws InterruptedException; boolean tryLock(); boolean tryLock(long time, TimeUnit unit) throws InterruptedException; void unlock(); &#125; Lock接口中每个方法的使用: lock()、tryLock()、tryLock(long time, TimeUnit unit)、lockInterruptibly()是用来获取锁的。 unLock()方法是用来释放锁的。 四个获取锁方法的区别： lock()方法 是平常使用得最多的一个方法，就是用来获取锁。如果锁已被其他线程获取，则进行等待。 由于在前面讲到如果采用Lock，必须主动去释放锁，并且在发生异常时，不会自动释放锁。因此一般来说，使用Lock必须在try{}catch{}块中进行，并且将释放锁的操作放在finally块中进行，以保证锁一定被被释放，防止死锁的发生。 tryLock()方法 是有返回值的，它表示用来尝试获取锁，如果获取成功，则返回true，如果获取失败（即锁已被其他线程获取），则返回false，也就说这个方法无论如何都会立即返回。在拿不到锁时不会一直在那等待。 tryLock(long time, TimeUnit unit)方法 和tryLock()方法是类似的，只不过区别在于这个方法在拿不到锁时会等待一定的时间，在时间期限之内如果还拿不到锁，就返回false。如果如果一开始拿到锁或者在等待期间内拿到了锁，则返回true。 lockInterruptibly()方法 比较特殊，当通过这个方法去获取锁时，如果线程正在等待获取锁，则这个线程能够响应中断，即中断线程的等待状态。也就使说，当两个线程同时通过lock.lockInterruptibly()想获取某个锁时，假若此时线程A获取到了锁，而线程B只有等待，那么对线程B调用threadB.interrupt()方法能够中断线程B的等待过程。 注意，当一个线程获取了锁之后，是不会被interrupt()方法中断的。 因此当通过lockInterruptibly()方法获取某个锁时，如果不能获取到，只有进行等待的情况下，是可以响应中断的。 而用synchronized修饰的话，当一个线程处于等待某个锁的状态，是无法被中断的，只有一直等待下去。 Lock和synchronized的区别 Lock不是Java语言内置的，synchronized是Java语言的关键字，因此是内置特性。Lock是一个类，通过这个类可以实现同步访问； Lock和synchronized有一点非常大的不同，采用synchronized不需要用户去手动释放锁，当synchronized方法或者synchronized代码块执行完之后，系统会自动让线程释放对锁的占用；而Lock则必须要用户去手动释放锁，如果没有主动释放锁，就有可能导致出现死锁现象。 ####ReentrantLock 直接使用lock接口的话，我们需要实现很多方法，不太方便，ReentrantLock是唯一实现了Lock接口的类，并且ReentrantLock提供了更多的方法，ReentrantLock，意思是“可重入锁”。 #####lock()的使用方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.ArrayList;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class MyLockTest &#123; private static ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); static Lock lock = new ReentrantLock(); // 注意这个地方 public static &lt;E&gt; void main(String[] args) &#123; new Thread() &#123; public void run() &#123; Thread thread = Thread.currentThread(); lock.lock(); try &#123; System.out.println(thread.getName() + &quot;得到了锁&quot;); for (int i = 0; i &lt; 5; i++) &#123; arrayList.add(i); &#125; &#125; catch (Exception e) &#123; // TODO: handle exception &#125; finally &#123; System.out.println(thread.getName() + &quot;释放了锁&quot;); lock.unlock(); &#125; &#125;; &#125;.start(); new Thread() &#123; public void run() &#123; Thread thread = Thread.currentThread(); lock.lock(); try &#123; System.out.println(thread.getName() + &quot;得到了锁&quot;); for (int i = 0; i &lt; 5; i++) &#123; arrayList.add(i); &#125; &#125; catch (Exception e) &#123; // TODO: handle exception &#125; finally &#123; System.out.println(thread.getName() + &quot;释放了锁&quot;); lock.unlock(); &#125; &#125;; &#125;.start(); &#125;&#125; #####tryLock()的使用方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.util.ArrayList;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * 观察现象：一个线程获得锁后，另一个线程取不到锁，不会一直等待 * @author * */public class MyTryLock &#123; private static ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); static Lock lock = new ReentrantLock(); // 注意这个地方 public static void main(String[] args) &#123; new Thread() &#123; public void run() &#123; Thread thread = Thread.currentThread(); boolean tryLock = lock.tryLock(); System.out.println(thread.getName()+&quot; &quot;+tryLock); if (tryLock) &#123; try &#123; System.out.println(thread.getName() + &quot;得到了锁&quot;); for (int i = 0; i &lt; 5; i++) &#123; arrayList.add(i); &#125; &#125; catch (Exception e) &#123; // TODO: handle exception &#125; finally &#123; System.out.println(thread.getName() + &quot;释放了锁&quot;); lock.unlock(); &#125; &#125; &#125;; &#125;.start(); new Thread() &#123; public void run() &#123; Thread thread = Thread.currentThread(); boolean tryLock = lock.tryLock(); System.out.println(thread.getName()+&quot; &quot;+tryLock); if (tryLock) &#123; try &#123; System.out.println(thread.getName() + &quot;得到了锁&quot;); for (int i = 0; i &lt; 5; i++) &#123; arrayList.add(i); &#125; &#125; catch (Exception e) &#123; // TODO: handle exception &#125; finally &#123; System.out.println(thread.getName() + &quot;释放了锁&quot;); lock.unlock(); &#125; &#125; &#125;; &#125;.start(); &#125;&#125; #####lockInterruptibly()响应中断的使用方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * 观察现象：如果thread-0得到了锁，阻塞。。。thread-1尝试获取锁，如果拿不到，则可以被中断等待 * @author * */public class MyInterruptibly &#123; private Lock lock = new ReentrantLock(); public static void main(String[] args) &#123; MyInterruptibly test = new MyInterruptibly(); MyThread thread0 = new MyThread(test); MyThread thread1 = new MyThread(test); thread0.start(); thread1.start(); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; thread1.interrupt(); System.out.println(&quot;=====================&quot;); &#125; public void insert(Thread thread) throws InterruptedException&#123; lock.lockInterruptibly(); //注意，如果需要正确中断等待锁的线程，必须将获取锁放在外面，然后将InterruptedException抛出 try &#123; System.out.println(thread.getName()+&quot;得到了锁&quot;); long startTime = System.currentTimeMillis(); for( ; ;) &#123; if(System.currentTimeMillis() - startTime &gt;= Integer.MAX_VALUE) break; //插入数据 &#125; &#125; finally &#123; System.out.println(Thread.currentThread().getName()+&quot;执行finally&quot;); lock.unlock(); System.out.println(thread.getName()+&quot;释放了锁&quot;); &#125; &#125; &#125; class MyThread extends Thread &#123; private MyInterruptibly test = null; public MyThread(MyInterruptibly test) &#123; this.test = test; &#125; @Override public void run() &#123; try &#123; test.insert(Thread.currentThread()); &#125; catch (Exception e) &#123; System.out.println(Thread.currentThread().getName()+&quot;被中断&quot;); &#125; &#125;&#125; ###ReadWriteLock读写锁 ReadWriteLock也是一个接口，在它里面只定义了两个方法. public interface ReadWriteLock { /** Returns the lock used for reading.* @return the lock used for reading.*/Lock readLock(); /** Returns the lock used for writing.* @return the lock used for writing.*/Lock writeLock();} 一个用来获取读锁，一个用来获取写锁。也就是说将文件的读写操作分开，分成2个锁来分配给线程，从而使得多个线程可以同时进行读操作。 ####ReentrantReadWriteLock ReentrantReadWriteLock里面提供了很多丰富的方法，不过最主要的有两个方法：readLock()和writeLock()用来获取读锁和写锁。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import java.util.concurrent.locks.ReentrantReadWriteLock;/** * 使用读写锁，可以实现读写分离锁定，读操作并发进行，写操作锁定单个线程 * * 如果有一个线程已经占用了读锁，则此时其他线程如果要申请写锁，则申请写锁的线程会一直等待释放读锁。 * 如果有一个线程已经占用了写锁，则此时其他线程如果申请写锁或者读锁，则申请的线程会一直等待释放写锁。 * @author * */public class MyReentrantReadWriteLock &#123; private ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); public static void main(String[] args) &#123; final MyReentrantReadWriteLock test = new MyReentrantReadWriteLock(); new Thread()&#123; public void run() &#123; test.get(Thread.currentThread()); test.write(Thread.currentThread()); &#125;; &#125;.start(); new Thread()&#123; public void run() &#123; test.get(Thread.currentThread()); test.write(Thread.currentThread()); &#125;; &#125;.start(); &#125; /** * 读操作,用读锁来锁定 * @param thread */ public void get(Thread thread) &#123; rwl.readLock().lock(); try &#123; long start = System.currentTimeMillis(); while(System.currentTimeMillis() - start &lt;= 1) &#123; System.out.println(thread.getName()+&quot;正在进行读操作&quot;); &#125; System.out.println(thread.getName()+&quot;读操作完毕&quot;); &#125; finally &#123; rwl.readLock().unlock(); &#125; &#125; /** * 写操作，用写锁来锁定 * @param thread */ public void write(Thread thread) &#123; rwl.writeLock().lock();; try &#123; long start = System.currentTimeMillis(); while(System.currentTimeMillis() - start &lt;= 1) &#123; System.out.println(thread.getName()+&quot;正在进行写操作&quot;); &#125; System.out.println(thread.getName()+&quot;写操作完毕&quot;); &#125; finally &#123; rwl.writeLock().unlock(); &#125; &#125;&#125; ####注意事项 如果有一个线程已经占用了读锁，则此时其他线程如果要申请写锁，则申请写锁的线程会一直等待释放读锁。 如果有一个线程已经占用了写锁，则此时其他线程如果申请写锁或者读锁，则申请的线程会一直等待释放写锁。 ###Lock和synchronized的选择 Lock是一个接口，而synchronized是Java中的关键字，synchronized是内置的语言实现； synchronized在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而Lock在发生异常时，如果没有主动通过unLock()去释放锁，则很可能造成死锁现象，因此使用Lock时需要在finally块中释放锁； Lock可以让等待锁的线程响应中断，而synchronized却不行，使用synchronized时，等待的线程会一直等待下去，不能够响应中断； 通过Lock可以知道有没有成功获取锁，而synchronized却无法办到. Lock可以提高多个线程进行读操作的效率。 在性能上来说，如果竞争资源不激烈，两者的性能是差不多的，而当竞争资源非常激烈时（即有大量线程同时竞争），此时Lock的性能要远远优于synchronized。所以说，在具体使用时要根据适当情况选择。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2Fjava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[java多线程基础标签（空格分隔）： 并发编程学习 ##同步和异步的区别 例如一个请求访问 同步：一个请求一定要等待返回之后再进行下一个操作 异步：一个请求不用等待返回之后就可以进行下一个操作##进程介绍 不管是我们开发的应用程序，还是我们运行的其他的应用程序，都需要先把程序安装在本地的硬盘上。然后找到这个程序的启动文件，启动程序的时候，其实是电脑把当前的这个程序加载到内存中，在内存中需要给当前的程序分配一段独立的运行空间。这片空间就专门负责当前这个程序的运行。不同的应用程序运行的过程中都需要在内存中分配自己独立的运行空间，彼此之间不会相互的影响。我们把每个独立应用程序在内存的独立空间称为当前应用程序运行的一个进程。进程：它是内存中的一段独立的空间，可以负责当前应用程序的运行。当前这个进程负责调度当前程序中的所有运行细节。 ##线程介绍 启动的QQ聊天软件，需要和多个人进行聊天。这时多个人之间是不能相互影响，但是它们都位于当前QQ这个软件运行时所分配的内存的独立空间中。 在一个进程中，每个独立的功能都需要独立的去运行，这时又需要把当前这个进程划分成多个运行区域，每个独立的小区域（小单元）称为一个线程。线程：它是位于进程中，负责当前进程中的某个具备独立运行资格的空间。进程是负责整个程序的运行，而线程是程序中具体的某个独立功能的运行。一个进程中至少应该有一个线程。 ##多线程介绍 现在的操作系统基本都是多用户，多任务的操作系统。每个任务就是一个进程。而在这个进程中就会有线程。 真正可以完成程序运行和功能的实现靠的是进程中的线程。多线程：在一个进程中，我们同时开启多个线程，让多个线程同时去完成某些任务（功能）。(比如后台服务系统，就可以用多个线程同时响应多个客户的请求)多线程的目的：提高程序的运行效率。 ##多线程运行的原理 cpu在线程中做时间片的切换。 其实真正电脑中的程序的运行不是同时在运行的。CPU负责程序的运行，而CPU在运行程序的过程中某个时刻点上，它其实只能运行一个程序。而不是多个程序。而CPU它可以在多个程序之间进行高速的切换。而切换频率和速度太快，导致人的肉眼看不到。每个程序就是进程， 而每个进程中会有多个线程，而CPU是在这些线程之间进行切换。了解了CPU对一个任务的执行过程，我们就必须知道，多线程可以提高程序的运行效率，但不能无限制的开线程。 ##实现多线程的三种方式 ###第一种：继承Thread类 123456789101112131415161718192021222324252627282930313233343536public class MyThreadWithExtends extends Thread &#123; String flag; public MyThreadWithExtends(String flag)&#123; this.flag = flag; &#125; @Override public void run() &#123; String tname = Thread.currentThread().getName(); System.out.println(tname+"线程的run方法被调用……"); Random random = new Random(); for(int i=0;i&lt;20;i++)&#123; try &#123; Thread.sleep(random.nextInt(10)*100); System.out.println(tname+ "...."+ flag); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; Thread thread1 = new MyThreadWithExtends("a"); Thread thread2 = new MyThreadWithExtends("b"); thread1.start(); thread2.start(); /** * 如果是调用thread的run方法，则只是一个普通的方法调用，不会开启新的线程 */// thread1.run();// thread2.run(); &#125;&#125; ###第二种：实现Runnable接口 1234567891011121314151617181920212223242526272829303132public class MyThreadWithImpliment implements Runnable &#123; int x; public MyThreadWithImpliment(int x) &#123; this.x = x; &#125; @Override public void run() &#123; String name = Thread.currentThread().getName(); System.out.println("线程" + name + "的run方法被调用……"); for (int i = 0; i &lt; 10; i++) &#123; System.out.println(x); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; Thread thread1 = new Thread(new MyThreadWithImpliment(1), "thread-1"); Thread thread2 = new Thread(new MyThreadWithImpliment(2), "thread-2"); thread1.start(); thread2.start(); // 注意调用run和调用start的区别,直接调用run，则都运行在main线程中// thread1.run();// thread2.run(); &#125;&#125; ###第三种：线程池 ####线程池的5种创建方式 Single Thread Executor : 只有一个线程的线程池，因此所有提交的任务是顺序执行。 代码：Executors.newSingleThreadExecutor() Cached Thread Pool : 线程池里有很多线程需要同时执行，老的可用线程将被新的任务触发重新执行，如果线程超过60秒内没执行，那么将被终止并从池中删除。 代码：Executors.newCachedThreadPool() Fixed Thread Pool : 拥有固定线程数的线程池，如果没有任务执行，那么线程会一直等待。 代码：Executors.newFixedThreadPool(4) 在构造函数中的参数4是线程池的大小，你可以随意设置，也可以和cpu的核数量保持一致，获取cpu的核数量int cpuNums = Runtime.getRuntime().availableProcessors(); Scheduled Thread Pool : 用来调度即将执行的任务的线程池，可能是不是直接执行, 每隔多久执行一次… 策略型的。 代码：Executors.newScheduledThreadPool() Single Thread Scheduled Pool : 只有一个线程，用来调度任务在指定时间执行。 代码：Executors.newSingleThreadScheduledExecutor() ####线程池的使用 传入 Runnable ，任务完成后 Future 对象返回 null 调用excute提交任务，匿名Runable重写run方法, run方法里是业务逻辑 调用shutdown，关闭线程池。123456789101112131415161718192021222324252627public class ThreadPoolWithRunable &#123; /** * 通过线程池执行线程 * @param args */ public static void main(String[] args) &#123; //创建一个线程池 ExecutorService pool = Executors.newCachedThreadPool(); for(int i = 1; i &lt; 5; i++)&#123; pool.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println("thread name: " + Thread.currentThread().getName()); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; pool.shutdown(); &#125;&#125; 传入 Callable，该方法返回一个 Future 实例表示任务的状态。 future.isDone() 方法返回布尔值。表示任务完成状态。true表示完成。false表示未完成 调用submit提交任务, 匿名Callable,重写call方法, 有返回值, 获取返回值会阻塞,一直要等到线程任务返回结果。 调用shutdown，关闭线程池。 12345678910111213141516171819202122public class ThreadPoolWithcallable &#123; public static void main(String[] args) throws InterruptedException, ExecutionException &#123; ExecutorService pool = Executors.newFixedThreadPool(4); for(int i = 0; i &lt; 10; i++)&#123; Future&lt;String&gt; submit = pool.submit(new Callable&lt;String&gt;()&#123; @Override public String call() throws Exception &#123; //System.out.println(&quot;a&quot;); Thread.sleep(5000); return &quot;b--&quot;+Thread.currentThread().getName(); &#125; &#125;); //从Future中get结果，这个方法是会被阻塞的，一直要等到线程任务返回结果 System.out.println(submit.get()); &#125; pool.shutdown(); &#125;&#125; callable 跟runnable的区别： runnable的run方法不会有任何返回结果，所以主线程无法获得任务线程的返回值 callable的call方法可以返回结果，但是主线程在获取时是被阻塞，需要等待任务线程返回才能拿到结果]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2FJava%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[Java动态代理业务场景 旧业务 买家调用action，购买衣服，衣服在数据库的标价为50元，购买流程就是简单的调用。 新业务 在原先的价格上可以使用优惠券，但是这个功能在以前没有实现过，我们通过代理类，代理了原先的接口方法，在这个方法的基础上，修改了返回值。 代理实现流程 书写代理类和代理方法，在代理方法中实现代理Proxy.newProxyInstance 代理中需要的参数分别为：被代理的类的类加载器soneObjectclass.getClassLoader()，被代理类的所有实现接口new Class[] { Interface.class }，句柄方法new InvocationHandler() 在句柄方法中复写invoke方法，invoke方法的输入有3个参数Object proxy（代理类对象）, Method method（被代理类的方法）,Object[] args（被代理类方法的传入参数），在这个方法中，我们可以定制化的开发新的业务。 获取代理类，强转成被代理的接口 最后，我们可以像没被代理一样，调用接口的认可方法，方法被调用后，方法名和参数列表将被传入代理类的invoke方法中，进行新业务的逻辑流程。 原业务接口IBoss123public interface IBoss &#123;//接口 int yifu(String size);&#125; 原业务实现类12345678910public class Boss implements IBoss&#123; public int yifu(String size)&#123; System.err.println("天猫小强旗舰店，老板给客户发快递----衣服型号："+size); //这件衣服的价钱，从数据库读取 return 50; &#125; public void kuzi()&#123; System.err.println("天猫小强旗舰店，老板给客户发快递----裤子"); &#125;&#125; 原业务调用123456789public class SaleAction &#123; @Test public void saleByBossSelf() throws Exception &#123; IBoss boss = new Boss(); System.out.println("老板自营！"); int money = boss.yifu("xxl"); System.out.println("衣服成交价：" + money); &#125;&#125; 代理类12345678910111213public static IBoss getProxyBoss(final int discountCoupon) throws Exception &#123; Object proxedObj = Proxy.newProxyInstance(Boss.class.getClassLoader(), new Class[] &#123; IBoss.class &#125;, new InvocationHandler() &#123; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Integer returnValue = (Integer) method.invoke(new Boss(), args);// 调用原始对象以后返回的值 return returnValue - discountCoupon; &#125; &#125;); return (IBoss)proxedObj;&#125;&#125; 新业务调用123456789public class ProxySaleAction &#123; @Test public void saleByProxy() throws Exception &#123; IBoss boss = ProxyBoss.getProxyBoss(20);// 将代理的方法实例化成接口 System.out.println("代理经营！"); int money = boss.yifu("xxl");// 调用接口的方法，实际上调用方式没有变 System.out.println("衣服成交价：" + money); &#125;&#125;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2Fjava%E7%9A%84JMS%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[java的JMS技术标签（空格分隔）： 大数据学习 ##什么是JMS JMS即Java消息服务（Java Message Service）应用程序接口是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。Java消息服务是一个与具体平台无关的API，绝大多数MOM提供商都对JMS提供支持。 JMS是一种与厂商无关的 API，用来访问消息收发系统消息。它类似于JDBC(Java Database Connectivity)：这里，JDBC 是可以用来访问许多不同关系数据库的 API，而 JMS 则提供同样与厂商无关的访问方法，以访问消息收发服务。许多厂商都支持 JMS，包括 IBM 的 MQSeries、BEA的 Weblogic JMS service和 Progress 的 SonicMQ，这只是几个例子。 JMS 使您能够通过消息收发服务（有时称为消息中介程序或路由器）从一个 JMS 客户机向另一个 JMS客户机发送消息。消息是 JMS 中的一种类型对象，由两部分组成：报头和消息主体。报头由路由信息以及有关该消息的元数据组成。消息主体则携带着应用程序的数据或有效负载。根据有效负载的类型来划分，可以将消息分为几种类型，它们分别携带：简单文本(TextMessage)、可序列化的对象 (ObjectMessage)、属性集合 (MapMessage)、字节流 (BytesMessage)、原始值流 (StreamMessage)，还有无有效负载的消息 (Message)。 ##JMS规范 ###专业技术规范 JMS（Java Messaging Service）是Java平台上有关面向消息中间件(MOM)的技术规范，它便于消息系统中的Java应用程序进行消息交换,并且通过提供标准的产生、发送、接收消息的接口简化企业应用的开发，翻译为Java消息服务。 ###体系架构 JMS由以下元素组成。 JMS提供者provider：连接面向消息中间件的，JMS接口的一个实现。提供者可以是Java平台的JMS实现，也可以是非Java平台的面向消息中间件的适配器。 JMS客户：生产或消费基于消息的Java的应用程序或对象。 JMS生产者：创建并发送消息的JMS客户。 JMS消费者：接收消息的JMS客户。 JMS消息：包括可以在JMS客户之间传递的数据的对象 JMS队列：一个容纳那些被发送的等待阅读的消息的区域。与队列名字所暗示的意思不同，消息的接受顺序并不一定要与消息的发送顺序相同。一旦一个消息被阅读，该消息将被从队列中移走。 JMS主题：一种支持发送消息给多个订阅者的机制。 ###Java消息服务应用程序结构支持两种模型 ####点对点或队列模型 在点对点或队列模型下，一个生产者向一个特定的队列发布消息，一个消费者从该队列中读取消息。这里，生产者知道消费者的队列，并直接将消息发送到消费者的队列。 这种模式被概括为：只有一个消费者将获得消息生产者不需要在接收者消费该消息期间处于运行状态，接收者也同样不需要在消息发送时处于运行状态。每一个成功处理的消息都由接收者签收 ####发布者/订阅者模型 发布者/订阅者模型支持向一个特定的消息主题发布消息。0或多个订阅者可能对接收来自特定消息主题的消息感兴趣。在这种模型下，发布者和订阅者彼此不知道对方。这种模式好比是匿名公告板。 这种模式被概括为： 多个消费者可以获得消息 在发布者和订阅者之间存在时间依赖性。发布者需要建立一个订阅（subscription），以便客户能够订阅。订阅者必须保持持续的活动状态以接收消息，除非订阅者建立了持久的订阅。在那种情况下，在订阅者未连接时发布的消息将在订阅者重新连接时重新发布。 ##常用的JMS实现 要使用Java消息服务，你必须要有一个JMS提供者，管理会话和队列。既有开源的提供者也有专有的提供者。 开源的提供者包括： Apache ActiveMQJBoss 社区所研发的 HornetQJoramCoridan的MantaRayThe OpenJMS Group的OpenJMS 专有的提供者包括： BEA的BEA WebLogic Server JMSTIBCO Software的EMSGigaSpaces Technologies的GigaSpacesSoftwired 2006的iBusIONA Technologies的IONA JMSSeeBeyond的IQManager（2005年8月被Sun Microsystems并购）webMethods的JMS+ -my-channels的NirvanaSonic Software的SonicMQSwiftMQ的SwiftMQIBM的WebSphere MQ]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2Fjava%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[java并发编程学习的一些总结标签（空格分隔）： 并发编程学习 ##1.不应用线程池的缺点 有些开发者图省事，遇到需要多线程处理的地方，直接new Thread(…).start()，对于一般场景是没问题的，但如果是在并发请求很高的情况下，就会有些隐患： 新建线程的开销。线程虽然比进程要轻量许多，但对于JVM来说，新建一个线程的代价还是挺大的，决不同于新建一个对象 资源消耗量。没有一个池来限制线程的数量，会导致线程的数量直接取决于应用的并发量，这样有潜在的线程数据巨大的可能，那么资源消耗量将是巨大的 稳定性。当线程数量超过系统资源所能承受的程度，稳定性就会成问题##2.制定执行策略 在每个需要多线程处理的地方，不管并发量有多大，需要考虑线程的执行策略 任务以什么顺序执行 可以有多少个任务并发执行 可以有多少个任务进入等待执行队列 系统过载的时候，应该放弃哪些任务？如何通知到应用程序？ 一个任务的执行前后应该做什么处理##3.线程池的类型 不管是通过Executors创建线程池，还是通过Spring来管理，都得清楚知道有哪几种线程池： FixedThreadPool：定长线程池，提交任务时创建线程，直到池的最大容量，如果有线程非预期结束，会补充新线程 CachedThreadPool：可变线程池，它犹如一个弹簧，如果没有任务需求时，它回收空闲线程，如果需求增加，则按需增加线程，不对池的大小做限制 SingleThreadExecutor：单线程。处理不过来的任务会进入FIFO队列等待执行 SecheduledThreadPool：周期性线程池。支持执行周期性线程任务 其实，这些不同类型的线程池都是通过构建一个ThreadPoolExecutor来完成的，所不同的是corePoolSize,maximumPoolSize,keepAliveTime,unit,workQueue,threadFactory这么几个参数。具体可以参见JDK DOC。##4.线程池饱和策略 由以上线程池类型可知，除了CachedThreadPool其他线程池都有饱和的可能，当饱和以后就需要相应的策略处理请求线程的任务，比如，达到上限时通过ThreadPoolExecutor.setRejectedExecutionHandler方法设置一个拒绝任务的策略，JDK提供了AbortPolicy、CallerRunsPolicy、DiscardPolicy、DiscardOldestPolicy几种策略，具体差异可见JDK DOC##5.线程无依赖性 多线程任务设计上尽量使得各任务是独立无依赖的，所谓依赖性可两个方面： 线程之间的依赖性。如果线程有依赖可能会造成死锁或饥饿 调用者与线程的依赖性。调用者得监视线程的完成情况，影响可并发量当然，在有些业务里确实需要一定的依赖性，比如调用者需要得到线程完成后结果，传统的Thread是不便完成的，因为run方法无返回值，只能通过一些共享的变量来传递结果，但在Executor框架里可以通过Future和Callable实现需要有返回值的任务，当然线程的异步性导致需要有相应机制来保证调用者能等待任务完成。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2Fjava%E5%B9%B6%E5%8F%91%E5%8C%85%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[java并发包消息队列标签（空格分隔）： 并发编程学习 ##BlockingQueue BlockingQueue也是java.util.concurrent下的主要用来控制线程同步的工具。 主要的方法是：put、take一对阻塞存取；add、poll一对非阻塞存取。 插入: add(anObject):把anObject加到BlockingQueue里,即如果BlockingQueue可以容纳,则返回true,否则抛出异常,不好 offer(anObject):表示如果可能的话,将anObject加到BlockingQueue里,即如果BlockingQueue可以容纳,则返回true,否则返回false. put(anObject):把anObject加到BlockingQueue里,如果BlockQueue没有空间,则调用此方法的线程被阻断直到BlockingQueue里面有空间再继续, 有阻塞, 放不进去就等待 读取： poll(time):取走BlockingQueue里排在首位的对象,若不能立即取出,则可以等time参数规定的时间,取不到时返回null; 取不到返回null take():取走BlockingQueue里排在首位的对象,若BlockingQueue为空,阻断进入等待状态直到Blocking有新的对象被加入为止; 阻塞, 取不到就一直等 其他 int remainingCapacity();返回队列剩余的容量，在队列插入和获取的时候，数据可能不准, 不能保证数据的准确性 boolean remove(Object o); 从队列移除元素，如果存在，即移除一个或者更多，队列改 变了返回true public boolean contains(Object o); 查看队列是否存在这个元素，存在返回true int drainTo(Collection&lt;? super E&gt; c); //移除此队列中所有可用的元素,并将它们添加到给定 collection 中。取出放到集合中 int drainTo(Collection&lt;? super E&gt; c, int maxElements); 和上面方法的区别在于，指定了移 动的数量; 取出指定个数放到集合 BlockingQueue有四个具体的实现类,常用的两种实现类为： ArrayBlockingQueue： 一个由数组支持的有界阻塞队列，规定大小的BlockingQueue,其构造函数必须带一个int参数来指明其大小.其所含的对象是以FIFO(先入先出)顺序排序的。 LinkedBlockingQueue： 大小不定的BlockingQueue,若其构造函数带一个规定大小的参数,生成的BlockingQueue有大小限制,若不带大小参数,所生成的BlockingQueue的大小由Integer.MAX_VALUE来决定.其所含的对象是以FIFO(先入先出)顺序排序的。 LinkedBlockingQueue 可以指定容量，也可以不指定，不指定的话，默认最大是Integer.MAX_VALUE,其中主要用到put和take方法，put方法在队列满的时候会阻塞直到有队列成员被消费，take方法在队列空的时候会阻塞，直到有队列成员被放进来。 LinkedBlockingQueue和ArrayBlockingQueue区别： LinkedBlockingQueue和ArrayBlockingQueue比较起来,它们背后所用的数据结构不一样,导致LinkedBlockingQueue的数据吞吐量要大于ArrayBlockingQueue,但在线程数量很大时其性能的可预见性低于ArrayBlockingQueue. ###生产者代码 12345678910111213141516171819202122232425import java.util.Random;import java.util.concurrent.BlockingQueue;public class TestBlockingQueueProducer implements Runnable &#123; BlockingQueue&lt;String&gt; queue; Random random = new Random(); public TestBlockingQueueProducer(BlockingQueue&lt;String&gt; queue) &#123; this.queue = queue; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Thread.sleep(random.nextInt(10)); String task = Thread.currentThread().getName() + &quot; made a product &quot; + i; System.out.println(task); queue.put(task); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; ###消费者代码 12345678910111213141516171819202122mport java.util.Random;import java.util.concurrent.BlockingQueue;public class TestBlockingQueueConsumer implements Runnable&#123; BlockingQueue&lt;String&gt; queue; Random random = new Random(); public TestBlockingQueueConsumer(BlockingQueue&lt;String&gt; queue)&#123; this.queue = queue; &#125; @Override public void run() &#123; try &#123; Thread.sleep(random.nextInt(10)); System.out.println(Thread.currentThread().getName()+ &quot;trying...&quot;); String temp = queue.take();//如果队列为空，会阻塞当前线程 System.out.println(Thread.currentThread().getName() + &quot; get a job &quot; +temp); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; ###测试代码123456789101112131415161718192021222324import java.util.concurrent.BlockingQueue;import java.util.concurrent.LinkedBlockingQueue;public class TestBlockingQueue &#123; public static void main(String[] args) &#123; BlockingQueue&lt;String&gt; queue = new LinkedBlockingQueue&lt;String&gt;(2); // BlockingQueue&lt;String&gt; queue = new LinkedBlockingQueue&lt;String&gt;(); // 不设置的话，LinkedBlockingQueue默认大小为Integer.MAX_VALUE // BlockingQueue&lt;String&gt; queue = new ArrayBlockingQueue&lt;String&gt;(2); TestBlockingQueueConsumer consumer = new TestBlockingQueueConsumer(queue); TestBlockingQueueProducer producer = new TestBlockingQueueProducer(queue); for (int i = 0; i &lt; 3; i++) &#123; new Thread(producer, &quot;Producer&quot; + (i + 1)).start(); &#125; for (int i = 0; i &lt; 5; i++) &#123; new Thread(consumer, &quot;Consumer&quot; + (i + 1)).start(); &#125; new Thread(producer, &quot;Producer&quot; + (5)).start(); &#125;&#125;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2FJava-nio%2F</url>
    <content type="text"><![CDATA[Java-nionio原理学习nio简介 nio 是New IO 的简称，在jdk1.4 里提供的新api 。Sun 官方标榜的特性如下： 为所有的原始类型提供(Buffer)缓存支持。字符集编码解码解决方案。 Channel ：一个新的原始I/O 抽象。 支持锁和内存映射文件的文件访问接口。 提供多路(non-bloking) 非阻塞式的高伸缩性网络I/O 。 传统的I/O 使用传统的I/O程序读取文件内容, 并写入到另一个文件(或Socket), 如下程序: File.read(fileDesc, buf, len); Socket.send(socket, buf, len); 会有较大的性能开销, 主要表现在一下两方面: 上下文切换(context switch), 此处有4次用户态和内核态的切换 Buffer内存开销, 一个是应用程序buffer, 另一个是系统读取buffer以及socket buffer其运行示意图如下 ​ 先将文件内容从磁盘中拷贝到操作系统buffer 再从操作系统buffer拷贝到程序应用buffer 从程序buffer拷贝到socket buffer 从socket buffer拷贝到协议引擎. NIO NIO技术省去了将操作系统的read buffer拷贝到程序的buffer, 以及从程序buffer拷贝到socket buffer的步骤, 直接将 read buffer 拷贝到 socket buffer. java 的 FileChannel.transferTo() 方法就是这样的实现, 这个实现是依赖于操作系统底层的sendFile()实现的. publicvoid transferTo(long position, long count, WritableByteChannel target); 他的底层调用的是系统调用sendFile()方法 sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 如下图 传统socket和socket nio代码 传统socket server 12345678910111213141516171819202122232425262728public static void main(String args[]) throws Exception &#123; // 监听端口 ServerSocket server_socket = new ServerSocket(2000); System.out.println("等待，端口为：" + server_socket.getLocalPort()); while (true) &#123; // 阻塞接受消息 Socket socket = server_socket.accept(); // 打印链接信息 System.out.println("新连接： " + socket.getInetAddress() + ":" + socket.getPort()); // 从socket中获取流 DataInputStream input = new DataInputStream(socket.getInputStream()); // 接收数据 byte[] byteArray = new byte[4096]; while (true) &#123; int nread = input.read(byteArray, 0, 4096); System.out.println(new String(byteArray, "UTF-8")); if (-1 == nread) &#123; break; &#125; &#125; socket.close(); System.out.println("Connection closed by client"); &#125; &#125; client 123456789101112131415161718192021222324252627public static void main(String[] args) throws Exception &#123; long start = System.currentTimeMillis(); // 创建socket链接 Socket socket = new Socket("localhost", 2000); System.out.println("Connected with server " + socket.getInetAddress() + ":" + socket.getPort()); // 读取文件 FileInputStream inputStream = new FileInputStream("C:/sss.txt"); // 输出文件 DataOutputStream output = new DataOutputStream(socket.getOutputStream()); // 缓冲区4096K byte[] b = new byte[4096]; // 传输长度 long read = 0, total = 0; // 读取文件，写到socketio中 while ((read = inputStream.read(b)) &gt;= 0) &#123; total = total + read; output.write(b); &#125; // 关闭 output.close(); socket.close(); inputStream.close(); // 打印时间 System.out.println("bytes send--" + total + " and totaltime--" + (System.currentTimeMillis() - start)); &#125; socket nio 代码 server 1234567891011121314151617181920212223242526272829303132public static void main(String[] args) throws IOException &#123; // 创建socket channel ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); ServerSocket ss = serverSocketChannel.socket(); ss.setReuseAddress(true);// 地址重用 ss.bind(new InetSocketAddress("localhost", 9026));// 绑定地址 System.out.println("监听端口 : " + new InetSocketAddress("localhost", 9026).toString()); // 分配一个新的字节缓冲区 ByteBuffer dst = ByteBuffer.allocate(4096); // 读取数据 while (true) &#123; SocketChannel channle = serverSocketChannel.accept();// 接收数据 System.out.println("Accepted : " + channle); channle.configureBlocking(true);// 设置阻塞，接不到就停 int nread = 0; while (nread != -1) &#123; try &#123; nread = channle.read(dst);// 往缓冲区里读 byte[] array = dst.array();//将数据转换为array //打印 String string = new String(array, 0, dst.position()); System.out.print(string); dst.clear(); &#125; catch (IOException e) &#123; e.printStackTrace(); nread = -1; &#125; &#125; &#125; &#125; client 1234567891011121314151617181920212223242526272829public static void main(String[] args) throws IOException &#123; long start = System.currentTimeMillis(); // 打开socket的nio管道 SocketChannel sc = SocketChannel.open(); sc.connect(new InetSocketAddress("localhost", 9026));// 绑定相应的ip和端口 sc.configureBlocking(true);// 设置阻塞 // 将文件放到channel中 FileChannel fc = new FileInputStream("C:/sss.txt").getChannel();// 打开文件管道 //做好标记量 long size = fc.size(); int pos = 0; int offset = 4096; long curnset = 0; long counts = 0; //循环写 while (pos&lt;size) &#123; curnset = fc.transferTo(pos, 4096, sc);// 把文件直接读取到socket chanel中，返回文件大小 pos+=offset; counts+=curnset; &#125; //关闭 fc.close(); sc.close(); //打印传输字节数 System.out.println(counts); // 打印时间 System.out.println("bytes send--" + counts + " and totaltime--" + (System.currentTimeMillis() - start)); &#125; ​]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2Fjava-netty%2F</url>
    <content type="text"><![CDATA[java-nettynetty常用API学习netty简介 Netty是基于Java NIO的网络应用框架. Netty是一个NIO client-server(客户端服务器)框架，使用Netty可以快速开发网络应用，例如服务器和客户端协议。Netty提供了一种新的方式来使开发网络应用程序，这种新的方式使得它很容易使用和有很强的扩展性。Netty的内部实现时很复杂的，但是Netty提供了简单易用的api从网络处理代码中解耦业务逻辑。Netty是完全基于NIO实现的，所以整个Netty都是异步的。 网络应用程序通常需要有较高的可扩展性，无论是Netty还是其他的基于Java NIO的框架，都会提供可扩展性的解决方案。Netty中一个关键组成部分是它的异步特性. netty的helloworld下载netty包 下载netty包，下载地址http://netty.io/-N/ 服务端启动类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.Channel;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;/** * • 配置服务器功能，如线程、端口 • 实现服务器处理程序，它包含业务逻辑，决定当有一个请求连接或接收数据时该做什么 * */public class EchoServer &#123; private final int port; public EchoServer(int port) &#123; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup eventLoopGroup = null; try &#123; //创建ServerBootstrap实例来引导绑定和启动服务器 ServerBootstrap serverBootstrap = new ServerBootstrap(); //创建NioEventLoopGroup对象来处理事件，如接受新连接、接收数据、写数据等等 eventLoopGroup = new NioEventLoopGroup(); //指定通道类型为NioServerSocketChannel，设置InetSocketAddress让服务器监听某个端口已等待客户端连接。 serverBootstrap.group(eventLoopGroup).channel(NioServerSocketChannel.class).localAddress("localhost",port).childHandler(new ChannelInitializer&lt;Channel&gt;() &#123; //设置childHandler执行所有的连接请求 @Override protected void initChannel(Channel ch) throws Exception &#123; ch.pipeline().addLast(new EchoServerHandler()); &#125; &#125;); // 最后绑定服务器等待直到绑定完成，调用sync()方法会阻塞直到服务器完成绑定,然后服务器等待通道关闭，因为使用sync()，所以关闭操作也会被阻塞。 ChannelFuture channelFuture = serverBootstrap.bind().sync(); System.out.println("开始监听，端口为：" + channelFuture.channel().localAddress()); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; eventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; new EchoServer(20000).start(); &#125;&#125; 服务端回调方法1234567891011121314151617181920212223242526272829303132333435363738394041import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelFutureListener;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import java.util.Date;public class EchoServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println("server 读取数据……"); //读取数据 ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, "UTF-8"); System.out.println("接收客户端数据:" + body); //向客户端写数据 System.out.println("server向client发送数据"); String currentTime = new Date(System.currentTimeMillis()).toString(); ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes()); ctx.write(resp); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("server 读取数据完毕.."); ctx.flush();//刷新后才将数据发出到SocketChannel &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 客户端启动类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import io.netty.bootstrap.Bootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioSocketChannel;import java.net.InetSocketAddress;/** * • 连接服务器 • 写数据到服务器 • 等待接受服务器返回相同的数据 • 关闭连接 * */public class EchoClient &#123; private final String host; private final int port; public EchoClient(String host, int port) &#123; this.host = host; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup nioEventLoopGroup = null; try &#123; //创建Bootstrap对象用来引导启动客户端 Bootstrap bootstrap = new Bootstrap(); //创建EventLoopGroup对象并设置到Bootstrap中，EventLoopGroup可以理解为是一个线程池，这个线程池用来处理连接、接受数据、发送数据 nioEventLoopGroup = new NioEventLoopGroup(); //创建InetSocketAddress并设置到Bootstrap中，InetSocketAddress是指定连接的服务器地址 bootstrap.group(nioEventLoopGroup).channel(NioSocketChannel.class).remoteAddress(new InetSocketAddress(host, port)) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; //添加一个ChannelHandler，客户端成功连接服务器后就会被执行 @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new EchoClientHandler()); &#125; &#125;); // • 调用Bootstrap.connect()来连接服务器 ChannelFuture f = bootstrap.connect().sync(); // • 最后关闭EventLoopGroup来释放资源 f.channel().closeFuture().sync(); &#125; finally &#123; nioEventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; new EchoClient("localhost", 20000).start(); &#125;&#125; netty中handler的执行顺序 Handler在netty中，无疑占据着非常重要的地位。Handler与Servlet中的filter很像，通过Handler可以完成通讯报文的解码编码、拦截指定的报文、统一对日志错误进行处理、统一对请求进行计数、控制Handler执行与否。一句话，没有它做不到的只有你想不到的。 Netty中的所有handler都实现自ChannelHandler接口。按照输出输出来分，分为ChannelInboundHandler、ChannelOutboundHandler两大类。ChannelInboundHandler对从客户端发往服务器的报文进行处理，一般用来执行解码、读取客户端数据、进行业务处理等；ChannelOutboundHandler对从服务器发往客户端的报文进行处理，一般用来进行编码、发送报文到客户端。 Netty中，可以注册多个handler。ChannelInboundHandler按照注册的先后顺序执行；ChannelOutboundHandler按照注册的先后顺序逆序执行，如下图所示，按照注册的先后顺序对Handler进行排序，request进入Netty后的执行顺序为 代码示例 server 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.Channel;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;/** * • 配置服务器功能，如线程、端口 • 实现服务器处理程序，它包含业务逻辑，决定当有一个请求连接或接收数据时该做什么 */public class EchoServer &#123; private final int port; public EchoServer(int port) &#123; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup eventLoopGroup = null; try &#123; //server端引导类 ServerBootstrap serverBootstrap = new ServerBootstrap(); //连接池处理数据 eventLoopGroup = new NioEventLoopGroup(); serverBootstrap.group(eventLoopGroup) .channel(NioServerSocketChannel.class)//指定通道类型为NioServerSocketChannel，一种异步模式，OIO阻塞模式为OioServerSocketChannel .localAddress("localhost",port)//设置InetSocketAddress让服务器监听某个端口已等待客户端连接。 .childHandler(new ChannelInitializer&lt;Channel&gt;() &#123;//设置childHandler执行所有的连接请求 @Override protected void initChannel(Channel ch) throws Exception &#123; // 注册两个InboundHandler，执行顺序为注册顺序，所以应该是InboundHandler1 InboundHandler2 // 注册两个OutboundHandler，执行顺序为注册顺序的逆序，所以应该是OutboundHandler2 OutboundHandler1 ch.pipeline().addLast(new EchoInHandler1()); ch.pipeline().addLast(new EchoInHandler2()); ch.pipeline().addLast(new EchoOutHandler1()); ch.pipeline().addLast(new EchoOutHandler2()); &#125; &#125;); // 最后绑定服务器等待直到绑定完成，调用sync()方法会阻塞直到服务器完成绑定,然后服务器等待通道关闭，因为使用sync()，所以关闭操作也会被阻塞。 ChannelFuture channelFuture = serverBootstrap.bind().sync(); System.out.println("开始监听，端口为：" + channelFuture.channel().localAddress()); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; eventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; new EchoServer(20000).start(); &#125;&#125; EchoInHandler1 1234567891011121314151617181920212223public class EchoInHandler1 extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println("in1"); // 通知执行下一个InboundHandler ctx.fireChannelRead(msg); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush();//刷新后才将数据发出到SocketChannel &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; ​ EchoInHandler2 1234567891011121314151617181920212223242526272829303132333435363738394041import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import java.util.Date;import cn.itcast_03_netty.sendobject.bean.Person;public class EchoInHandler2 extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println("in2"); ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, "UTF-8"); System.out.println("接收客户端数据:" + body); //向客户端写数据 System.out.println("server向client发送数据"); String currentTime = new Date(System.currentTimeMillis()).toString(); ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes()); ctx.write(resp); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush();//刷新后才将数据发出到SocketChannel &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; ​ EchoOutHandler1 123456789101112131415161718192021import java.util.Date;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelOutboundHandlerAdapter;import io.netty.channel.ChannelPromise;public class EchoOutHandler1 extends ChannelOutboundHandlerAdapter &#123; @Override // 向client发送消息 public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; System.out.println("out1"); /*System.out.println(msg);*/ String currentTime = new Date(System.currentTimeMillis()).toString(); ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes()); ctx.write(resp); ctx.flush(); &#125;&#125; ​ EchoOutHandler2 12345678910111213141516import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelOutboundHandlerAdapter;import io.netty.channel.ChannelPromise;public class EchoOutHandler2 extends ChannelOutboundHandlerAdapter &#123; @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; System.out.println("out2"); // 执行下一个OutboundHandler /*System.out.println("at first..msg = "+msg); msg = "hi newed in out2";*/ super.write(ctx, msg, promise); &#125;&#125; ​ 总结 在使用Handler的过程中，需要注意： ChannelInboundHandler之间的传递，通过调用ctx.fireChannelRead(msg)实现；调用ctx.write(msg) 将传递到ChannelOutboundHandler。 ctx.write()方法执行后，需要调用flush()方法才能令它立即执行。 流水线pipeline中outhandler不能放在最后，否则不生效 Handler的消费处理放在最后一个处理。 netty发送对象简介 Netty中，通讯的双方建立连接后，会把数据按照ByteBuf的方式进行传输，例如http协议中，就是通过HttpRequestDecoder对ByteBuf数据流进行处理，转换成http的对象。基于这个思路，我自定义一种通讯协议：Server和客户端直接传输java对象。 实现的原理是通过Encoder把java对象转换成ByteBuf流进行传输，通过Decoder把ByteBuf转换成java对象进行处理，处理逻辑如下图所示： 代码 bean 1234567891011121314import java.io.Serializable;public class Person implements Serializable &#123; private static final long serialVersionUID = 1L; private String name; private String sex; private int age; public String toString() &#123; return "name:" + name + " sex:" + sex + " age:" + age; &#125;get/set...&#125; 序列化 1234567891011121314151617181920import io.netty.buffer.ByteBuf;import io.netty.channel.ChannelHandlerContext;import io.netty.handler.codec.MessageToByteEncoder;import cn.itcast_03_netty.sendobject.bean.Person;import cn.itcast_03_netty.sendobject.utils.ByteObjConverter; /** * 序列化 * 将object转换成Byte[] * */public class PersonEncoder extends MessageToByteEncoder&lt;Person&gt; &#123; @Override protected void encode(ChannelHandlerContext ctx, Person msg, ByteBuf out) throws Exception &#123; //工具类：将object转换为byte[] byte[] datas = ByteObjConverter.objectToByte(msg); out.writeBytes(datas); ctx.flush(); &#125;&#125; 反序列化 1234567891011121314151617181920212223242526import io.netty.buffer.ByteBuf;import io.netty.channel.ChannelHandlerContext;import io.netty.handler.codec.ByteToMessageDecoder;import java.util.List;import cn.itcast_03_netty.sendobject.utils.ByteBufToBytes;import cn.itcast_03_netty.sendobject.utils.ByteObjConverter; /** * 反序列化 * 将Byte[]转换为Object * */public class PersonDecoder extends ByteToMessageDecoder &#123; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; //工具类：将ByteBuf转换为byte[] ByteBufToBytes read = new ByteBufToBytes(); byte[] bytes = read.read(in); //工具类：将byte[]转换为object Object obj = ByteObjConverter.byteToObject(bytes); out.add(obj); &#125; &#125; 转换工具类 123456789101112131415import io.netty.buffer.ByteBuf;public class ByteBufToBytes &#123; /** * 将ByteBuf转换为byte[] * @param datas * @return */ public byte[] read(ByteBuf datas) &#123; byte[] bytes = new byte[datas.readableBytes()];// 创建byte[] datas.readBytes(bytes);// 将ByteBuf转换为byte[] return bytes; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import java.io.ByteArrayInputStream;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;public class ByteObjConverter &#123; /** * 使用IO的inputstream流将byte[]转换为object * @param bytes * @return */ public static Object byteToObject(byte[] bytes) &#123; Object obj = null; ByteArrayInputStream bi = new ByteArrayInputStream(bytes); ObjectInputStream oi = null; try &#123; oi = new ObjectInputStream(bi); obj = oi.readObject(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; bi.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; oi.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return obj; &#125; /** * 使用IO的outputstream流将object转换为byte[] * @param bytes * @return */ public static byte[] objectToByte(Object obj) &#123; byte[] bytes = null; ByteArrayOutputStream bo = new ByteArrayOutputStream(); ObjectOutputStream oo = null; try &#123; oo = new ObjectOutputStream(bo); oo.writeObject(obj); bytes = bo.toByteArray(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; bo.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; oo.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return bytes; &#125;&#125; ServerHandler 123456789101112131415161718192021222324252627282930import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import cn.itcast_03_netty.sendobject.bean.Person;public class EchoServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; Person person = (Person) msg; System.out.println(person.getName()); System.out.println(person.getAge()); System.out.println(person.getSex()); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("server 读取数据完毕.."); ctx.flush();//刷新后才将数据发出到SocketChannel &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; server 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.Channel;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;import cn.itcast_03_netty.sendobject.coder.PersonDecoder;/** * • 配置服务器功能，如线程、端口 • 实现服务器处理程序，它包含业务逻辑，决定当有一个请求连接或接收数据时该做什么 * * */public class EchoServer &#123; private final int port; public EchoServer(int port) &#123; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup eventLoopGroup = null; try &#123; //创建ServerBootstrap实例来引导绑定和启动服务器 ServerBootstrap serverBootstrap = new ServerBootstrap(); //创建NioEventLoopGroup对象来处理事件，如接受新连接、接收数据、写数据等等 eventLoopGroup = new NioEventLoopGroup(); //指定通道类型为NioServerSocketChannel，一种异步模式，OIO阻塞模式为OioServerSocketChannel //设置InetSocketAddress让服务器监听某个端口已等待客户端连接。 serverBootstrap.group(eventLoopGroup).channel(NioServerSocketChannel.class).localAddress("localhost",port) .childHandler(new ChannelInitializer&lt;Channel&gt;() &#123; //设置childHandler执行所有的连接请求 @Override protected void initChannel(Channel ch) throws Exception &#123; //注册解码的handler ch.pipeline().addLast(new PersonDecoder()); //IN1 反序列化 //添加一个入站的handler到ChannelPipeline ch.pipeline().addLast(new EchoServerHandler()); //IN2 &#125; &#125;); // 最后绑定服务器等待直到绑定完成，调用sync()方法会阻塞直到服务器完成绑定,然后服务器等待通道关闭，因为使用sync()，所以关闭操作也会被阻塞。 ChannelFuture channelFuture = serverBootstrap.bind().sync(); System.out.println("开始监听，端口为：" + channelFuture.channel().localAddress()); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; eventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; new EchoServer(20000).start(); &#125;&#125; clientHandler 12345678910111213141516171819202122232425262728293031323334353637383940import io.netty.buffer.ByteBuf;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.SimpleChannelInboundHandler;import cn.itcast_03_netty.sendobject.bean.Person;public class EchoClientHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123; // 客户端连接服务器后被调用 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; Person person = new Person(); person.setName("angelababy"); person.setSex("girl"); person.setAge(18); ctx.write(person); ctx.flush(); &#125; // • 从服务器接收到数据后调用 @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception &#123; System.out.println("client 读取server数据.."); // 服务端返回消息后 ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, "UTF-8"); System.out.println("服务端数据为 :" + body); &#125; // • 发生异常时被调用 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println("client exceptionCaught.."); // 释放资源 ctx.close(); &#125;&#125; client 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import io.netty.bootstrap.Bootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioSocketChannel;import java.net.InetSocketAddress;import cn.itcast_03_netty.sendobject.coder.PersonEncoder;/** * • 连接服务器 • 写数据到服务器 • 等待接受服务器返回相同的数据 • 关闭连接 * */public class EchoClient &#123; private final String host; private final int port; public EchoClient(String host, int port) &#123; this.host = host; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup nioEventLoopGroup = null; try &#123; // 创建Bootstrap对象用来引导启动客户端 Bootstrap bootstrap = new Bootstrap(); // 创建EventLoopGroup对象并设置到Bootstrap中，EventLoopGroup可以理解为是一个线程池，这个线程池用来处理连接、接受数据、发送数据 nioEventLoopGroup = new NioEventLoopGroup(); // 创建InetSocketAddress并设置到Bootstrap中，InetSocketAddress是指定连接的服务器地址 bootstrap.group(nioEventLoopGroup)// .channel(NioSocketChannel.class)// .remoteAddress(new InetSocketAddress(host, port))// .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123;// // 添加一个ChannelHandler，客户端成功连接服务器后就会被执行 @Override protected void initChannel(SocketChannel ch) throws Exception &#123; // 注册编码的handler ch.pipeline().addLast(new PersonEncoder()); //out //注册处理消息的handler ch.pipeline().addLast(new EchoClientHandler()); //in &#125; &#125;); // • 调用Bootstrap.connect()来连接服务器 ChannelFuture f = bootstrap.connect().sync(); // • 最后关闭EventLoopGroup来释放资源 f.channel().closeFuture().sync(); &#125; finally &#123; nioEventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; new EchoClient("localhost", 20000).start(); &#125;&#125;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2Fhive%E9%97%AE%E9%A2%98%E9%9B%86%2F</url>
    <content type="text"><![CDATA[1、FAILED: Error in metadata: java.lang.RuntimeException: Unable to instantiate org问题： 起因是我重装了mysql数据库。 安装之后 把访问权限都配置好 ： 1234GRANT ALL PRIVILEGES ON*.* TO &apos;hive&apos;@&apos;%&apos; Identified by &apos;hive&apos;; GRANT ALL PRIVILEGES ON*.* TO &apos;hive&apos;@&apos;localhost&apos; Identified by &apos;hive&apos;; GRANT ALL PRIVILEGES ON*.* TO &apos;hive&apos;@&apos;127.0.0.1&apos; Identified by &apos;hive&apos;; 本机地址： 192.168.103.43 机器名字：192-168-103-43 flush privileges;启动hive 发生下面的错误： 12hive&gt; show tables;FAILED: Error in metadata: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTaskFAILED: Error in metadata: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClientFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask 123cd $&#123;HIVE_HOME&#125;/bin ./hive -hiveconf hive.root.logger=DEBUG,consolehive&gt; show tables; 得到如下的错误信息(当然 不同的问题所产生的日志是不同的)： 1234567891011121314151617181920212223242526272829303132333435Caused by: javax.jdo.JDOFatalDataStoreException: Access denied for user &apos;hive&apos;@&apos;192-168-103-43&apos; (using password: YES) NestedThrowables: java.sql.SQLException: Access denied for user &apos;hive&apos;@&apos;192-168-103-43&apos; (using password: YES) at org.datanucleus.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:298) at org.datanucleus.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:601) at org.datanucleus.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:286) at org.datanucleus.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:182) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at javax.jdo.JDOHelper$16.run(JDOHelper.java:1958) at java.security.AccessController.doPrivileged(Native Method) at javax.jdo.JDOHelper.invoke(JDOHelper.java:1953) at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1159) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:803) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:698) at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:262) at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:291) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:224) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:199) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117) at org.apache.hadoop.hive.metastore.RetryingRawStore.&lt;init&gt;(RetryingRawStore.java:62) at org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:413) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:401) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:439) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:325) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.&lt;init&gt;(HiveMetaStore.java:285) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:53) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:58) at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4102) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:121) ... 28 more 原因： 发现数据库的权限 HIVE需要的是 ‘hive‘@’192-168-103-43’ 这个IP地址 解决： 然后试着在mysql中加上权限： 12GRANT ALL PRIVILEGES ON*.* TO &apos;hive&apos;@&apos;192-168-103-43&apos; Identified by &apos;hive&apos;; flush privileges; 再次登录hive hive&gt; show tables;OK 2、Hive出现异常 FAILED: Error In Metadata: Java.Lang.RuntimeException: Unable To Instan问题： 在公司的虚拟机上运行hive计算，因为要计算的数据量较大，频繁，导致了服务器负载过高，mysql也出现无法连接的问题，最后虚拟机出现The remote system refused the connection.重启虚拟机后，进入hive。 hive&gt; show tables;出现了下面的问题： 12FAILED: Error in metadata: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTaskFAILED: Error in metadata: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClientFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask 原因： 用下面的命令，重新启动hive 12./hive -hiveconf hive.root.logger=DEBUG,consolehive&gt; show tables; 能够看到更深层次的原因的是： 12345678910Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1076) … 23 moreCaused by: javax.jdo.JDODataStoreException: Exception thrown obtaining schema column information from datastore NestedThrowables: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table ‘hive.DELETEME1370713761025′ doesn’t exist 根据提示的信息，登陆mysql或者mysql客户端查看hive的数据库的表信息 12345678910111213141516171819202122232425262728293031323334353637383940414243mysql&gt; use hive; mysql&gt; show tables; +—————————+| Tables_in_hive |+—————————+| BUCKETING_COLS || CDS || COLUMNS_V2 || DATABASE_PARAMS || DBS || DELETEME1370677637267 || DELETEME1370712928271 || DELETEME1370713342355 || DELETEME1370713589772 || DELETEME1370713761025 || DELETEME1370713792915 || IDXS || INDEX_PARAMS || PARTITIONS || PARTITION_KEYS || PARTITION_KEY_VALS || PARTITION_PARAMS || PART_COL_PRIVS || PART_COL_STATS || PART_PRIVS || SDS || SD_PARAMS || SEQUENCE_TABLE || SERDES || SERDE_PARAMS || SKEWED_COL_NAMES || SKEWED_COL_VALUE_LOC_MAP || SKEWED_STRING_LIST || SKEWED_STRING_LIST_VALUES || SKEWED_VALUES || SORT_COLS || TABLE_PARAMS || TAB_COL_STATS || TBLS || TBL_COL_PRIVS || TBL_PRIVS |+—————————+36 rows in set (0.00 sec) 能够看到“DELETEME1370713792915”这个表，问题明确了，由于计算的压力过大，服务器停止响应，mysql也停止了响应，mysql进程被异常终止，在运行中的mysql表数据异常，hive的元数据表异常。 解决问题的办法有两个： 1.直接在mysql中drop 异常提示中的table;mysql&gt;drop table DELETEME1370713761025;2.保守的做法，根据DELETEME*表的结构，创建不存在的表CREATE TABLE DELETEME1370713792915 ( UNUSED int(11) NOT NULL ) ENGINE=InnoDB DEFAULT CHARSET=latin1;通过实践，第一个方法就能够解决问题，如果不行可以尝试第二个方法。 3、hive错误show tables无法使用 : Unable to instantiate rg.apache.hadoop.hive.metastore.问题： hive异常show tables无法使用:Unable to instantiate rg.apache.hadoop.hive.metastore.HiveMetaStoreClient异常： 123hive&gt; show tables; FAILED: Error in metadata: java.lang.RuntimeException: Unable to instantiate rg.apache.hadoop.hive.metastore.HiveMetaStoreClient FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask 原因： 在其他shell 开了hive 没有关闭 解决： 使用 ps -ef | grep hive kill -9杀死进程 4、FAILED: Error in metadata: java.lang.RuntimeException: Unable to in(2)(08-52-23)问题：安装配置Hive时报错： 12FAILED: Error in metadata: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask 用调试模式报错如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899[root@hadoop1 bin]# hive -hiveconf hive.root.logger=DEBUG,console13/10/09 16:16:27 DEBUG common.LogUtils: Using hive-site.xml found on CLASSPATH at /opt/hive-0.11.0/conf/hive-site.xml 13/10/09 16:16:27 DEBUG conf.Configuration: java.io.IOException: config() at org.apache.hadoop.conf.Configuration.&lt;init&gt;(Configuration.java:227) at org.apache.hadoop.conf.Configuration.&lt;init&gt;(Configuration.java:214) at org.apache.hadoop.hive.conf.HiveConf.&lt;init&gt;(HiveConf.java:1039) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:636) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:156)13/10/09 16:16:27 DEBUG conf.Configuration: java.io.IOException: config() at org.apache.hadoop.conf.Configuration.&lt;init&gt;(Configuration.java:227) at org.apache.hadoop.conf.Configuration.&lt;init&gt;(Configuration.java:214) at org.apache.hadoop.mapred.JobConf.&lt;init&gt;(JobConf.java:330) at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:1073) at org.apache.hadoop.hive.conf.HiveConf.&lt;init&gt;(HiveConf.java:1040) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:636) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:156)Logging initialized using configuration in file:/opt/hive-0.11.0/conf/hive-log4j.properties 13/10/09 16:16:27 INFO SessionState: Logging initialized using configuration in file:/opt/hive-0.11.0/conf/hive-log4j.properties 13/10/09 16:16:27 DEBUG parse.VariableSubstitution: Substitution is on: hive Hive history file=/tmp/root/hive_job_log_root_4666@hadoop1_201310091616_1069706211.txt 13/10/09 16:16:27 INFO exec.HiveHistory: Hive history file=/tmp/root/hive_job_log_root_4666@hadoop1_201310091616_1069706211.txt 13/10/09 16:16:27 DEBUG conf.Configuration: java.io.IOException: config() at org.apache.hadoop.conf.Configuration.&lt;init&gt;(Configuration.java:227) at org.apache.hadoop.conf.Configuration.&lt;init&gt;(Configuration.java:214) at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:187) at org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled(UserGroupInformation.java:239) at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:438) at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:424) at org.apache.hadoop.hive.shims.HadoopShimsSecure.getUGIForConf(HadoopShimsSecure.java:491) at org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.setConf(HadoopDefaultAuthenticator.java:51) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117) at org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthenticator(HiveUtils.java:365) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:270) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:670) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:156)13/10/09 16:16:27 DEBUG security.Groups: Creating new Groups object 13/10/09 16:16:27 DEBUG security.Groups: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping; cacheTimeout=300000 13/10/09 16:16:27 DEBUG security.UserGroupInformation: hadoop login 13/10/09 16:16:27 DEBUG security.UserGroupInformation: hadoop login commit 13/10/09 16:16:27 DEBUG security.UserGroupInformation: using local user:UnixPrincipal锛?root 13/10/09 16:16:27 DEBUG security.UserGroupInformation: UGI loginUser:root 13/10/09 16:16:27 DEBUG security.Groups: Returning fetched groups for &apos;root&apos; 13/10/09 16:16:27 DEBUG security.Groups: Returning cached groups for &apos;root&apos; 13/10/09 16:16:27 DEBUG conf.Configuration: java.io.IOException: config(config) at org.apache.hadoop.conf.Configuration.&lt;init&gt;(Configuration.java:260) at org.apache.hadoop.hive.conf.HiveConf.&lt;init&gt;(HiveConf.java:1044) at org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider.init(DefaultHiveAuthorizationProvider.java:30) at org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.setConf(HiveAuthorizationProviderBase.java:108) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117) at org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthorizeProviderManager(HiveUtils.java:339) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:272) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:670) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:156)13/10/09 16:16:27 DEBUG conf.Configuration: java.io.IOException: config() at org.apache.hadoop.conf.Configuration.&lt;init&gt;(Configuration.java:227) at org.apache.hadoop.conf.Configuration.&lt;init&gt;(Configuration.java:214) at org.apache.hadoop.mapred.JobConf.&lt;init&gt;(JobConf.java:330) at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:1073) at org.apache.hadoop.hive.conf.HiveConf.&lt;init&gt;(HiveConf.java:1045) at org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider.init(DefaultHiveAuthorizationProvider.java:30) at org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.setConf(HiveAuthorizationProviderBase.java:108) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117) at org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthorizeProviderManager(HiveUtils.java:339) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:272) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:670) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:156) 原因： 这个错误应该是你集成了mysql，从而报错 解决： 修改hive-site.xml，参照： 12345&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.1.101:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; 5、Hive的–auxpath使用相对路径遇到的一个奇怪的异常 问题： 在使用Hive的–auxpath过程中，如果我使用的是相对路径（例如，–auxpath=abc.jar），会产生下面的一个异常： 1234567java.lang.IllegalArgumentException: Can not create a Path from an empty string at org.apache.hadoop.fs.Path.checkPathArg(Path.java:91) at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:99) at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:58) at org.apache.hadoop.mapred.JobClient.copyRemoteFiles(JobClient.java:619) at org.apache.hadoop.mapred.JobClient.copyAndConfigureFiles(JobClient.java:724) at org.apache.hadoop.mapred.JobClient.copyAndConfigureFiles(JobClient.java:648) 原因： 从异常的内容来看，是由于使用了一个空字符串来创建一个Path对象。 经过分析发现，使用”–auxpath=abc.jar”来启动Hive时，Hive会自动在abc.jar前面补上”file://“。也就是说Hive最后使用的路径是”file://abc.jar”。 当我们使用”file://abc.jar”来生成一个Path时，调用这个Path的getName将会返回””(空字符串)。而Hive在提交MapReduce的Job时，会使用getName来获取文件名，并创建一个新的Path对象。下面的示例代码演示了一下这个过程，会抛出上文提到的异常。 1234Path path = new Path(&quot;file://abc.jar&quot;); System.out.println(&quot;path name:&quot; + path.getName()); System.out.println(&quot;authority:&quot; + path.toUri().getAuthority()); Path newPath = new Path(path.getName()); 上文的代码输出 path name:authority:abc.jar并抛出了异常”Can not create a Path from an empty string” 那么为什么”file://abc.jar”生成的Path的getName返回的是””而不是”abc.jar”呢，而且”abc.jar”却成了authority？在Path中的处理代码如下： 12345678if (pathString.startsWith(&quot;//&quot;, start) &amp;&amp; (pathString.length()-start &gt; 2)) &#123; // has authority int nextSlash = pathString.indexOf(&apos;/&apos;, start+2); int authEnd = nextSlash &gt; 0 ? nextSlash : pathString.length(); authority = pathString.substring(start+2, authEnd); start = authEnd;&#125;// uri path is the rest of the string -- query &amp; fragment not supportedString path = pathString.substring(start, pathString.length()); pathString就是传进去的”file://abc.jar”，由于我们只有两个”/“因此，从第二个”/“到结尾的字符串（”abc.jar”）都被当成了authority，path（内部的成员）则设置成了””而getName返回的就是path，因此也就为””了。 解决： 如果使用Hive的–auxpath来设置jar，必须使用绝对路径，或者使用”file:///.abc.jar”这样的表示法。这个才是Hadoop的Path支持的方式。事实上，hadoop许多相关的Path的设置，都存在这个问题，所以在无法确定的情况下，就不要使用相对路径了。 6、启动hive hwi服务时出现 HWI WAR file not found错误问题： 1234hive --service hwi [niy@niy-computer /]$ $HIVE_HOME/bin/hive --service hwi13/04/26 00:21:17 INFO hwi.HWIServer: HWI is starting up 13/04/26 00:21:18 FATAL hwi.HWIServer: HWI WAR file not found at /usr/local/hive/usr/local/hive/lib/hive-hwi-0.12.0-SNAPSHOT.war 原因： 可以看出/usr/local/hive/usr/local/hive/lib/hive-hwi-0.12.0-SNAPSHOT.war肯定不是正确路径，真正路径是/usr/local/hive/lib/hive-hwi-0.12.0-SNAPSHOT.war，断定是配置的问题 解决：将hive-default.xml中关于 hwi的设置拷贝到hive-site.xml中即可 123456789101112131415&lt;property&gt; &lt;name&gt;hive.hwi.war.file&lt;/name&gt; &lt;value&gt;lib/hive-hwi-0.12.0-SNAPSHOT.war&lt;/value&gt; &lt;description&gt;This sets the path to the HWI war file, relative to $&#123;HIVE_HOME&#125;. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.hwi.listen.host&lt;/name&gt; &lt;value&gt;0.0.0.0&lt;/value&gt; &lt;description&gt;This is the host address the Hive Web Interface will listen on&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.hwi.listen.port&lt;/name&gt; &lt;value&gt;9999&lt;/value&gt; &lt;description&gt;This is the port the Hive Web Interface will listen on&lt;/description&gt; &lt;/property&gt; 7、问题：当启动Hive的时候报错： 1234Caused by: javax.jdo.JDOException: Couldnt obtain a new sequence (unique id) : Cannot execute statement: impossible to write to binary log since BINLOG_FORMAT = STATEMENTandat least one table uses a storage engine limited to row-based logging. InnoDB is limited to row-logging when transaction isolation level is READ COMMITTED or READ UNCOMMITTED. NestedThrowables: java.sql.SQLException: Cannot execute statement: impossible to write to binary log since BINLOG_FORMAT = STATEMENT andat least one table uses a storage engine limited to row-based logging. InnoDB is limited to row-logging when transaction isolation level is READ COMMITTED or READ UNCOMMITTED. 原因： 这个问题是由于hive的元数据存储MySQL配置不当引起的 解决办法： mysql&gt; setglobal binlog_format=&#39;MIXED&#39;;8、问题： 当在Hive中创建表的时候报错： 12create table years (year string, eventstring) row format delimited fields terminated by&apos;\t&apos;; FAILED: Execution Error, return code 1from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:For direct MetaStore DB connections, we don&apos;t support retries at the client level.) 原因： 这是由于字符集的问题，需要配置MySQL的字符集： 解决办法： mysql&gt; alter database hive character set latin1;9、问题： 当执行Hive客户端时候出现如下错误： WARN conf.HiveConf: HiveConf of name hive.metastore.localdoesnot exist原因：这是由于在0.10 0.11或者之后的HIVE版本 hive.metastore.local 属性不再使用。 解决办法： 将该参数从hive-site.xml删除即可。 10、问题：在启动Hive报如下错误： (Permission denied: user=anonymous, access=EXECUTE, inode=”/tmp”:hadoop:supergroup:drwx—— 原因：这是由于Hive没有hdfs:/tmp目录的权限， 解决办法： 赋权限即可：hadoop dfs -chmod -R 777 /tmp 11、Hive查询数据库时，出现null。无数据显示。问题如下： Hive问题集 解决办法： LOAD DATA LOCAL INPATH &#39;/tmp/sanple.txt&#39; overwrite into table animal FIELDS TERMINATED BY &#39;\t&#39;;解释： 数据分隔符的问题，定义表的时候需要定义数据分隔符， FIELDS TERMINATED BY &#39;\t&#39;这个字段就说明了数据分隔符是tab。 具体分割符请以自己的文化中具体的情况来定。 12、ERROR beeline.ClassNameCompleter: Fail to parse the class name from the Jar file due to在使用beeline链接Hive服务的时候，报了下面的这个错误： 1234567beeline&gt; !connect jdbc:hive2//h2slave1:10000 scan complete in 1ms 16/07/27 11:40:54 [main]: ERROR beeline.ClassNameCompleter: Fail to parse the class name from the Jar file due to the exception:java.io.FileNotFoundException: minlog-1.2.jar (没有那个文件或目录) 16/07/27 11:40:54 [main]: ERROR beeline.ClassNameCompleter: Fail to parse the class name from the Jar file due to the exception:java.io.FileNotFoundException: objenesis-1.2.jar (没有那个文件或目录) 16/07/27 11:40:54 [main]: ERROR beeline.ClassNameCompleter: Fail to parse the class name from the Jar file due to the exception:java.io.FileNotFoundException: reflectasm-1.07-shaded.jar (没有那个文件或目录) scan complete in 596ms No known driver to handle &quot;jdbc:hive2//h2slave1:10000&quot; 解决： 其实这个问题是由于jdbc协议地址写错造成的，在hive2之后少了个“：” 改成以下这个形式即可： beeline&gt; !connect jdbc:hive2://h2slave1:1000013、Missing Hive Execution Jar: /…/hive-exec-.jar 运行hive时显示Missing Hive Execution Jar: /usr/hive/hive-0.11.0/bin/lib/hive-exec-.jar 运行hive时显示Missing Hive Execution Jar: /usr/hive/hive-0.11.0/bin/lib/hive-exec-*.jar 细细分析这个目录/bin/lib，在hive安装文件夹中这两个目录是并列的，而系统能够找到这样的链接，说明hive在centos系统配置文件中的路径有误，打开 /etc/profile会发现hive的配置路径为 export PATH=$JAVA_HOME/bin:$PATH:/usr/hive/hive-0.11.0/bin明显可以看出是路径配置的问题，这样的配置系统会在hive安装文件夹中的bin目录下寻找它所需要的jar包，而bin和lib文件夹是并列的，所以我们需要在centos系统配置文件中将hive路径配置为文件夹安装路径，即 export PATH=$JAVA_HOME/bin:$PATH:/usr/hive/hive-0.11.0注意：这种问题一般都是出在环境变量上面的配置。请认真检查etc/profile跟你hive的安装路径。 14、hive启动报错： Found class jline.Terminal, but interface was expected错误如下： 12345678910111213141516[ERROR] Terminal initialization failed; falling back to unsupportedjava.lang.IncompatibleClassChangeError: Found class jline.Terminal, but interface was expected at jline.TerminalFactory.create(TerminalFactory.java:101) at jline.TerminalFactory.get(TerminalFactory.java:158) at jline.console.ConsoleReader.&lt;init&gt;(ConsoleReader.java:229) at jline.console.ConsoleReader.&lt;init&gt;(ConsoleReader.java:221) at jline.console.ConsoleReader.&lt;init&gt;(ConsoleReader.java:209) at org.apache.hadoop.hive.cli.CliDriver.getConsoleReader(CliDriver.java:773) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:715) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212) 原因： jline版本冲突不一致导致的，hadoop目录下存在老版本jline：/hadoop-2.6.0/share/hadoop/yarn/lib： jline-0.9.94.jar解决： 把hive中的新版本jline拷贝一份到hadoop的share/hadoop/yarn/lib即可 同时要把那个老版本的给删除 cp /hive/apache-hive-1.1.0-bin/lib/jline-2.12.jar /hadoop-2.5.2/share/hadoop/yarn/lib15、MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes 在使用hive时，使用mysql存储元数据的时候，遇到下面的错误： 123456com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes at sun.reflect.GeneratedConstructorAccessor31.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at com.mysql.jdbc.Util.handleNewInstance(Util.java:377) at com.mysql.jdbc.Util.getInstance(Util.java:360) 解决办法： 用mysql做元数据,要修改数据字符集 alter database hive character set latin1]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2FHDFS%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[#HDFS学习 HDFS前言 设计思想 分而治之：将大文件、大批量文件，分布式存放在大量服务器上，以便于采取分而治之的方式对海量数据进行运算分析； 在大数据系统中作用： 为各类分布式运算框架（如：mapreduce，spark，tez，……）提供数据存储服务 重点概念：文件切块，副本存放，元数据 HDFS的概念和特性 首先，它是一个文件系统，用于存储文件，通过统一的命名空间——目录树来定位文件 其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色； 重要特性如下： HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M HDFS文件系统会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data 目录结构及文件分块信息(元数据)的管理由namenode节点承担——namenode是HDFS集群主节点，负责维护整个hdfs文件系统的目录树，以及每一个路径（文件）所对应的block块信息（block的id，及所在的datanode服务器） 文件的各个block的存储管理由datanode节点承担—- datanode是HDFS集群从节点，每一个block都可以在多个datanode上存储多个副本（副本数量也可以通过参数设置dfs.replication） HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改 (注：适合用来做数据分析，并不适合用来做网盘应用，因为，不便修改，延迟大，网络开销大，成本太高) HDFS的shell(命令行客户端)操作HDFS命令行客户端使用操作 hadoop fs -ls / 常用命令参数介绍 命令 功能 示例 -help 输出这个命令参数手册 -ls 显示目录信息 hadoop fs -ls / -mkdir 在hdfs上创建目录 hadoop fs -mkdir -p /aaa/bbb/cc/dd -moveFromLocal 从本地剪切粘贴到hdfs hadoop fs -moveFromLocal /home/hadoop/a.txt /aaa/bbb/cc/dd -moveToLocal 从hdfs剪切粘贴到本地 hadoop fs -moveToLocal /aaa/bbb/cc/dd /home/hadoop/a.txt -appendToFile 追加一个文件到已经存在的文件末尾 Hadoop fs -appendToFile ./hello.txt /hello.txt -cat 显示文件内容 hadoop fs -cat /hello.txt -tail 显示一个文件的末尾 hadoop fs -tail /weblog/access_log.1 -text 以字符形式打印一个文件的内容 hadoop fs -text /weblog/access_log.1 -chgrp linux文件系统中的用法一样，对文件所属权限 -chmod linux文件系统中的用法一样，对文件所属权限 hadoop fs -chmod 666 /hello.txt -chown linux文件系统中的用法一样，对文件所属权限 hadoop fs -chown someuser:somegrp /hello.txt -copyFromLocal 从本地文件系统中拷贝文件到hdfs路径去 hadoop fs -copyFromLocal ./jdk.tar.gz /aaa/ -copyToLocal 从hdfs拷贝到本地 hadoop fs -copyToLocal /aaa/jdk.tar.gz -cp 从hdfs的一个路径拷贝hdfs的另一个路径 hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 -mv 在hdfs目录中移动文件 hadoop fs -mv /aaa/jdk.tar.gz / -get 等同于copyToLocal，就是从hdfs下载文件到本地 hadoop fs -get /aaa/jdk.tar.gz -getmerge 合并下载多个文件 hadoop fs -getmerge /aaa/log.* ./log.sum -put 等同于copyFromLocal hadoop fs -put /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 -rm 删除文件或文件夹 hadoop fs -rm -r /aaa/bbb/ -rmdir 删除空目录 hadoop fs -rmdir /aaa/bbb/ccc -df 统计文件系统的可用空间信息 hadoop fs -df -h / -du 统计文件夹的大小信息 hadoop fs -du -s -h /aaa/* -count 统计一个指定目录下的文件节点数量 hadoop fs -count /aaa/ -setrep 设置hdfs中文件的副本数量 `hadoop fs -setrep 3 /aaa/jdk.tar.gz HDFS原理篇概述 HDFS集群分为两大角色：NameNode、DataNode (Secondary Namenode) NameNode负责管理整个文件系统的元数据 DataNode 负责管理用户的文件数据块 文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上 每一个文件块可以有多个副本，并存放在不同的datanode上 Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量 HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行 HDFS写数据流程 客户端要向HDFS写数据，首先要跟namenode通信以确认可以写文件并获得接收文件block的datanode，然后，客户端按顺序将文件逐个block传递给相应datanode，并由接收到block的datanode负责向其他datanode复制block的副本 ####详细步骤图 ​ ####详细步骤解析 根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在 namenode返回是否可以上传 client请求第一个 block该传输到哪些datanode服务器上 namenode返回3个datanode服务器ABC client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端 client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答 当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。 HDFS读数据流程 客户端将要读取的文件路径发送给namenode，namenode获取文件的元信息（主要是block的存放位置信息）返回给客户端，客户端根据返回的信息找到相应datanode逐个获取文件的block并在客户端本地进行数据追加合并从而获得整个文件 详细步骤图 详细步骤解析 跟namenode通信查询元数据，找到文件块所在的datanode服务器 挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流 datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验） 客户端以packet为单位接收，现在本地缓存，然后写入目标文件 NAMENODE工作机制问题场景： 集群启动后，可以查看文件，但是上传文件时报错，打开web页面可看到namenode正处于safemode状态，怎么处理？ Namenode服务器的磁盘故障导致namenode宕机，如何挽救集群及数据？ Namenode是否可以有多个？namenode内存要配置多大？namenode跟集群数据存储能力有关系吗？ 文件的blocksize究竟调大好还是调小好？ NAMENODE职责 NAMENODE职责： 负责客户端请求的响应 元数据的管理（查询，修改） 元数据管理 namenode对数据的管理采用了三种存储形式： 内存元数据(NameSystem) 磁盘元数据镜像文件 数据操作日志文件（可通过日志运算出元数据） 元数据存储机制 内存中有一份完整的元数据(内存meta data) 磁盘有一个“准完整”的元数据镜像（fsimage）文件(在namenode的工作目录中) 用于衔接内存metadata和持久化元数据镜像fsimage之间的操作日志（edits文件）注：当客户端对hdfs中的文件进行新增或者修改操作，操作记录首先被记入edits日志文件中，当客户端操作成功后，相应的元数据会更新到内存meta.data中 元数据手动查看 可以通过hdfs的一个工具来查看edits中的信息 bin/hdfs oev -i edits -o edits.xml bin/hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml 元数据的checkpoint 每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint） checkpoint的详细过程图 checkpoint操作的触发条件配置参数 dfs.namenode.checkpoint.check.period=60 #检查触发条件是否满足的频率，60秒 dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary #以上两个参数做checkpoint操作时，secondary namenode的本地工作目录 dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir} dfs.namenode.checkpoint.max-retries=3 #最大重试次数dfs.namenode.checkpoint.period=3600 #两次checkpoint之间的时间间隔3600秒dfs.namenode.checkpoint.txns=1000000 #两次checkpoint之间最大的操作记录 checkpoint的附带作用 namenode和secondary namenode的工作目录存储结构完全相同，所以，当namenode故障退出需要重新恢复时，可以从secondary namenode的工作目录中将fsimage拷贝到namenode的工作目录，以恢复namenode的元数据 元数据目录说明 在第一次部署好Hadoop集群的时候，我们需要在NameNode（NN）节点上格式化磁盘\$HADOOP_HOME/bin/hdfs namenode -format格式化完成之后，将会在$dfs.namenode.name.dir/current目录下如下的文件结构: current/ |– VERSION |– edits_*_ _|– fsimage_0000000000008547077 |– fsimage_0000000000008547077.md5 |– seen_txid 其中的dfs.name.dir是在hdfs-site.xml文件中配置的，默认值如下: 1234&lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;file://$&#123;hadoop.tmp.dir&#125;/dfs/name&lt;/value&gt;&lt;/property&gt; hadoop.tmp.dir是在core-site.xml中配置的，默认值如下: 12345&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/tmp/hadoop-$&#123;user.name&#125;&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt;&lt;/property&gt; dfs.namenode.name.dir属性可以配置多个目录 如：/data1/dfs/name,/data2/dfs/name,/data3/dfs/name,….。各个目录存储的文件结构和内容都完全一样，相当于备份，这样做的好处是当其中一个目录损坏了，也不会影响到Hadoop的元数据，特别是当其中一个目录是NFS（网络文件系统Network File System，NFS）之上，即使你这台机器损坏了，元数据也得到保存。 下面对$dfs.namenode.name.dir/current/目录下的文件进行解释。 VERSION文件是Java属性文件，内容大致如下： #Fri Nov 15 19:47:46 CST 2013 namespaceID=934548976clusterID=CID-cdff7d73-93cd-4783-9399-0a22e6dce196cTime=0storageType=NAME_NODEblockpoolID=BP-893790215-192.168.24.72-1383809616115layoutVersion=-47 namespaceID是文件系统的唯一标识符，在文件系统首次格式化之后生成的； storageType说明这个文件存储的是什么进程的数据结构信息（如果是DataNode，storageType=DATA_NODE）； cTime表示NameNode存储时间的创建时间，由于我的NameNode没有更新过，所以这里的记录值为0，以后对NameNode升级之后，cTime将会记录更新时间戳； layoutVersion表示HDFS永久性数据结构的版本信息， 只要数据结构变更，版本号也要递减，此时的HDFS也需要升级，否则磁盘仍旧是使用旧版本的数据结构，这会导致新版本的NameNode无法使用； clusterID是系统生成或手动指定的集群ID，在-clusterid选项中可以使用它；如下说明 使用如下命令格式化一个Namenode： $HADOOP_HOME/bin/hdfs namenode -format [-clusterId &lt;cluster_id&gt;]选择一个唯一的cluster_id，并且这个cluster_id不能与环境中其他集群有冲突。如果没有提供cluster_id，则会自动生成一个唯一的ClusterID。 使用如下命令格式化其他Namenode $HADOOP_HOME/bin/hdfs namenode -format -clusterId &lt;cluster_id&gt; 升级集群至最新版本。在升级过程中需要提供一个ClusterID，例如： $HADOOP_CONF_DIR -upgrade -clusterId &lt;cluster_ID&gt;如果没有提供ClusterID，则会自动生成一个ClusterID。 blockpoolID：是针对每一个Namespace所对应的blockpool的ID，上面的这个BP-893790215-192.168.24.72-1383809616115就是在我的ns1的namespace下的存储块池的ID，这个ID包括了其对应的NameNode节点的ip地址。 \$dfs.namenode.name.dir/current/seen_txid非常重要，是存放transactionId的文件，format之后是0，它代表的是namenode里面的edits_*文件的尾数，namenode重启的时候，会按照seen_txid的数字，循序从头跑edits_0000001~到seen_txid的数字。所以当你的hdfs发生异常重启的时候，一定要比对seen_txid内的数字是不是你edits最后的尾数，不然会发生建置namenode时metaData的资料有缺少，导致误删Datanode上多余Block的资讯。 $dfs.namenode.name.dir/current目录下在format的同时也会生成fsimage和edits文件，及其对应的md5校验文件。 文件中记录的是edits滚动的序号，每次重启namenode时，namenode就知道要将哪些edits进行加载edits DATANODE的工作机制问题场景： 集群容量不够，怎么扩容？ 如果有一些datanode宕机，该怎么办？ datanode明明已启动，但是集群中的可用datanode列表中就是没有，怎么办？ 概述Datanode工作职责 存储管理用户的文件块数据 定期向namenode汇报自身所持有的block信息（通过心跳信息上报） （这点很重要，因为，当集群中发生某些block副本失效时，集群如何恢复block初始副本数量的问题） 12345&lt;property&gt; &lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt; &lt;value&gt;3600000&lt;/value&gt; &lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt;&lt;/property&gt; #####Datanode掉线判断时限参数 datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为： timeout = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval。 而默认的heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。 需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。所以，举个例子，如果heartbeat.recheck.interval设置为5000（毫秒），dfs.heartbeat.interval设置为3（秒，默认），则总的超时时间为40秒。 12345678&lt;property&gt; &lt;name&gt;heartbeat.recheck.interval&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; #####观察验证DATANODE功能 上传一个文件，观察文件的block具体的物理存放情况： 在每一台datanode机器上的这个目录中能找到文件的切块： /home/hadoop/app/hadoop-2.4.1/tmp/dfs/data/current/BP-193442119-192.168.2.120-1432457733977/current/finalized HDFS应用开发篇HDFS的java操作hdfs在生产应用中主要是客户端的开发，其核心步骤是从hdfs提供的api中构造一个HDFS的访问客户端对象，然后通过该客户端对象操作（增删改查）HDFS上的文件 搭建开发环境 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt;&lt;/dependency&gt; 注：如需手动引入jar包，hdfs的jar包—-hadoop的安装目录的share下 window下开发的说明 建议在linux下进行hadoop应用的开发，不会存在兼容性问题。如在window上做客户端应用开发，需要设置以下环境：A. 在windows的某个目录下解压一个hadoop的安装包B. 将安装包下的lib和bin目录用对应windows版本平台编译的本地库替换C. 在window系统中配置HADOOP_HOME指向你解压的安装包D. 在windows系统的path变量中加入hadoop的bin目录 获取api中的客户端对象 在java中操作hdfs，首先要获得一个客户端实例 12Configuration conf = new Configuration()FileSystem fs = FileSystem.get(conf) 而我们的操作目标是HDFS，所以获取到的fs对象应该是DistributedFileSystem的实例； get方法是从何处判断具体实例化那种客户端类呢？ 从conf中的一个参数 fs.defaultFS的配置值判断； 如果我们的代码中没有指定fs.defaultFS，并且工程classpath下也没有给定相应的配置，conf中的默认值就来自于hadoop的jar包中的core-default.xml，默认值为： file:///，则获取的将不是一个DistributedFileSystem的实例，而是一个本地文件系统的客户端对象 DistributedFileSystem实例对象所具备的方法 HDFS客户端操作数据代码示例文件的增删改查123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145import java.net.URI;import java.util.Iterator;import java.util.Map.Entry;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.BlockLocation;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.LocatedFileStatus;import org.apache.hadoop.fs.Path;import org.apache.hadoop.fs.RemoteIterator;import org.junit.Before;import org.junit.Test;/** * * 客户端去操作hdfs时，是有一个用户身份的 * 默认情况下，hdfs客户端api会从jvm中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=hadoop * * 也可以在构造客户端fs对象时，通过参数传递进去 * @author * */public class HdfsClientDemo &#123; FileSystem fs = null; Configuration conf = null; @Before public void init() throws Exception&#123; conf = new Configuration();// conf.set("fs.defaultFS", "hdfs://mini1:9000"); conf.set("dfs.replication", "5"); //拿到一个文件系统操作的客户端实例对象 fs = FileSystem.get(conf); //可以直接传入 uri和用户身份 fs = FileSystem.get(new URI("hdfs://mini1:9000"),conf,"hadoop"); &#125; /** * 上传文件 * @throws Exception */ @Test public void testUpload() throws Exception &#123; fs.copyFromLocalFile(new Path("c:/access.log"), new Path("/access.log.copy")); fs.close(); &#125; /** * 下载文件 * @throws Exception */ @Test public void testDownload() throws Exception &#123; fs.copyToLocalFile(new Path("/access.log.copy"), new Path("d:/")); &#125; /** * 打印参数 */ @Test public void testConf()&#123; Iterator&lt;Entry&lt;String, String&gt;&gt; it = conf.iterator(); while(it.hasNext())&#123; Entry&lt;String, String&gt; ent = it.next(); System.out.println(ent.getKey() + " : " + ent.getValue()); &#125; &#125; @Test public void testMkdir() throws Exception &#123; boolean mkdirs = fs.mkdirs(new Path("/testmkdir/aaa/bbb")); System.out.println(mkdirs); &#125; @Test public void testDelete() throws Exception &#123; boolean flag = fs.delete(new Path("/testmkdir/aaa"), true); System.out.println(flag); &#125; /** * 递归列出指定目录下所有子文件夹中的文件 * @throws Exception */ @Test public void testLs() throws Exception &#123; RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path("/"), true); while(listFiles.hasNext())&#123; LocatedFileStatus fileStatus = listFiles.next(); System.out.println("blocksize: " +fileStatus.getBlockSize()); System.out.println("owner: " +fileStatus.getOwner()); System.out.println("Replication: " +fileStatus.getReplication()); System.out.println("Permission: " +fileStatus.getPermission()); System.out.println("Name: " +fileStatus.getPath().getName()); System.out.println("------------------"); BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for(BlockLocation b:blockLocations)&#123; System.out.println("块起始偏移量: " +b.getOffset()); System.out.println("块长度:" + b.getLength()); //块所在的datanode节点 String[] datanodes = b.getHosts(); for(String dn:datanodes)&#123; System.out.println("datanode:" + dn); &#125; &#125; &#125; &#125; @Test public void testLs2() throws Exception &#123; FileStatus[] listStatus = fs.listStatus(new Path("/")); for(FileStatus file :listStatus)&#123; System.out.println("name: " + file.getPath().getName()); System.out.println((file.isFile()?"file":"directory")); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://mini1:9000"); //拿到一个文件系统操作的客户端实例对象 FileSystem fs = FileSystem.get(conf); fs.copyFromLocalFile(new Path("c:/access.log"), new Path("/access.log.copy")); fs.close(); &#125; &#125; 通过流的方式访问hdfs12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * 相对那些封装好的方法而言的更底层一些的操作方式 * 上层那些mapreduce spark等运算框架，去hdfs中获取数据的时候，就是调的这种底层的api * @author * */public class StreamAccess &#123; FileSystem fs = null; @Before public void init() throws Exception &#123; Configuration conf = new Configuration(); fs = FileSystem.get(new URI("hdfs://hdp-node01:9000"), conf, "hadoop"); &#125; /** * 通过流的方式上传文件到hdfs * @throws Exception */ @Test public void testUpload() throws Exception &#123; FSDataOutputStream outputStream = fs.create(new Path("/angelababy.love"), true); FileInputStream inputStream = new FileInputStream("c:/angelababy.love"); IOUtils.copy(inputStream, outputStream); &#125; @Test public void testDownLoadFileToLocal() throws IllegalArgumentException, IOException&#123; //先获取一个文件的输入流----针对hdfs上的 FSDataInputStream in = fs.open(new Path("/jdk-7u65-linux-i586.tar.gz")); //再构造一个文件的输出流----针对本地的 FileOutputStream out = new FileOutputStream(new File("c:/jdk.tar.gz")); //再将输入流中数据传输到输出流 IOUtils.copyBytes(in, out, 4096); &#125; /** * hdfs支持随机定位进行文件读取，而且可以方便地读取指定长度 * 用于上层分布式运算框架并发处理数据 * @throws IllegalArgumentException * @throws IOException */ @Test public void testRandomAccess() throws IllegalArgumentException, IOException&#123; //先获取一个文件的输入流----针对hdfs上的 FSDataInputStream in = fs.open(new Path("/iloveyou.txt")); //可以将流的起始偏移量进行自定义 in.seek(22); //再构造一个文件的输出流----针对本地的 FileOutputStream out = new FileOutputStream(new File("c:/iloveyou.line.2.txt")); IOUtils.copyBytes(in,out,19L,true); &#125; /** * 显示hdfs上文件的内容 * @throws IOException * @throws IllegalArgumentException */ @Test public void testCat() throws IllegalArgumentException, IOException&#123; FSDataInputStream in = fs.open(new Path("/iloveyou.txt")); IOUtils.copyBytes(in, System.out, 1024); &#125;&#125; 场景编程在mapreduce 、spark等运算框架中，有一个核心思想就是将运算移往数据，或者说，就是要在并发计算中尽可能让运算本地化，这就需要获取数据所在位置的信息并进行相应范围读取以下模拟实现：获取一个文件的所有block位置信息，然后读取指定block中的内容 123456789101112131415161718192021222324252627282930 @Test public void testCat() throws IllegalArgumentException, IOException&#123; FSDataInputStream in = fs.open(new Path("/wordcount/data")); //拿到文件信息 FileStatus[] listStatus = fs.listStatus(new Path("/wordcount/data")); //获取这个文件的所有block的信息 BlockLocation[] fileBlockLocations = fs.getFileBlockLocations(listStatus[0], 0L, listStatus[0].getLen()); //第一个block的长度 long length = fileBlockLocations[0].getLength(); //第一个block的起始偏移量 long offset = fileBlockLocations[0].getOffset(); System.out.println(length); System.out.println(offset); //获取第一个block写入输出流// IOUtils.copyBytes(in, System.out, (int)length); byte[] b = new byte[4096]; FileOutputStream os = new FileOutputStream(new File("d:/block0")); while(in.read(offset, b, 0, 4096)!=-1)&#123; os.write(b); offset += 4096; if(offset&gt;=length) return; &#125;; os.flush(); os.close(); in.close(); &#125;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2Fhadoop%E5%86%85%E7%BD%AERPC%20API%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[hadoop内置RPC API的使用client1234567891011121314151617import java.net.InetSocketAddress;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.ipc.RPC;import cn.demo.bigdata.hadooprpc.protocol.ClientNamenodeProtocol;public class MyHdfsClient &#123; public static void main(String[] args) throws Exception &#123; ClientNamenodeProtocol namenode = RPC.getProxy(ClientNamenodeProtocol.class, 1L, new InetSocketAddress("localhost", 8888), new Configuration()); String metaData = namenode.getMetaData("/angela.mygirl"); System.out.println(metaData); &#125;&#125; Protocol1234public interface ClientNamenodeProtocol &#123;// public static final long versionID=1L; public String getMetaData(String path);&#125; Server12345678910import cn.demo.bigdata.hadooprpc.protocol.ClientNamenodeProtocol;public class MyNameNode implements ClientNamenodeProtocol&#123; //模拟namenode的业务方法之一：查询元数据 @Override public String getMetaData(String path)&#123; return path+": 3 - &#123;BLK_1,BLK_2&#125; ...."; &#125;&#125; Publish123456789101112131415161718192021import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.ipc.RPC;import org.apache.hadoop.ipc.RPC.Builder;import org.apache.hadoop.ipc.RPC.Server;import cn.demo.bigdata.hadooprpc.protocol.ClientNamenodeProtocol;import cn.demo.bigdata.hadooprpc.protocol.IUserLoginService;public class PublishService &#123; public static void main(String[] args) throws Exception &#123; Builder builder = new RPC.Builder(new Configuration()); builder.setBindAddress("localhost") .setPort(8888) .setProtocol(ClientNamenodeProtocol.class) .setInstance(new MyNameNode()); Server server = builder.build(); server.start(); &#125;&#125;]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2Fgit%20%E6%8F%90%E4%BA%A4%E5%89%8D%E5%BC%BA%E5%88%B6%E6%A3%80%E6%9F%A5%E5%90%84%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%94%A8%E6%88%B7%E5%90%8D%E9%82%AE%E7%AE%B1%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[保证提交日志的准确性在提交时，user.name, user.email会进入日志。这些信息，是追踪代码变更的关键。 我们公司为了保证这些信息的准确性，在push时，强制检查，如果user.name和user.email信息不正确，则拒绝push。 全局配置 如果我们工作中只涉及一个git服务器，用一个全局配置就可以搞定了： 12git config --global user.name &quot;huqiu.lhq&quot;git config --global user.email &quot;huqiu.lhq@alibaba-inc.com&quot; 工作在多个git项目但是我们可能同时工作在多个项目中，公司内部用自有的git管理项目，我们在github上还有自己的项目。 对于使用不同的用户身份，需要设置不用的sshkey，具体的配置可以看这里：多个sshkey配置 这个时候，对于user.name和user.email我们不能采用全局的配置。而是要对各个项目单独配置。 项目配置 12git config user.name &quot;huqiu.lhq&quot;git config user.email &quot;huqiu.lhq@alibaba-inc.com&quot; 忘记了做配置对于项目配置，有时我们会忘记在git init或者git clone之后，配置user.name以及user.email。 如果有全局配置，则使用全局配置。如果没全局配置，报错。 12345678910111213[huqiu@srain test]$ git ci -a -m &apos;commit for testing no user.name empty&apos;*** Please tell me who you are.Rungit config --global user.email &quot;you@example.com&quot;git config --global user.name &quot;Your Name&quot;to set your account&apos;s default identity.Omit --global to set the identity only in this repository.fatal: empty ident name (for &lt;huqiu@srain.localhost&gt;) not allowed 报错能够及时纠正我们的错误，最糟糕的情况是： 没有项目单独配置，提交的时候，自动采用全局配置。在发现问题之后需要对日志进行修复。 强制检查强制检查可以在服务器端push的时候检查，也可以在客户端进行检查，这里介绍使用`pre-commit钩子进行检查。 全局钩子的配置，可以参见这里: git全局钩子 如何确定配置正确 确定没有全局配置 确定有项目配置 pre-commit hook 1234567891011121314global_email=$(git config --global user.email)global_name=$(git config --global user.name)repository_email=$(git config user.email)repository_name=$(git config user.name)if [ -z &quot;$repository_email&quot; ] || [ -z &quot;$repository_name&quot; ] || [ -n &quot;$global_email&quot; ] || [ -n &quot;$global_name&quot; ]; then # user.email is empty echo &quot;ERROR: [pre-commit hook] Aborting commit because user.email or user.name is missing. Configure them for this repository. Make sure not to configure globally.&quot; exit 1else # user.email is not empty exit 0fi 直接可用的代码上面谈到的： pre-commit 的源码 自动设置全局钩子的脚本(其中包含了这个强制检查的pre-commit钩子)，并对git做一些易用配置： update-git-config.sh]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F03%2F06%2F%E5%B7%B2%E7%BB%8F%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%E5%A5%BD%E7%9A%84nginx%EF%BC%8C%E6%B7%BB%E5%8A%A0%E6%9C%AA%E8%A2%AB%E7%BC%96%E8%AF%91%E7%9A%84%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[今天再做nginx下https配置的时候，配置完nginx重启的时候，报了如下错误： 1nginx: [emerg] the &quot;ssl&quot; parameter requires ngx_http_ssl_module in /usr/local/nginx/conf/nginx.conf:37 原因也很简单，nginx缺少httpsslmodule模块，编译安装的时候带上–with-httpsslmodule配置就行了，但是现在的情况是我的nginx已经安装过了，怎么添加模块，其实也很简单，往下看： 做个说明：我的nginx的安装目录是/usr/local/nginx这个目录，我的源码包在/usr/local/src/nginx-1.6.2目录 步骤开始： 切换到源码包： cd /usr/local/src/nginx-1.6.2 查看nginx原有的模块 /usr/local/nginx/sbin/nginx -V在configure arguments:后面显示的原有的configure参数如下： --prefix=/usr/local/nginx --with-http_stub_status_module那么我们的新配置信息就应该这样写： 1./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module 运行上面的命令即可，等配置完 配置完成后，运行命令 make这里不要进行make install，否则就是覆盖安装 然后备份原有已安装好的nginx cp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx.bak然后将刚刚编译好的nginx覆盖掉原有的nginx（这个时候nginx要停止状态） cp ./objs/nginx /usr/local/nginx/sbin/提示是否覆盖，输入yes即可 然后启动nginx，仍可以通过命令查看是否已经加入成功 /usr/local/nginx/sbin/nginx -V]]></content>
  </entry>
  <entry>
    <title><![CDATA[ActiveMQ]]></title>
    <url>%2F2018%2F03%2F06%2FActiveMQ%2F</url>
    <content type="text"><![CDATA[ActiveMQ简单的示例下载ActiveMQ 去官方网站下载：http://activemq.apache.org/ 运行ActiveMQ 解压缩apache-activemq-5.9.0-bin.zip， 修改配置文件activeMQ.xml，将0.0.0.0修改为localhost 123456789&lt;transportConnector name="openwire" uri="tcp://localhost:61616"/&gt;&lt;transportConnector name="ssl" uri="ssl://localhost:61617"/&gt;&lt;transportConnector name="stomp" uri="stomp://localhost:61613"/&gt;&lt;transportConnector uri="http://localhost:8081"/&gt;&lt;transportConnector uri="udp://localhost:61618"/&gt; 然后双击apache-activemq-5.9.0\bin\activemq.bat运行ActiveMQ程序。 启动ActiveMQ以后，登陆：http://localhost:8161/admin/ 账号密码：admin 创建一个Queue，命名为FirstQueue。 点对点 即一个生产者和一个消费者一一对应 producer生产者12345678910111213141516171819202122232425262728293031323334public static void main(String[] args) throws Exception &#123; // 1. 创建连接工厂ActiveMQConnectionFactory，需要ip和端口61616 ActiveMQConnectionFactory factory = new ActiveMQConnectionFactory("tcp://192.168.37.161:61616"); // 2. 从连接工厂中创建连接对象 Connection connection = factory.createConnection(); // 3. 执行start方法开启连接 connection.start(); // 4. 从连接中创建session对象 // 第一个参数，是否开启事务，JTA分布式事务 // 第二个参数，是否自动应答，如果第一个参数为true，第二个参数失效 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); // 5. 从session中创建Destination对象，设置queue名称（有两种类型queue和topic） Queue queue = session.createQueue("test-queue"); // 6. 从session中创建Product对象 MessageProducer producer = session.createProducer(queue); // 7. 创建消息对象 TextMessage textMessage = new ActiveMQTextMessage(); // 设置消息内容 textMessage.setText("开始发消息！"); // 8. 发送消息 producer.send(textMessage); // 9. 关闭session、连接 producer.close(); session.close(); connection.close(); &#125; consumer消费者 直接获取消息 123456789101112131415161718192021222324252627282930313233343536373839404142public static void main(String[] args) throws Exception &#123; // 1. 创建连接工厂ActiveMQConnectionFactory，需要ip和端口61616 ActiveMQConnectionFactory factory = new ActiveMQConnectionFactory("tcp://192.168.37.161:61616"); // 2. 使用工厂创建连接 Connection connection = factory.createConnection(); // 3. 使用start开启连接 connection.start(); // 4. 从连接中创建session对象 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); // 5. 从session中创建Destination对象，设置queue名字 Queue queue = session.createQueue("test-queue"); // 6. 从session中创建Consumer MessageConsumer consumer = session.createConsumer(queue); // 7， 接收消息,直接获取 while (true) &#123; // 消息超时时间是20秒 Message message = consumer.receive(20000); // 如果消息为空，则跳出死循环 if (message == null) &#123; break; &#125; // 8. 打印消息 if (message instanceof TextMessage) &#123; // 获取消息 TextMessage textMessage = (TextMessage) message; // 打印 System.out.println(textMessage.getText()); &#125; &#125; // 9. 关闭session、连接等 consumer.close(); session.close(); connection.close(); &#125; ​ 使用监听器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public static void main(String[] args) throws Exception &#123; // 1. 创建连接工厂ActiveMQConnectionFactory，需要ip和端口61616 ActiveMQConnectionFactory factory = new ActiveMQConnectionFactory("tcp://192.168.37.161:61616"); // 2. 使用工厂创建连接 Connection connection = factory.createConnection(); // 3. 使用start开启连接 connection.start(); // 4. 从连接中创建session对象 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); // 5. 从session中创建Destination对象，设置queue名字 Queue queue = session.createQueue("test-queue"); // 6. 从session中创建Consumer MessageConsumer consumer = session.createConsumer(queue); // // 7， 接收消息,直接获取 // while (true) &#123; // // 消息超时时间是20秒 // Message message = consumer.receive(20000); // // 如果消息为空，则跳出死循环 // if (message == null) &#123; // break; // &#125; // // // 8. 打印消息 // if (true) &#123; // if (message instanceof TextMessage) &#123; // // 获取消息 // TextMessage textMessage = (TextMessage) message; // // 打印 // System.out.println(textMessage.getText()); // &#125; // &#125; // &#125; // 7.接收消息 // 监听器的方式实际上是开启了一个新的线程，专门处理消息的接受 // 现在的情况是，主线程执行完就结束了，新的线程也跟着没了 consumer.setMessageListener(new MessageListener() &#123; @Override public void onMessage(Message message) &#123; if (message instanceof TextMessage) &#123; // 获取消息 TextMessage textMessage = (TextMessage) message; try &#123; // 打印 System.out.println(textMessage.getText()); &#125; catch (JMSException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; &#125;); // 让主线程等待一会，监听器能够有时间执行 Thread.sleep(10000); // 9. 关闭session、连接等 consumer.close(); session.close(); connection.close(); &#125; ​ 发布/订阅模式 即一个生产者产生消息并进行发送后，可以由多个消费者进行接收。 producer生产者12345678910111213141516171819202122232425262728293031323334public static void main(String[] args) throws Exception &#123; // 1. 创建连接工厂ActiveMQConnectionFactory ActiveMQConnectionFactory activeMQConnectionFactory = new ActiveMQConnectionFactory( "tcp://192.168.37.161:61616"); // 2. 使用工厂创建连接 Connection connection = activeMQConnectionFactory.createConnection(); // 3. 使用start方法开启连接 connection.start(); // 4. 从连接创建session Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); // 5. 从session创建Destination对象，设置topic名称 Topic topic = session.createTopic("test-topic"); // 6. 从session创建Product MessageProducer producer = session.createProducer(topic); // 7. 创建消息对象 TextMessage textMessage = new ActiveMQTextMessage(); textMessage.setText("topic消息"); // 8. 发送消息 producer.send(textMessage); // 9. 关闭session、连接等 producer.close(); session.close(); connection.close(); &#125;&#125; consumer消费者 consumer1 1234567891011121314151617181920212223242526272829303132333435363738394041public static void main(String[] args) throws Exception &#123; // 1. 创建连接工厂ActiveMQConnectionFactory ActiveMQConnectionFactory activeMQConnectionFactory = new ActiveMQConnectionFactory( "tcp://192.168.37.161:61616"); // 2. 从连接工厂创建连接 Connection connection = activeMQConnectionFactory.createConnection(); // 3. 使用start方法开启连接 connection.start(); // 4. 从连接创建session对象 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); // 5. 从session创建安Destination，设置topic名称 Topic topic = session.createTopic("test-topic"); // 6. 从session创建Consumer对象 MessageConsumer consumer = session.createConsumer(topic); // 7. 接收消息，直接接受 while (true) &#123; Message message = consumer.receive(20000); if (message == null) &#123; break; &#125; if (message instanceof TextMessage) &#123; TextMessage textMessage = (TextMessage) message; // 8. 打印消息 System.out.println(textMessage.getText()); &#125; &#125; // 9. 关闭session、消息等 consumer.close(); session.close(); connection.close(); &#125; consumer2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public static void main(String[] args) throws Exception &#123; // 1. 创建连接工厂ActiveMQConnectionFactory ActiveMQConnectionFactory activeMQConnectionFactory = new ActiveMQConnectionFactory( "tcp://192.168.37.161:61616"); // 2. 从连接工厂创建连接 Connection connection = activeMQConnectionFactory.createConnection(); // 3. 使用start方法开启连接 connection.start(); // 4. 从连接创建session对象 Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); // 5. 从session创建安Destination，设置topic名称 Topic topic = session.createTopic("test-topic"); // 6. 从session创建Consumer对象 MessageConsumer consumer = session.createConsumer(topic); // 7. 接收消息，直接接受 // while (true) &#123; // Message message = consumer.receive(20000); // // if (message == null) &#123; // break; // &#125; // // if (message instanceof TextMessage) &#123; // TextMessage textMessage = (TextMessage) message; // // 8. 打印消息 // System.out.println(textMessage.getText()); // &#125; // &#125; // 7.接受消息，使用监听器 consumer.setMessageListener(new MessageListener() &#123; @Override public void onMessage(Message message) &#123; if (message instanceof TextMessage) &#123; TextMessage textMessage = (TextMessage) message; try &#123; // 打印消息 System.out.println(textMessage.getText()); &#125; catch (JMSException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; &#125;); // 等待监听器执行 Thread.sleep(10000); // 9. 关闭session、消息等 consumer.close(); session.close(); connection.close(); &#125; 整合spring加入依赖1234567891011121314151617&lt;!-- 加入ActiveMQ依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-all&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 加入spring-jms依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- Spring --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt;&lt;/dependency&gt; 消息发送1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123;// 1. 创建spring容器ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext("applicationContext-activemq.xml");// 2. 从容器中获取JMSTemplate对象JmsTemplate jmsTemplate = context.getBean(JmsTemplate.class);// 3. 从容器中获取Destination对象Destination destination = context.getBean(Destination.class);// 4. 使用JMSTemplate发送消息jmsTemplate.send(destination, new MessageCreator() &#123;@Overridepublic Message createMessage(Session session) throws JMSException &#123;// 创建消息对象TextMessage textMessage = new ActiveMQTextMessage();// 设置消息内容textMessage.setText("spring整合ActiveMQ");// 打印消息System.out.println(textMessage.getText());return textMessage;&#125;&#125;);&#125; 消息接收1234567891011121314151617181920212223public class MyMessageListener implements MessageListener &#123;@Overridepublic void onMessage(Message message) &#123;if (message instanceof TextMessage) &#123;TextMessage textMessage = (TextMessage) message;try &#123;// 获取消息内容String msg = textMessage.getText();// 打印消息System.out.println("接受消息:" + msg);&#125; catch (JMSException e) &#123;// TODO Auto-generated catch blocke.printStackTrace();&#125;&#125;&#125;&#125; queue方式配置spring1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xmlns:p="http://www.springframework.org/schema/p" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:jms="http://www.springframework.org/schema/jms" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd http://www.springframework.org/schema/jms http://www.springframework.org/schema/jms/spring-jms-4.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.0.xsd"&gt; &lt;!-- 真正可以产生Connection的ConnectionFactory，由对应的 JMS服务厂商提供 --&gt; &lt;bean id="targetConnectionFactory" class="org.apache.activemq.ActiveMQConnectionFactory"&gt; &lt;property name="brokerURL" value="tcp://192.168.37.161:61616" /&gt; &lt;/bean&gt; &lt;!-- Spring用于管理真正的ConnectionFactory的ConnectionFactory --&gt; &lt;bean id="connectionFactory" class="org.springframework.jms.connection.SingleConnectionFactory"&gt; &lt;!-- 目标ConnectionFactory对应真实的可以产生JMS Connection的ConnectionFactory --&gt; &lt;property name="targetConnectionFactory" ref="targetConnectionFactory" /&gt; &lt;/bean&gt; &lt;!-- Spring提供的JMS工具类，它可以进行消息发送、接收等 --&gt; &lt;bean id="jmsTemplate" class="org.springframework.jms.core.JmsTemplate"&gt; &lt;!-- 这个connectionFactory对应的是我们定义的Spring提供的那个ConnectionFactory对象 --&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;/bean&gt; &lt;!--这个是队列目的地，点对点的 --&gt; &lt;bean id="queueDestination" class="org.apache.activemq.command.ActiveMQQueue"&gt; &lt;constructor-arg&gt; &lt;value&gt;queue&lt;/value&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!--这个是主题目的地，一对多的 --&gt; &lt;!-- &lt;bean id="topicDestination" class="org.apache.activemq.command.ActiveMQTopic"&gt; --&gt; &lt;!-- &lt;constructor-arg value="topic" /&gt; --&gt; &lt;!-- &lt;/bean&gt; --&gt; &lt;!-- messageListener实现类 --&gt; &lt;bean id="myMessageListener" class="cn.itcast.activemq.spring.MyMessageListener"&gt;&lt;/bean&gt; &lt;!-- 配置一个jsm监听容器 --&gt; &lt;bean id="jmsContainer" class="org.springframework.jms.listener.DefaultMessageListenerContainer"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="destination" ref="queueDestination" /&gt; &lt;property name="messageListener" ref="myMessageListener" /&gt; &lt;/bean&gt;&lt;/beans&gt; topic方式配置spring1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xmlns:p="http://www.springframework.org/schema/p" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:jms="http://www.springframework.org/schema/jms" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd http://www.springframework.org/schema/jms http://www.springframework.org/schema/jms/spring-jms-4.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.0.xsd"&gt; &lt;!-- 真正可以产生Connection的ConnectionFactory，由对应的 JMS服务厂商提供 --&gt; &lt;bean id="targetConnectionFactory" class="org.apache.activemq.ActiveMQConnectionFactory"&gt; &lt;property name="brokerURL" value="tcp://192.168.37.161:61616" /&gt; &lt;/bean&gt; &lt;!-- Spring用于管理真正的ConnectionFactory的ConnectionFactory --&gt; &lt;bean id="connectionFactory" class="org.springframework.jms.connection.SingleConnectionFactory"&gt; &lt;!-- 目标ConnectionFactory对应真实的可以产生JMS Connection的ConnectionFactory --&gt; &lt;property name="targetConnectionFactory" ref="targetConnectionFactory" /&gt; &lt;/bean&gt; &lt;!-- Spring提供的JMS工具类，它可以进行消息发送、接收等 --&gt; &lt;bean id="jmsTemplate" class="org.springframework.jms.core.JmsTemplate"&gt; &lt;!-- 这个connectionFactory对应的是我们定义的Spring提供的那个ConnectionFactory对象 --&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;/bean&gt; &lt;!--这个是队列目的地，点对点的 --&gt; &lt;!-- &lt;bean id="queueDestination" class="org.apache.activemq.command.ActiveMQQueue"&gt; --&gt; &lt;!-- &lt;constructor-arg&gt; --&gt; &lt;!-- &lt;value&gt;queue&lt;/value&gt; --&gt; &lt;!-- &lt;/constructor-arg&gt; --&gt; &lt;!-- &lt;/bean&gt; --&gt; &lt;!--这个是主题目的地，一对多的 --&gt; &lt;bean id="topicDestination" class="org.apache.activemq.command.ActiveMQTopic"&gt; &lt;constructor-arg value="topic" /&gt; &lt;/bean&gt; &lt;!-- messageListener实现类 --&gt; &lt;bean id="myMessageListener" class="cn.itcast.activemq.spring.MyMessageListener"&gt;&lt;/bean&gt; &lt;!-- messageListener实现类 --&gt; &lt;bean id="myMessageListener2" class="cn.itcast.activemq.spring.MyMessageListener2"&gt;&lt;/bean&gt; &lt;!-- 配置一个jsm监听容器 --&gt; &lt;bean id="jmsContainer" class="org.springframework.jms.listener.DefaultMessageListenerContainer"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="destination" ref="topicDestination" /&gt; &lt;property name="messageListener" ref="myMessageListener" /&gt; &lt;/bean&gt; &lt;!-- 配置一个jsm监听容器 --&gt; &lt;bean id="jmsContainer2" class="org.springframework.jms.listener.DefaultMessageListenerContainer"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="destination" ref="topicDestination" /&gt; &lt;property name="messageListener" ref="myMessageListener2" /&gt; &lt;/bean&gt;&lt;/beans&gt;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JMS</tag>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos升级Python 2.7.12并安装最新pip]]></title>
    <url>%2F2018%2F03%2F06%2FCentos%E5%8D%87%E7%BA%A7Python%202.7.12%E5%B9%B6%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0pip%2F</url>
    <content type="text"><![CDATA[Centos升级Python 2.7.12并安装最新pipCentos系统一般默认就安装有Python2.6.6版本，不少软件需要2.7以上的，通过包管理工具安装不了最新的版本，通过源码编译可以方便安装指定版本，只需要把下面版本的数字换成你想要的版本号。 1.安装步骤下载源码 wget http://www.python.org/ftp/python/2.7.12/Python-2.7.12.tgz 在下载目录解压源码 tar -zxvf Python-2.7.12.tgz 进入解压后的文件夹 cd Python-2.7.12 在编译前先在/usr/local建一个文件夹python2.7.12（作为python的安装路径，以免覆盖老的版本，新旧版本可以共存的) mkdir /usr/local/python2.7.12 ，编译前需要安装下面依赖，否则下面安装pip就会出错 yum install openssl openssl-devel zlib-devel gcc -y 安装完依赖后执行下面命令 vim ./Modules/Setup 找到#zlib zlibmodule.c -I$(prefix)/include -L$(exec_prefix)/lib -lz去掉注释并保存(即去掉井号) 在解压缩后的目录下编译安装 123./configure --prefix=/usr/local/python2.7.12 --with-zlibmake make install 此时没有覆盖老版本，再将原来/usr/bin/python链接改为别的名字 mv /usr/bin/python /usr/bin/python2.6.6 再建立新版本python的软链接 ln -s /usr/local/python2.7.12/bin/python2.7 /usr/bin/python 这个时候输入 python 就会显示出python的新版本信息 123Python 2.7.12 (default, Oct 13 2016, 03:17:14) [GCC 4.4.7 20120313 (Red Hat 4.4.7-17)] on linux2Type “help”, “copyright”, “credits” or “license” for more information. 2.修改yum配置文件之所以要保留旧版本，因为yum依赖Python2.6，改下yum的配置文件，指定旧的Python版本就可以了。 vim /usr/bin/yum， 将第一行的#!/usr/bin/python修改成#!/usr/bin/python2.6.6 3.安装最新版本的pipwget https://bootstrap.pypa.io/get-pip.py python get-pip.py 找到pip2.7的路径 find / -name &quot;pip*&quot; 上面的命令输出 /root/.cache/pip 这里省略一堆输出 123456/usr/local/python2.7.12/bin/pip/usr/local/python2.7.12/bin/pip2/usr/local/python2.7.12/bin/pip2.7 #就是这个/usr/bin/pip/usr/bin/pip2/usr/bin/pip2.6 为其创建软链作为系统默认的启动版本（之前有旧版本的话就先删掉rm -rf /usr/bin/pip） ln -s /usr/local/python2.7.12/bin/pip2.7 /usr/bin/pip 看下pip的版本 pip -V 1pip 8.1.2 from /usr/local/python2.7.12/lib/python2.7/site-packages (python 2.7) pip安装完毕，现在可以用它下载安装各种包了 我把上面的所有写成下面简单的脚本，一键就可以升级好。 wget http://7xpt4s.com1.z0.glb.clouddn.com/update-python2.7.12.sh &amp;&amp; bash update-python2.7.12.sh 参考： https://ruter.github.io/2015/12/03/Update-python/ https://blog.phpgao.com/pip-easy_install-setuptool.html 原文地址：https://blog.fazero.me/2016/10/13/centos-update-python/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>python</tag>
      </tags>
  </entry>
</search>
